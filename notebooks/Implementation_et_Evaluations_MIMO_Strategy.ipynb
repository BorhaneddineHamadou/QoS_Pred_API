{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwAhLC4yqGp1"
      },
      "outputs": [],
      "source": [
        "# Importing functions and classes we'll use\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dropout, Flatten\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from warnings import catch_warnings\n",
        "from warnings import filterwarnings\n",
        "from tqdm import tqdm\n",
        "import keras\n",
        "import sys\n",
        "import scipy.stats\n",
        "import json\n",
        "import numpy.fft\n",
        "import time\n",
        "from decimal import Decimal\n",
        "import math\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "tf.random.set_seed(1234)"
      ],
      "metadata": {
        "id": "tgnqfQhuHwSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/sample_data/new_dataset.xlsx')\n",
        "node1_delay = df[['node1_delay']]\n",
        "\n",
        "dataset = node1_delay.values"
      ],
      "metadata": {
        "id": "6XcGP3x4HyQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MA Filter Implementation"
      ],
      "metadata": {
        "id": "qjAkcsCCUssQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ASAP\n",
        "class Metrics(object):\n",
        "    def __init__(self, values):\n",
        "        self.set_values( values )\n",
        "\n",
        "    def set_values(self, values):\n",
        "        self.values = values\n",
        "        self.r = self.k = None\n",
        "\n",
        "    @property\n",
        "    def kurtosis(self):\n",
        "        if self.k is None:\n",
        "            self.k = scipy.stats.kurtosis(self.values)\n",
        "        return self.k\n",
        "\n",
        "    @property\n",
        "    def roughness(self):\n",
        "        if self.r is None:\n",
        "            self.r = np.std(np.diff(self.values))\n",
        "        return self.r\n",
        "\n",
        "class ACF(Metrics):\n",
        "    CORR_THRESH = 0.2\n",
        "    def __init__(self, values, max_lag=None):\n",
        "        super(ACF, self).__init__(values)\n",
        "        if max_lag is None:\n",
        "            max_lag = len(values) / 5\n",
        "        self.max_lag = int(max_lag)\n",
        "        self.max_acf = 0.0\n",
        "\n",
        "        # Calculate autocorrelation via FFT\n",
        "        # Demean\n",
        "        demeaned = values - np.mean(values)\n",
        "        # Pad data to power of 2\n",
        "        l = int(2.0 ** (int(math.log(len(demeaned),2.0)) + 1))\n",
        "        padded = np.append(demeaned, ([0.0] * (l - len(demeaned))))\n",
        "        # FFT and inverse FFT\n",
        "        F_f = numpy.fft.fft( padded )\n",
        "        R_t = numpy.fft.ifft( F_f * np.conjugate(F_f) )\n",
        "        self.correlations = R_t[:int(max_lag)].real / R_t[0].real\n",
        "\n",
        "        # Find autocorrelation peaks\n",
        "        self.peaks = []\n",
        "        if len(self.correlations) >1 :\n",
        "            positive = self.correlations[1] > self.correlations[0]\n",
        "            max = 1\n",
        "            for i in range(2, len(self.correlations)):\n",
        "                if not positive and self.correlations[i] > self.correlations[i-1]:\n",
        "                    max = i\n",
        "                    positive = not positive\n",
        "                elif positive and self.correlations[i] > self.correlations[max]:\n",
        "                    max = i\n",
        "                elif positive and self.correlations[i] < self.correlations[i-1]:\n",
        "                    if max > 1 and self.correlations[max] > self.CORR_THRESH:\n",
        "                        self.peaks.append(max)\n",
        "                        if self.correlations[max] > self.max_acf:\n",
        "                            self.max_acf = self.correlations[max]\n",
        "                    positive = not positive\n",
        "        # If there is no autocorrelation peak within the MAX_WINDOW boundary,\n",
        "        # try windows from the largest to the smallest\n",
        "        if len(self.peaks) <= 1:\n",
        "            self.peaks = range(2, len(self.correlations))\n",
        "\n",
        "def moving_average(data, _range):\n",
        "    ret = np.cumsum(data)\n",
        "    ret[_range:] = ret[int(_range):] - ret[:-int(_range)]\n",
        "    return ret[int(_range) - 1:] / _range\n",
        "\n",
        "def SMA(data, _range, slide):\n",
        "    ret = moving_average(data, int(_range))[::int(slide)]\n",
        "    return list(ret)\n",
        "\n",
        "def binary_search(head,tail,data,min_obj,orig_kurt,window_size):\n",
        "    while head <= tail:\n",
        "        w = int(round((head + tail) / 2.0))\n",
        "        smoothed = SMA(data,w,1)\n",
        "        metrics  = Metrics(smoothed)\n",
        "        if metrics.kurtosis >= orig_kurt:\n",
        "            if metrics.roughness < min_obj:\n",
        "                window_size = w\n",
        "                min_obj = metrics.roughness\n",
        "            head = w + 1\n",
        "        else:\n",
        "            tail = w - 1\n",
        "    return window_size\n",
        "\n",
        "def smooth_ASAP(data, max_window=5, resolution=None):\n",
        "    data = np.array(data)\n",
        "    # Preaggregate according to resolution\n",
        "    slide_size = 1\n",
        "    window_size = 1\n",
        "    if resolution and len(data) >= 2 * resolution:\n",
        "        slide_size = len(data) / resolution\n",
        "        data = SMA(data, slide_size, slide_size)\n",
        "    acf         = ACF(data, max_lag=len(data) / max_window)\n",
        "    peaks       = acf.peaks\n",
        "    orig_kurt   = acf.kurtosis\n",
        "    min_obj     = acf.roughness\n",
        "    lb          = 1\n",
        "    largest_feasible = -1\n",
        "    tail = len(data) / max_window\n",
        "    for i in range(len(peaks) - 1, -1, -1):\n",
        "        w = peaks[i]\n",
        "\n",
        "        if w < lb or w == 1:\n",
        "            break\n",
        "        elif math.sqrt(1 - acf.correlations[w]) * window_size > math.sqrt(1 - acf.correlations[window_size]) * w:\n",
        "            continue\n",
        "\n",
        "        smoothed = SMA(data, w, 1)\n",
        "        metrics = Metrics(smoothed)\n",
        "        if metrics.roughness < min_obj and metrics.kurtosis >= orig_kurt:\n",
        "            min_obj = metrics.roughness\n",
        "            window_size = w\n",
        "            lb = round( max(w*math.sqrt( (acf.max_acf -1) / (acf.correlations[w]-1) ), lb) )\n",
        "    if largest_feasible > 0:\n",
        "        if largest_feasible < len(peaks) - 2:\n",
        "            tail = peaks[largest_feasible + 1]\n",
        "        lb = max(lb, peaks[largest_feasible] + 1)\n",
        "\n",
        "    window_size = binary_search(lb, tail, data, min_obj, orig_kurt, window_size)\n",
        "    return window_size, slide_size"
      ],
      "metadata": {
        "id": "JClaXWPKUod5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm to transfer data in tabular format"
      ],
      "metadata": {
        "id": "o15lRHTkVj8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multistep_dataset(data, n_input, n_out=1):\n",
        "    X, y = list(), list()\n",
        "    in_start = 0\n",
        "    # step over the entire history one time step at a time\n",
        "    for _ in range(len(data)):\n",
        "        # define the end of the input sequence\n",
        "        in_end = in_start + n_input\n",
        "        out_end = in_end + n_out\n",
        "        # ensure we have enough data for this instance\n",
        "        if out_end <= len(data):\n",
        "            x_input = data[in_start:in_end]\n",
        "            X.append(x_input)\n",
        "            y.append(data[in_end:out_end])\n",
        "        # move along one time step\n",
        "        in_start += 1\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "aknEDSo0VoTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ESN Model Library"
      ],
      "metadata": {
        "id": "REoUU7it1k9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_dimensions(s, targetlength):\n",
        "    \"\"\"checks the dimensionality of some numeric argument s, broadcasts it\n",
        "       to the specified length if possible.\n",
        "\n",
        "    Args:\n",
        "        s: None, scalar or 1D array\n",
        "        targetlength: expected length of s\n",
        "\n",
        "    Returns:\n",
        "        None if s is None, else numpy vector of length targetlength\n",
        "    \"\"\"\n",
        "    if s is not None:\n",
        "        s = np.array(s)\n",
        "        if s.ndim == 0:\n",
        "            s = np.array([s] * targetlength)\n",
        "        elif s.ndim == 1:\n",
        "            if not len(s) == targetlength:\n",
        "                raise ValueError(\"arg must have length \" + str(targetlength))\n",
        "        else:\n",
        "            raise ValueError(\"Invalid argument\")\n",
        "    return s\n",
        "\n",
        "\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "class ESN():\n",
        "\n",
        "    def __init__(self, n_inputs, n_outputs, n_reservoir=200,\n",
        "                 spectral_radius=0.95, sparsity=0, noise=0.001, input_shift=None,\n",
        "                 input_scaling=None, teacher_forcing=True, feedback_scaling=None,\n",
        "                 teacher_scaling=None, teacher_shift=None,\n",
        "                 out_activation=identity, inverse_out_activation=identity,\n",
        "                 random_state=None, silent=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_inputs: nr of input dimensions\n",
        "            n_outputs: nr of output dimensions\n",
        "            n_reservoir: nr of reservoir neurons\n",
        "            spectral_radius: spectral radius of the recurrent weight matrix\n",
        "            sparsity: proportion of recurrent weights set to zero\n",
        "            noise: noise added to each neuron (regularization)\n",
        "            input_shift: scalar or vector of length n_inputs to add to each\n",
        "                        input dimension before feeding it to the network.\n",
        "            input_scaling: scalar or vector of length n_inputs to multiply\n",
        "                        with each input dimension before feeding it to the netw.\n",
        "            teacher_forcing: if True, feed the target back into output units\n",
        "            teacher_scaling: factor applied to the target signal\n",
        "            teacher_shift: additive term applied to the target signal\n",
        "            out_activation: output activation function (applied to the readout)\n",
        "            inverse_out_activation: inverse of the output activation function\n",
        "            random_state: positive integer seed, np.rand.RandomState object,\n",
        "                          or None to use numpy's builting RandomState.\n",
        "            silent: supress messages\n",
        "        \"\"\"\n",
        "        # check for proper dimensionality of all arguments and write them down.\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_reservoir = n_reservoir\n",
        "        self.n_outputs = n_outputs\n",
        "        self.spectral_radius = spectral_radius\n",
        "        self.sparsity = sparsity\n",
        "        self.noise = noise\n",
        "        self.input_shift = correct_dimensions(input_shift, n_inputs)\n",
        "        self.input_scaling = correct_dimensions(input_scaling, n_inputs)\n",
        "\n",
        "        self.teacher_scaling = teacher_scaling\n",
        "        self.teacher_shift = teacher_shift\n",
        "\n",
        "        self.out_activation = out_activation\n",
        "        self.inverse_out_activation = inverse_out_activation\n",
        "        self.random_state = random_state\n",
        "\n",
        "        # the given random_state might be either an actual RandomState object,\n",
        "        # a seed or None (in which case we use numpy's builtin RandomState)\n",
        "        if isinstance(random_state, np.random.RandomState):\n",
        "            self.random_state_ = random_state\n",
        "        elif random_state:\n",
        "            try:\n",
        "                self.random_state_ = np.random.RandomState(random_state)\n",
        "            except TypeError as e:\n",
        "                raise Exception(\"Invalid seed: \" + str(e))\n",
        "        else:\n",
        "            self.random_state_ = np.random.mtrand._rand\n",
        "\n",
        "        self.teacher_forcing = teacher_forcing\n",
        "        self.silent = silent\n",
        "        self.initweights()\n",
        "\n",
        "    def initweights(self):\n",
        "        # initialize recurrent weights:\n",
        "        # begin with a random matrix centered around zero:\n",
        "        W = self.random_state_.rand(self.n_reservoir, self.n_reservoir) - 0.5\n",
        "        # delete the fraction of connections given by (self.sparsity):\n",
        "        W[self.random_state_.rand(*W.shape) < self.sparsity] = 0\n",
        "        # compute the spectral radius of these weights:\n",
        "        radius = np.max(np.abs(np.linalg.eigvals(W)))\n",
        "        # rescale them to reach the requested spectral radius:\n",
        "        self.W = W * (self.spectral_radius / radius)\n",
        "\n",
        "        # random input weights:\n",
        "        self.W_in = self.random_state_.rand(\n",
        "            self.n_reservoir, self.n_inputs) * 2 - 1\n",
        "        # random feedback (teacher forcing) weights:\n",
        "        self.W_feedb = self.random_state_.rand(\n",
        "            self.n_reservoir, self.n_outputs) * 2 - 1\n",
        "\n",
        "    def _update(self, state, input_pattern, output_pattern):\n",
        "        \"\"\"performs one update step.\n",
        "\n",
        "        i.e., computes the next network state by applying the recurrent weights\n",
        "        to the last state & and feeding in the current input and output patterns\n",
        "        \"\"\"\n",
        "        if self.teacher_forcing:\n",
        "            preactivation = (np.dot(self.W, state)\n",
        "                             + np.dot(self.W_in, input_pattern)\n",
        "                             + np.dot(self.W_feedb, output_pattern))\n",
        "        else:\n",
        "            preactivation = (np.dot(self.W, state)\n",
        "                             + np.dot(self.W_in, input_pattern))\n",
        "        return (np.tanh(preactivation)\n",
        "                + self.noise * (self.random_state_.rand(self.n_reservoir) - 0.5))\n",
        "\n",
        "    def _scale_inputs(self, inputs):\n",
        "        \"\"\"for each input dimension j: multiplies by the j'th entry in the\n",
        "        input_scaling argument, then adds the j'th entry of the input_shift\n",
        "        argument.\"\"\"\n",
        "        if self.input_scaling is not None:\n",
        "            inputs = np.dot(inputs, np.diag(self.input_scaling))\n",
        "        if self.input_shift is not None:\n",
        "            inputs = inputs + self.input_shift\n",
        "        return inputs\n",
        "\n",
        "    def _scale_teacher(self, teacher):\n",
        "        \"\"\"multiplies the teacher/target signal by the teacher_scaling argument,\n",
        "        then adds the teacher_shift argument to it.\"\"\"\n",
        "        if self.teacher_scaling is not None:\n",
        "            teacher = teacher * self.teacher_scaling\n",
        "        if self.teacher_shift is not None:\n",
        "            teacher = teacher + self.teacher_shift\n",
        "        return teacher\n",
        "\n",
        "    def _unscale_teacher(self, teacher_scaled):\n",
        "        \"\"\"inverse operation of the _scale_teacher method.\"\"\"\n",
        "        if self.teacher_shift is not None:\n",
        "            teacher_scaled = teacher_scaled - self.teacher_shift\n",
        "        if self.teacher_scaling is not None:\n",
        "            teacher_scaled = teacher_scaled / self.teacher_scaling\n",
        "        return teacher_scaled\n",
        "\n",
        "    def fit(self, inputs, outputs, inspect=False):\n",
        "        \"\"\"\n",
        "        Collect the network's reaction to training data, train readout weights.\n",
        "\n",
        "        Args:\n",
        "            inputs: array of dimensions (N_training_samples x n_inputs)\n",
        "            outputs: array of dimension (N_training_samples x n_outputs)\n",
        "            inspect: show a visualisation of the collected reservoir states\n",
        "\n",
        "        Returns:\n",
        "            the network's output on the training data, using the trained weights\n",
        "        \"\"\"\n",
        "        # transform any vectors of shape (x,) into vectors of shape (x,1):\n",
        "        if inputs.ndim < 2:\n",
        "            inputs = np.reshape(inputs, (len(inputs), -1))\n",
        "        if outputs.ndim < 2:\n",
        "            outputs = np.reshape(outputs, (len(outputs), -1))\n",
        "        # transform input and teacher signal:\n",
        "        inputs_scaled = self._scale_inputs(inputs)\n",
        "        teachers_scaled = self._scale_teacher(outputs)\n",
        "\n",
        "        if not self.silent:\n",
        "            print(\"harvesting states...\")\n",
        "        # step the reservoir through the given input,output pairs:\n",
        "        states = np.zeros((inputs.shape[0], self.n_reservoir))\n",
        "        for n in range(1, inputs.shape[0]):\n",
        "            states[n, :] = self._update(states[n - 1], inputs_scaled[n, :],\n",
        "                                        teachers_scaled[n - 1, :])\n",
        "\n",
        "        # learn the weights, i.e. find the linear combination of collected\n",
        "        # network states that is closest to the target output\n",
        "        if not self.silent:\n",
        "            print(\"fitting...\")\n",
        "        # we'll disregard the first few states:\n",
        "        transient = min(int(inputs.shape[1] / 10), 100)\n",
        "        # include the raw inputs:\n",
        "        extended_states = np.hstack((states, inputs_scaled))\n",
        "        # Solve for W_out:\n",
        "        self.W_out = np.dot(np.linalg.pinv(extended_states[transient:, :]),\n",
        "                            self.inverse_out_activation(teachers_scaled[transient:, :])).T\n",
        "\n",
        "        # remember the last state for later:\n",
        "        self.laststate = states[-1, :]\n",
        "        self.lastinput = inputs[-1, :]\n",
        "        self.lastoutput = teachers_scaled[-1, :]\n",
        "\n",
        "        # optionally visualize the collected states\n",
        "        if inspect:\n",
        "            from matplotlib import pyplot as plt\n",
        "            # (^-- we depend on matplotlib only if this option is used)\n",
        "            plt.figure(\n",
        "                figsize=(states.shape[0] * 0.0025, states.shape[1] * 0.01))\n",
        "            plt.imshow(extended_states.T, aspect='auto',\n",
        "                       interpolation='nearest')\n",
        "            plt.colorbar()\n",
        "\n",
        "        if not self.silent:\n",
        "            print(\"training error:\")\n",
        "        # apply learned weights to the collected states:\n",
        "        pred_train = self._unscale_teacher(self.out_activation(\n",
        "            np.dot(extended_states, self.W_out.T)))\n",
        "        if not self.silent:\n",
        "            print(np.sqrt(np.mean((pred_train - outputs)**2)))\n",
        "        return pred_train\n",
        "\n",
        "    def predict(self, inputs, continuation=True):\n",
        "        \"\"\"\n",
        "        Apply the learned weights to the network's reactions to new input.\n",
        "\n",
        "        Args:\n",
        "            inputs: array of dimensions (N_test_samples x n_inputs)\n",
        "            continuation: if True, start the network from the last training state\n",
        "\n",
        "        Returns:\n",
        "            Array of output activations\n",
        "        \"\"\"\n",
        "        if inputs.ndim < 2:\n",
        "            inputs = np.reshape(inputs, (len(inputs), -1))\n",
        "        n_samples = inputs.shape[0]\n",
        "\n",
        "        if continuation:\n",
        "            laststate = self.laststate\n",
        "            lastinput = self.lastinput\n",
        "            lastoutput = self.lastoutput\n",
        "        else:\n",
        "            laststate = np.zeros(self.n_reservoir)\n",
        "            lastinput = np.zeros(self.n_inputs)\n",
        "            lastoutput = np.zeros(self.n_outputs)\n",
        "\n",
        "        inputs = np.vstack([lastinput, self._scale_inputs(inputs)])\n",
        "        states = np.vstack(\n",
        "            [laststate, np.zeros((n_samples, self.n_reservoir))])\n",
        "        outputs = np.vstack(\n",
        "            [lastoutput, np.zeros((n_samples, self.n_outputs))])\n",
        "\n",
        "        for n in range(n_samples):\n",
        "            states[\n",
        "                n + 1, :] = self._update(states[n, :], inputs[n + 1, :], outputs[n, :])\n",
        "            outputs[n + 1, :] = self.out_activation(np.dot(self.W_out,\n",
        "                                                           np.concatenate([states[n + 1, :], inputs[n + 1, :]])))\n",
        "\n",
        "        return self._unscale_teacher(self.out_activation(outputs[1:]))"
      ],
      "metadata": {
        "id": "nZUjxupO1pYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MIMO Strategy"
      ],
      "metadata": {
        "id": "2pxVq8fvIF9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSTM Model"
      ],
      "metadata": {
        "id": "O9-uyydNJbsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Finding the best hyper-parameters"
      ],
      "metadata": {
        "id": "-4lgH7x-Jduj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF5YOSfnJplT",
        "outputId": "e10feb41-b7bd-4c4e-bd41-d0efc87d0dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler"
      ],
      "metadata": {
        "id": "y0rXVSFOIHhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 2\n",
        "\n",
        "# Hyper Parameters tuning\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "valid_size = int(len(train) * 0.1)\n",
        "temp_train = train[:-valid_size]\n",
        "valid = train[-valid_size:]\n",
        "print(len(train), len(temp_train), len(valid), len(test))\n",
        "\n",
        "def objective(trial):\n",
        "    # define search space for hyperparameters\n",
        "    look_back = trial.suggest_int('look_back', 5, 300)\n",
        "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 10)\n",
        "\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
        "    batch_size = trial.suggest_int('batch_size', 5, 300)\n",
        "    epochs = trial.suggest_int('epochs', 10, 100)\n",
        "\n",
        "    # reshape into X=t and Y=t+1\n",
        "    trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "    validX, validY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "    # reshape input to be [samples, time steps, features]\n",
        "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "    validX = np.reshape(validX, (validX.shape[0], 1, validX.shape[1]))\n",
        "    print(trainX.shape)\n",
        "\n",
        "    # Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "    model = Sequential()\n",
        "    for i in range(num_hidden_layers):\n",
        "        num_units = trial.suggest_int(f'lstm_units_layer_{i}', 8, 256, log=True)\n",
        "        return_sequences = (i < num_hidden_layers - 1)\n",
        "        model.add(LSTM(units=num_units, return_sequences=return_sequences))\n",
        "    model.add(Dense(horizon))\n",
        "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    validPredict = model.predict(validX, verbose=0)\n",
        "\n",
        "    # calculate root mean squared error\n",
        "    validScore = np.sqrt(mean_squared_error(validY, validPredict))\n",
        "\n",
        "    return validScore\n",
        "\n",
        "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "start_time = time.time()\n",
        "\n",
        "study.optimize(objective, n_trials=50, n_jobs=-1)\n",
        "\n",
        "# Record the ending time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print('done')\n",
        "print(\"LSTM HyperParameters Tuning Elapsed Time : %.5f\" % (elapsed_time), \"seconds\")\n",
        "\n",
        "best_params = study.best_params\n",
        "best_error = study.best_value\n",
        "\n",
        "print(\"Done\")\n",
        "print(f\"best_params: {best_params}, best_error: {best_error}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul25vgN1Jrj-",
        "outputId": "30805300-b258-4e8c-aec4-3174e02923b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 07:49:25,490] A new study created in memory with name: no-name-fbe15e96-8df8-413d-9ade-f6191159dabb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 16192 1799 2000\n",
            "(17958, 1, 32)\n",
            "(17917, 1, 73)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 07:51:49,713] Trial 1 finished with value: 0.023953876628435365 and parameters: {'look_back': 32, 'num_hidden_layers': 5, 'learning_rate': 0.0002486158494589482, 'batch_size': 123, 'epochs': 63, 'lstm_units_layer_0': 8, 'lstm_units_layer_1': 84, 'lstm_units_layer_2': 16, 'lstm_units_layer_3': 162, 'lstm_units_layer_4': 29}. Best is trial 1 with value: 0.023953876628435365.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17942, 1, 48)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 07:55:35,427] Trial 0 finished with value: 0.05171286313859949 and parameters: {'look_back': 73, 'num_hidden_layers': 9, 'learning_rate': 0.04175647503853579, 'batch_size': 123, 'epochs': 95, 'lstm_units_layer_0': 202, 'lstm_units_layer_1': 242, 'lstm_units_layer_2': 56, 'lstm_units_layer_3': 33, 'lstm_units_layer_4': 57, 'lstm_units_layer_5': 216, 'lstm_units_layer_6': 59, 'lstm_units_layer_7': 58, 'lstm_units_layer_8': 41}. Best is trial 1 with value: 0.023953876628435365.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17984, 1, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 07:56:04,168] Trial 3 finished with value: 0.026346584900726278 and parameters: {'look_back': 6, 'num_hidden_layers': 2, 'learning_rate': 0.0011880977811016143, 'batch_size': 257, 'epochs': 34, 'lstm_units_layer_0': 167, 'lstm_units_layer_1': 206}. Best is trial 1 with value: 0.023953876628435365.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17936, 1, 54)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 07:57:03,516] Trial 4 finished with value: 0.049712993111781435 and parameters: {'look_back': 54, 'num_hidden_layers': 9, 'learning_rate': 7.336059292198116e-05, 'batch_size': 224, 'epochs': 15, 'lstm_units_layer_0': 221, 'lstm_units_layer_1': 59, 'lstm_units_layer_2': 40, 'lstm_units_layer_3': 214, 'lstm_units_layer_4': 133, 'lstm_units_layer_5': 39, 'lstm_units_layer_6': 86, 'lstm_units_layer_7': 15, 'lstm_units_layer_8': 27}. Best is trial 1 with value: 0.023953876628435365.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17720, 1, 270)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 07:58:56,731] Trial 5 finished with value: 0.053158198372868656 and parameters: {'look_back': 270, 'num_hidden_layers': 9, 'learning_rate': 0.027169309605088485, 'batch_size': 180, 'epochs': 29, 'lstm_units_layer_0': 68, 'lstm_units_layer_1': 44, 'lstm_units_layer_2': 73, 'lstm_units_layer_3': 27, 'lstm_units_layer_4': 220, 'lstm_units_layer_5': 125, 'lstm_units_layer_6': 234, 'lstm_units_layer_7': 34, 'lstm_units_layer_8': 199}. Best is trial 1 with value: 0.023953876628435365.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17703, 1, 287)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 07:59:41,550] Trial 2 finished with value: 0.02903008017080907 and parameters: {'look_back': 48, 'num_hidden_layers': 8, 'learning_rate': 1.5000439380114383e-05, 'batch_size': 25, 'epochs': 37, 'lstm_units_layer_0': 127, 'lstm_units_layer_1': 20, 'lstm_units_layer_2': 82, 'lstm_units_layer_3': 79, 'lstm_units_layer_4': 33, 'lstm_units_layer_5': 11, 'lstm_units_layer_6': 204, 'lstm_units_layer_7': 19}. Best is trial 1 with value: 0.023953876628435365.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17755, 1, 235)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:00:04,252] Trial 6 finished with value: 0.08537517467285974 and parameters: {'look_back': 287, 'num_hidden_layers': 1, 'learning_rate': 0.08994030469321694, 'batch_size': 117, 'epochs': 74, 'lstm_units_layer_0': 167}. Best is trial 1 with value: 0.023953876628435365.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17855, 1, 135)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:01:14,036] Trial 8 finished with value: 0.052216505637323926 and parameters: {'look_back': 135, 'num_hidden_layers': 9, 'learning_rate': 2.176683517754323e-05, 'batch_size': 197, 'epochs': 20, 'lstm_units_layer_0': 94, 'lstm_units_layer_1': 192, 'lstm_units_layer_2': 119, 'lstm_units_layer_3': 42, 'lstm_units_layer_4': 44, 'lstm_units_layer_5': 20, 'lstm_units_layer_6': 61, 'lstm_units_layer_7': 61, 'lstm_units_layer_8': 9}. Best is trial 1 with value: 0.023953876628435365.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17862, 1, 128)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:04:06,502] Trial 9 finished with value: 0.02411867353728479 and parameters: {'look_back': 128, 'num_hidden_layers': 9, 'learning_rate': 0.0014418065297542013, 'batch_size': 106, 'epochs': 40, 'lstm_units_layer_0': 8, 'lstm_units_layer_1': 231, 'lstm_units_layer_2': 152, 'lstm_units_layer_3': 12, 'lstm_units_layer_4': 166, 'lstm_units_layer_5': 180, 'lstm_units_layer_6': 100, 'lstm_units_layer_7': 14, 'lstm_units_layer_8': 43}. Best is trial 1 with value: 0.023953876628435365.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17918, 1, 72)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:05:42,195] Trial 10 finished with value: 0.024479327075792374 and parameters: {'look_back': 72, 'num_hidden_layers': 5, 'learning_rate': 0.014605351056488584, 'batch_size': 300, 'epochs': 96, 'lstm_units_layer_0': 128, 'lstm_units_layer_1': 154, 'lstm_units_layer_2': 100, 'lstm_units_layer_3': 229, 'lstm_units_layer_4': 99}. Best is trial 1 with value: 0.023953876628435365.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17793, 1, 197)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:11:22,726] Trial 11 finished with value: 0.025063127997385186 and parameters: {'look_back': 197, 'num_hidden_layers': 5, 'learning_rate': 0.00016040998193655908, 'batch_size': 53, 'epochs': 62, 'lstm_units_layer_0': 8, 'lstm_units_layer_1': 8, 'lstm_units_layer_2': 13, 'lstm_units_layer_3': 102, 'lstm_units_layer_4': 12}. Best is trial 1 with value: 0.023953876628435365.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17846, 1, 144)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:15:03,198] Trial 12 finished with value: 0.022960222155561903 and parameters: {'look_back': 144, 'num_hidden_layers': 7, 'learning_rate': 0.0009514001581286831, 'batch_size': 82, 'epochs': 52, 'lstm_units_layer_0': 8, 'lstm_units_layer_1': 93, 'lstm_units_layer_2': 253, 'lstm_units_layer_3': 9, 'lstm_units_layer_4': 21, 'lstm_units_layer_5': 91, 'lstm_units_layer_6': 10}. Best is trial 12 with value: 0.022960222155561903.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17804, 1, 186)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:18:47,524] Trial 13 finished with value: 0.023248271100998016 and parameters: {'look_back': 186, 'num_hidden_layers': 6, 'learning_rate': 0.00033257883399393607, 'batch_size': 72, 'epochs': 54, 'lstm_units_layer_0': 16, 'lstm_units_layer_1': 89, 'lstm_units_layer_2': 223, 'lstm_units_layer_3': 9, 'lstm_units_layer_4': 18, 'lstm_units_layer_5': 66}. Best is trial 12 with value: 0.022960222155561903.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17798, 1, 192)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:23:02,852] Trial 14 finished with value: 0.02384232324538982 and parameters: {'look_back': 192, 'num_hidden_layers': 7, 'learning_rate': 0.0037894401604141397, 'batch_size': 68, 'epochs': 52, 'lstm_units_layer_0': 16, 'lstm_units_layer_1': 100, 'lstm_units_layer_2': 250, 'lstm_units_layer_3': 8, 'lstm_units_layer_4': 14, 'lstm_units_layer_5': 66, 'lstm_units_layer_6': 8}. Best is trial 12 with value: 0.022960222155561903.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17806, 1, 184)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:28:30,732] Trial 7 finished with value: 0.024463393554815593 and parameters: {'look_back': 235, 'num_hidden_layers': 7, 'learning_rate': 0.001046096482279084, 'batch_size': 14, 'epochs': 81, 'lstm_units_layer_0': 136, 'lstm_units_layer_1': 211, 'lstm_units_layer_2': 10, 'lstm_units_layer_3': 38, 'lstm_units_layer_4': 82, 'lstm_units_layer_5': 126, 'lstm_units_layer_6': 75}. Best is trial 12 with value: 0.022960222155561903.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17816, 1, 174)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:30:31,871] Trial 16 finished with value: 0.02348205328287459 and parameters: {'look_back': 174, 'num_hidden_layers': 3, 'learning_rate': 0.00027212583209424844, 'batch_size': 68, 'epochs': 52, 'lstm_units_layer_0': 22, 'lstm_units_layer_1': 109, 'lstm_units_layer_2': 225}. Best is trial 12 with value: 0.022960222155561903.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17881, 1, 109)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:33:58,799] Trial 17 finished with value: 0.02373148205250807 and parameters: {'look_back': 109, 'num_hidden_layers': 7, 'learning_rate': 0.00601116378952683, 'batch_size': 79, 'epochs': 48, 'lstm_units_layer_0': 16, 'lstm_units_layer_1': 67, 'lstm_units_layer_2': 180, 'lstm_units_layer_3': 16, 'lstm_units_layer_4': 8, 'lstm_units_layer_5': 63, 'lstm_units_layer_6': 14}. Best is trial 12 with value: 0.022960222155561903.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17762, 1, 228)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:35:48,023] Trial 18 finished with value: 0.023309951371610325 and parameters: {'look_back': 228, 'num_hidden_layers': 4, 'learning_rate': 0.0005098535914151417, 'batch_size': 159, 'epochs': 72, 'lstm_units_layer_0': 37, 'lstm_units_layer_1': 127, 'lstm_units_layer_2': 163, 'lstm_units_layer_3': 9}. Best is trial 12 with value: 0.022960222155561903.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17828, 1, 162)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 08:43:30,238] Trial 19 finished with value: 0.024897847618309467 and parameters: {'look_back': 162, 'num_hidden_layers': 6, 'learning_rate': 6.946997244973476e-05, 'batch_size': 40, 'epochs': 65, 'lstm_units_layer_0': 12, 'lstm_units_layer_1': 41, 'lstm_units_layer_2': 255, 'lstm_units_layer_3': 18, 'lstm_units_layer_4': 24, 'lstm_units_layer_5': 39}. Best is trial 12 with value: 0.022960222155561903.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17886, 1, 104)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:46:08,734] Trial 20 finished with value: 0.022449853837961428 and parameters: {'look_back': 104, 'num_hidden_layers': 6, 'learning_rate': 0.0027444218291277516, 'batch_size': 89, 'epochs': 44, 'lstm_units_layer_0': 26, 'lstm_units_layer_1': 125, 'lstm_units_layer_2': 129, 'lstm_units_layer_3': 8, 'lstm_units_layer_4': 20, 'lstm_units_layer_5': 83}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17895, 1, 95)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 08:46:13,286] Trial 15 finished with value: 0.024342194273360734 and parameters: {'look_back': 184, 'num_hidden_layers': 7, 'learning_rate': 0.0005105548254280217, 'batch_size': 17, 'epochs': 80, 'lstm_units_layer_0': 22, 'lstm_units_layer_1': 103, 'lstm_units_layer_2': 243, 'lstm_units_layer_3': 15, 'lstm_units_layer_4': 8, 'lstm_units_layer_5': 74, 'lstm_units_layer_6': 13}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17889, 1, 101)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 08:47:14,631] Trial 21 finished with value: 0.02298586519199345 and parameters: {'look_back': 95, 'num_hidden_layers': 4, 'learning_rate': 0.002763496988880127, 'batch_size': 102, 'epochs': 25, 'lstm_units_layer_0': 30, 'lstm_units_layer_1': 142, 'lstm_units_layer_2': 134, 'lstm_units_layer_3': 19}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17887, 1, 103)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:48:25,197] Trial 23 finished with value: 0.0356581392335764 and parameters: {'look_back': 103, 'num_hidden_layers': 4, 'learning_rate': 0.0028493733729509445, 'batch_size': 94, 'epochs': 25, 'lstm_units_layer_0': 38, 'lstm_units_layer_1': 155, 'lstm_units_layer_2': 135, 'lstm_units_layer_3': 22}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17898, 1, 92)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 08:48:40,988] Trial 22 finished with value: 0.023567236167530894 and parameters: {'look_back': 101, 'num_hidden_layers': 6, 'learning_rate': 0.0036714905492664404, 'batch_size': 92, 'epochs': 44, 'lstm_units_layer_0': 30, 'lstm_units_layer_1': 141, 'lstm_units_layer_2': 148, 'lstm_units_layer_3': 8, 'lstm_units_layer_4': 19, 'lstm_units_layer_5': 99}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17845, 1, 145)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:49:36,135] Trial 25 finished with value: 0.023643497776677135 and parameters: {'look_back': 145, 'num_hidden_layers': 4, 'learning_rate': 0.008888637348966728, 'batch_size': 150, 'epochs': 29, 'lstm_units_layer_0': 49, 'lstm_units_layer_1': 148, 'lstm_units_layer_2': 106, 'lstm_units_layer_3': 12}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17912, 1, 78)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:49:57,598] Trial 26 finished with value: 0.023224663975198224 and parameters: {'look_back': 78, 'num_hidden_layers': 3, 'learning_rate': 0.0021407194711455476, 'batch_size': 124, 'epochs': 15, 'lstm_units_layer_0': 55, 'lstm_units_layer_1': 77, 'lstm_units_layer_2': 164}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17871, 1, 119)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:50:04,108] Trial 24 finished with value: 0.022930546540366685 and parameters: {'look_back': 92, 'num_hidden_layers': 4, 'learning_rate': 0.007051207217321356, 'batch_size': 140, 'epochs': 44, 'lstm_units_layer_0': 29, 'lstm_units_layer_1': 149, 'lstm_units_layer_2': 111, 'lstm_units_layer_3': 11}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17869, 1, 121)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 08:52:17,275] Trial 28 finished with value: 0.05147324810686622 and parameters: {'look_back': 121, 'num_hidden_layers': 8, 'learning_rate': 0.0072828442967766025, 'batch_size': 149, 'epochs': 43, 'lstm_units_layer_0': 12, 'lstm_units_layer_1': 59, 'lstm_units_layer_2': 93, 'lstm_units_layer_3': 11, 'lstm_units_layer_4': 39, 'lstm_units_layer_5': 245, 'lstm_units_layer_6': 32, 'lstm_units_layer_7': 178}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17838, 1, 152)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 08:52:35,827] Trial 27 finished with value: 0.0516275074555072 and parameters: {'look_back': 119, 'num_hidden_layers': 10, 'learning_rate': 0.0016898037883337, 'batch_size': 149, 'epochs': 42, 'lstm_units_layer_0': 27, 'lstm_units_layer_1': 120, 'lstm_units_layer_2': 115, 'lstm_units_layer_3': 12, 'lstm_units_layer_4': 40, 'lstm_units_layer_5': 175, 'lstm_units_layer_6': 26, 'lstm_units_layer_7': 223, 'lstm_units_layer_8': 154, 'lstm_units_layer_9': 62}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17839, 1, 151)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 08:54:39,884] Trial 29 finished with value: 0.052233289878056144 and parameters: {'look_back': 152, 'num_hidden_layers': 8, 'learning_rate': 0.015257056739197248, 'batch_size': 181, 'epochs': 59, 'lstm_units_layer_0': 60, 'lstm_units_layer_1': 122, 'lstm_units_layer_2': 191, 'lstm_units_layer_3': 13, 'lstm_units_layer_4': 23, 'lstm_units_layer_5': 160, 'lstm_units_layer_6': 27, 'lstm_units_layer_7': 8}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17912, 1, 78)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:54:59,274] Trial 30 finished with value: 0.05181576111614775 and parameters: {'look_back': 151, 'num_hidden_layers': 8, 'learning_rate': 0.01731979973937967, 'batch_size': 177, 'epochs': 60, 'lstm_units_layer_0': 64, 'lstm_units_layer_1': 177, 'lstm_units_layer_2': 177, 'lstm_units_layer_3': 24, 'lstm_units_layer_4': 25, 'lstm_units_layer_5': 30, 'lstm_units_layer_6': 8, 'lstm_units_layer_7': 9}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17908, 1, 82)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:55:58,474] Trial 32 finished with value: 0.02456969777682355 and parameters: {'look_back': 82, 'num_hidden_layers': 3, 'learning_rate': 0.0049562914838878775, 'batch_size': 102, 'epochs': 33, 'lstm_units_layer_0': 39, 'lstm_units_layer_1': 252, 'lstm_units_layer_2': 120}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17892, 1, 98)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:56:11,950] Trial 31 finished with value: 0.022683887978325233 and parameters: {'look_back': 78, 'num_hidden_layers': 3, 'learning_rate': 0.0006753028585771783, 'batch_size': 132, 'epochs': 48, 'lstm_units_layer_0': 45, 'lstm_units_layer_1': 251, 'lstm_units_layer_2': 63}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17939, 1, 51)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:56:20,953] Trial 33 finished with value: 0.02754531126081975 and parameters: {'look_back': 98, 'num_hidden_layers': 4, 'learning_rate': 0.0025826734191035858, 'batch_size': 133, 'epochs': 11, 'lstm_units_layer_0': 48, 'lstm_units_layer_1': 172, 'lstm_units_layer_2': 66, 'lstm_units_layer_3': 18}. Best is trial 20 with value: 0.022449853837961428.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17948, 1, 42)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:57:29,523] Trial 34 finished with value: 0.020961226430799063 and parameters: {'look_back': 51, 'num_hidden_layers': 2, 'learning_rate': 0.0007917710008288247, 'batch_size': 129, 'epochs': 68, 'lstm_units_layer_0': 44, 'lstm_units_layer_1': 251}. Best is trial 34 with value: 0.020961226430799063.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17965, 1, 25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:58:27,972] Trial 36 finished with value: 0.021976169924482366 and parameters: {'look_back': 25, 'num_hidden_layers': 1, 'learning_rate': 0.0005642727243695498, 'batch_size': 125, 'epochs': 67, 'lstm_units_layer_0': 42}. Best is trial 34 with value: 0.020961226430799063.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17972, 1, 18)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:58:31,491] Trial 35 finished with value: 0.021233999319831182 and parameters: {'look_back': 42, 'num_hidden_layers': 2, 'learning_rate': 0.0007778793338867903, 'batch_size': 51, 'epochs': 49, 'lstm_units_layer_0': 76, 'lstm_units_layer_1': 242}. Best is trial 34 with value: 0.020961226430799063.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17974, 1, 16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 08:59:54,583] Trial 37 finished with value: 0.022616896885110184 and parameters: {'look_back': 18, 'num_hidden_layers': 1, 'learning_rate': 0.0007591572513607567, 'batch_size': 118, 'epochs': 68, 'lstm_units_layer_0': 74}. Best is trial 34 with value: 0.020961226430799063.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17945, 1, 45)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 09:00:59,067] Trial 38 finished with value: 0.022594853989648597 and parameters: {'look_back': 16, 'num_hidden_layers': 1, 'learning_rate': 0.000725180998166061, 'batch_size': 40, 'epochs': 66, 'lstm_units_layer_0': 81}. Best is trial 34 with value: 0.020961226430799063.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17938, 1, 52)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 09:02:58,450] Trial 39 finished with value: 0.021016460515713285 and parameters: {'look_back': 45, 'num_hidden_layers': 2, 'learning_rate': 0.0013300438887867243, 'batch_size': 58, 'epochs': 81, 'lstm_units_layer_0': 81, 'lstm_units_layer_1': 212}. Best is trial 34 with value: 0.020961226430799063.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17939, 1, 51)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 09:06:29,855] Trial 41 finished with value: 0.020879777783223082 and parameters: {'look_back': 51, 'num_hidden_layers': 2, 'learning_rate': 0.001519335526892746, 'batch_size': 51, 'epochs': 82, 'lstm_units_layer_0': 96, 'lstm_units_layer_1': 208}. Best is trial 41 with value: 0.020879777783223082.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17948, 1, 42)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 09:11:56,485] Trial 42 finished with value: 0.021741501943172155 and parameters: {'look_back': 42, 'num_hidden_layers': 2, 'learning_rate': 0.0013959134807686392, 'batch_size': 35, 'epochs': 86, 'lstm_units_layer_0': 90, 'lstm_units_layer_1': 219}. Best is trial 41 with value: 0.020879777783223082.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17945, 1, 45)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 09:18:30,180] Trial 43 finished with value: 0.02151414514232202 and parameters: {'look_back': 45, 'num_hidden_layers': 2, 'learning_rate': 0.0012746994454457513, 'batch_size': 31, 'epochs': 91, 'lstm_units_layer_0': 97, 'lstm_units_layer_1': 206}. Best is trial 41 with value: 0.020879777783223082.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17929, 1, 61)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 09:38:55,278] Trial 40 finished with value: 0.020927749473356766 and parameters: {'look_back': 52, 'num_hidden_layers': 2, 'learning_rate': 0.0014493733776387472, 'batch_size': 5, 'epochs': 88, 'lstm_units_layer_0': 102, 'lstm_units_layer_1': 205}. Best is trial 41 with value: 0.020879777783223082.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17929, 1, 61)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 09:52:00,274] Trial 44 finished with value: 0.021200046694117725 and parameters: {'look_back': 61, 'num_hidden_layers': 2, 'learning_rate': 0.0012128011900191028, 'batch_size': 6, 'epochs': 91, 'lstm_units_layer_0': 109, 'lstm_units_layer_1': 196}. Best is trial 41 with value: 0.020879777783223082.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17928, 1, 62)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 10:03:02,172] Trial 45 finished with value: 0.022126599790061232 and parameters: {'look_back': 61, 'num_hidden_layers': 2, 'learning_rate': 0.0017330667086979347, 'batch_size': 7, 'epochs': 79, 'lstm_units_layer_0': 113, 'lstm_units_layer_1': 191}. Best is trial 41 with value: 0.020879777783223082.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17927, 1, 63)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 10:20:33,603] Trial 46 finished with value: 0.020970886239938775 and parameters: {'look_back': 62, 'num_hidden_layers': 2, 'learning_rate': 0.0019114625536680334, 'batch_size': 7, 'epochs': 91, 'lstm_units_layer_0': 106, 'lstm_units_layer_1': 198}. Best is trial 41 with value: 0.020879777783223082.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17956, 1, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 10:27:38,861] Trial 48 finished with value: 0.021720396441367702 and parameters: {'look_back': 34, 'num_hidden_layers': 1, 'learning_rate': 0.0016806971889211637, 'batch_size': 23, 'epochs': 99, 'lstm_units_layer_0': 153}. Best is trial 41 with value: 0.020879777783223082.\n",
            "<ipython-input-9-f3b7a5822bef>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17933, 1, 57)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 10:32:35,691] Trial 49 finished with value: 0.02111907703570876 and parameters: {'look_back': 57, 'num_hidden_layers': 3, 'learning_rate': 0.0009834583346548876, 'batch_size': 49, 'epochs': 86, 'lstm_units_layer_0': 149, 'lstm_units_layer_1': 174, 'lstm_units_layer_2': 41}. Best is trial 41 with value: 0.020879777783223082.\n",
            "[I 2023-09-05 10:36:04,854] Trial 47 finished with value: 0.025932435893827148 and parameters: {'look_back': 63, 'num_hidden_layers': 2, 'learning_rate': 0.0013656938055084647, 'batch_size': 6, 'epochs': 99, 'lstm_units_layer_0': 153, 'lstm_units_layer_1': 181}. Best is trial 41 with value: 0.020879777783223082.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "LSTM HyperParameters Tuning Elapsed Time : 9999.36385 seconds\n",
            "Done\n",
            "best_params: {'look_back': 51, 'num_hidden_layers': 2, 'learning_rate': 0.001519335526892746, 'batch_size': 51, 'epochs': 82, 'lstm_units_layer_0': 96, 'lstm_units_layer_1': 208}, best_error: 0.020879777783223082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 4\n",
        "\n",
        "# Hyper Parameters tuning\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "valid_size = int(len(train) * 0.1)\n",
        "temp_train = train[:-valid_size]\n",
        "valid = train[-valid_size:]\n",
        "print(len(train), len(temp_train), len(valid), len(test))\n",
        "\n",
        "def objective(trial):\n",
        "    # define search space for hyperparameters\n",
        "    look_back = trial.suggest_int('look_back', 5, 300)\n",
        "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 10)\n",
        "\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
        "    batch_size = trial.suggest_int('batch_size', 5, 300)\n",
        "    epochs = trial.suggest_int('epochs', 10, 100)\n",
        "\n",
        "    # reshape into X=t and Y=t+1\n",
        "    trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "    validX, validY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "    # reshape input to be [samples, time steps, features]\n",
        "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "    validX = np.reshape(validX, (validX.shape[0], 1, validX.shape[1]))\n",
        "    print(trainX.shape)\n",
        "\n",
        "    # Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "    model = Sequential()\n",
        "    for i in range(num_hidden_layers):\n",
        "        num_units = trial.suggest_int(f'lstm_units_layer_{i}', 8, 256, log=True)\n",
        "        return_sequences = (i < num_hidden_layers - 1)\n",
        "        model.add(LSTM(units=num_units, return_sequences=return_sequences))\n",
        "    model.add(Dense(horizon))\n",
        "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    validPredict = model.predict(validX, verbose=0)\n",
        "\n",
        "    # calculate root mean squared error\n",
        "    validScore = np.sqrt(mean_squared_error(validY, validPredict))\n",
        "\n",
        "    return validScore\n",
        "\n",
        "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "start_time = time.time()\n",
        "\n",
        "study.optimize(objective, n_trials=50, n_jobs=-1)\n",
        "\n",
        "# Record the ending time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print('done')\n",
        "print(\"LSTM HyperParameters Tuning Elapsed Time : %.5f\" % (elapsed_time), \"seconds\")\n",
        "\n",
        "best_params = study.best_params\n",
        "best_error = study.best_value\n",
        "\n",
        "print(\"Done\")\n",
        "print(f\"best_params: {best_params}, best_error: {best_error}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmDXxekmUV0v",
        "outputId": "a40f9c58-2de4-45d5-9fc6-55d319623109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 10:36:04,897] A new study created in memory with name: no-name-76106382-f830-4891-bbfd-4c5c6efb0a5a\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 16192 1799 2000\n",
            "(17770, 1, 218)(17959, 1, 29)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 10:39:15,479] Trial 0 finished with value: 0.036401587641882976 and parameters: {'look_back': 29, 'num_hidden_layers': 7, 'learning_rate': 2.0431540891277735e-05, 'batch_size': 109, 'epochs': 57, 'lstm_units_layer_0': 62, 'lstm_units_layer_1': 26, 'lstm_units_layer_2': 11, 'lstm_units_layer_3': 127, 'lstm_units_layer_4': 142, 'lstm_units_layer_5': 82, 'lstm_units_layer_6': 69}. Best is trial 0 with value: 0.036401587641882976.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17710, 1, 278)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 10:41:06,425] Trial 1 finished with value: 0.03304651642641758 and parameters: {'look_back': 218, 'num_hidden_layers': 10, 'learning_rate': 0.00013597717429592702, 'batch_size': 15, 'epochs': 10, 'lstm_units_layer_0': 12, 'lstm_units_layer_1': 9, 'lstm_units_layer_2': 14, 'lstm_units_layer_3': 13, 'lstm_units_layer_4': 179, 'lstm_units_layer_5': 113, 'lstm_units_layer_6': 11, 'lstm_units_layer_7': 147, 'lstm_units_layer_8': 48, 'lstm_units_layer_9': 40}. Best is trial 1 with value: 0.03304651642641758.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17855, 1, 133)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 10:41:58,882] Trial 3 finished with value: 0.0290439228572222 and parameters: {'look_back': 133, 'num_hidden_layers': 2, 'learning_rate': 0.002826036872747792, 'batch_size': 129, 'epochs': 43, 'lstm_units_layer_0': 154, 'lstm_units_layer_1': 66}. Best is trial 3 with value: 0.0290439228572222.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17744, 1, 244)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 10:46:31,397] Trial 2 finished with value: 0.030875390055788828 and parameters: {'look_back': 278, 'num_hidden_layers': 7, 'learning_rate': 0.0013946414425380818, 'batch_size': 69, 'epochs': 96, 'lstm_units_layer_0': 21, 'lstm_units_layer_1': 30, 'lstm_units_layer_2': 94, 'lstm_units_layer_3': 18, 'lstm_units_layer_4': 53, 'lstm_units_layer_5': 8, 'lstm_units_layer_6': 217}. Best is trial 3 with value: 0.0290439228572222.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17887, 1, 101)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 10:49:03,189] Trial 4 finished with value: 0.05404855278815561 and parameters: {'look_back': 244, 'num_hidden_layers': 10, 'learning_rate': 0.0505200308897128, 'batch_size': 96, 'epochs': 96, 'lstm_units_layer_0': 24, 'lstm_units_layer_1': 16, 'lstm_units_layer_2': 96, 'lstm_units_layer_3': 8, 'lstm_units_layer_4': 50, 'lstm_units_layer_5': 74, 'lstm_units_layer_6': 10, 'lstm_units_layer_7': 18, 'lstm_units_layer_8': 8, 'lstm_units_layer_9': 19}. Best is trial 3 with value: 0.0290439228572222.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17934, 1, 54)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 10:49:25,032] Trial 5 finished with value: 0.03127509822682527 and parameters: {'look_back': 101, 'num_hidden_layers': 10, 'learning_rate': 8.620778165479956e-05, 'batch_size': 206, 'epochs': 71, 'lstm_units_layer_0': 61, 'lstm_units_layer_1': 20, 'lstm_units_layer_2': 13, 'lstm_units_layer_3': 23, 'lstm_units_layer_4': 134, 'lstm_units_layer_5': 41, 'lstm_units_layer_6': 30, 'lstm_units_layer_7': 24, 'lstm_units_layer_8': 17, 'lstm_units_layer_9': 239}. Best is trial 3 with value: 0.0290439228572222.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17701, 1, 287)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 10:53:02,716] Trial 7 finished with value: 0.05231352249636951 and parameters: {'look_back': 287, 'num_hidden_layers': 10, 'learning_rate': 0.07295954934829989, 'batch_size': 85, 'epochs': 38, 'lstm_units_layer_0': 13, 'lstm_units_layer_1': 47, 'lstm_units_layer_2': 15, 'lstm_units_layer_3': 245, 'lstm_units_layer_4': 55, 'lstm_units_layer_5': 8, 'lstm_units_layer_6': 149, 'lstm_units_layer_7': 49, 'lstm_units_layer_8': 43, 'lstm_units_layer_9': 55}. Best is trial 3 with value: 0.0290439228572222.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17956, 1, 32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 10:53:03,451] Trial 6 finished with value: 0.0525150763342634 and parameters: {'look_back': 54, 'num_hidden_layers': 9, 'learning_rate': 0.016265879338864784, 'batch_size': 65, 'epochs': 36, 'lstm_units_layer_0': 10, 'lstm_units_layer_1': 17, 'lstm_units_layer_2': 55, 'lstm_units_layer_3': 29, 'lstm_units_layer_4': 227, 'lstm_units_layer_5': 40, 'lstm_units_layer_6': 15, 'lstm_units_layer_7': 104, 'lstm_units_layer_8': 145}. Best is trial 3 with value: 0.0290439228572222.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17796, 1, 192)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 10:54:26,081] Trial 8 finished with value: 0.03636009291622532 and parameters: {'look_back': 32, 'num_hidden_layers': 6, 'learning_rate': 0.020492751914426576, 'batch_size': 211, 'epochs': 43, 'lstm_units_layer_0': 17, 'lstm_units_layer_1': 90, 'lstm_units_layer_2': 115, 'lstm_units_layer_3': 127, 'lstm_units_layer_4': 13, 'lstm_units_layer_5': 104}. Best is trial 3 with value: 0.0290439228572222.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17837, 1, 151)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 10:58:54,047] Trial 10 finished with value: 0.031654736177665496 and parameters: {'look_back': 151, 'num_hidden_layers': 10, 'learning_rate': 9.295038810862889e-05, 'batch_size': 54, 'epochs': 31, 'lstm_units_layer_0': 22, 'lstm_units_layer_1': 30, 'lstm_units_layer_2': 81, 'lstm_units_layer_3': 56, 'lstm_units_layer_4': 238, 'lstm_units_layer_5': 17, 'lstm_units_layer_6': 66, 'lstm_units_layer_7': 29, 'lstm_units_layer_8': 17, 'lstm_units_layer_9': 23}. Best is trial 3 with value: 0.0290439228572222.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17814, 1, 174)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 10:59:23,506] Trial 11 finished with value: 0.028602642816584856 and parameters: {'look_back': 174, 'num_hidden_layers': 1, 'learning_rate': 0.002541862873984105, 'batch_size': 282, 'epochs': 66, 'lstm_units_layer_0': 212}. Best is trial 11 with value: 0.028602642816584856.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17819, 1, 169)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 10:59:53,404] Trial 12 finished with value: 0.029014884085338977 and parameters: {'look_back': 169, 'num_hidden_layers': 1, 'learning_rate': 0.002476491139406791, 'batch_size': 300, 'epochs': 71, 'lstm_units_layer_0': 233}. Best is trial 11 with value: 0.028602642816584856.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17789, 1, 199)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:00:38,842] Trial 13 finished with value: 0.02959530677255362 and parameters: {'look_back': 199, 'num_hidden_layers': 1, 'learning_rate': 0.004401187621638749, 'batch_size': 297, 'epochs': 77, 'lstm_units_layer_0': 241}. Best is trial 11 with value: 0.028602642816584856.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17801, 1, 187)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:01:33,199] Trial 14 finished with value: 0.029283097725037482 and parameters: {'look_back': 187, 'num_hidden_layers': 3, 'learning_rate': 0.0006560456814854699, 'batch_size': 300, 'epochs': 75, 'lstm_units_layer_0': 135, 'lstm_units_layer_1': 198, 'lstm_units_layer_2': 238}. Best is trial 11 with value: 0.028602642816584856.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17885, 1, 103)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:02:41,130] Trial 15 finished with value: 0.028574695662655684 and parameters: {'look_back': 103, 'num_hidden_layers': 4, 'learning_rate': 0.0006059970803515718, 'batch_size': 249, 'epochs': 64, 'lstm_units_layer_0': 235, 'lstm_units_layer_1': 241, 'lstm_units_layer_2': 25, 'lstm_units_layer_3': 48}. Best is trial 15 with value: 0.028574695662655684.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17897, 1, 91)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:03:50,304] Trial 16 finished with value: 0.028870621508702433 and parameters: {'look_back': 91, 'num_hidden_layers': 4, 'learning_rate': 0.0004800589747529588, 'batch_size': 248, 'epochs': 63, 'lstm_units_layer_0': 107, 'lstm_units_layer_1': 250, 'lstm_units_layer_2': 26, 'lstm_units_layer_3': 46}. Best is trial 15 with value: 0.028574695662655684.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17877, 1, 111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:05:18,024] Trial 17 finished with value: 0.02821586030007657 and parameters: {'look_back': 111, 'num_hidden_layers': 4, 'learning_rate': 0.007177032470346985, 'batch_size': 247, 'epochs': 84, 'lstm_units_layer_0': 97, 'lstm_units_layer_1': 158, 'lstm_units_layer_2': 29, 'lstm_units_layer_3': 69}. Best is trial 17 with value: 0.02821586030007657.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17891, 1, 97)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:06:57,058] Trial 9 finished with value: 0.03256784605021801 and parameters: {'look_back': 192, 'num_hidden_layers': 8, 'learning_rate': 5.658901986292581e-05, 'batch_size': 40, 'epochs': 100, 'lstm_units_layer_0': 158, 'lstm_units_layer_1': 17, 'lstm_units_layer_2': 17, 'lstm_units_layer_3': 127, 'lstm_units_layer_4': 101, 'lstm_units_layer_5': 11, 'lstm_units_layer_6': 14, 'lstm_units_layer_7': 67}. Best is trial 17 with value: 0.02821586030007657.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17888, 1, 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:07:06,669] Trial 18 finished with value: 0.02716251930629508 and parameters: {'look_back': 97, 'num_hidden_layers': 4, 'learning_rate': 0.007788571680391916, 'batch_size': 181, 'epochs': 84, 'lstm_units_layer_0': 84, 'lstm_units_layer_1': 144, 'lstm_units_layer_2': 27, 'lstm_units_layer_3': 68}. Best is trial 18 with value: 0.02716251930629508.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17915, 1, 73)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 11:08:52,421] Trial 19 finished with value: 0.029506584130350417 and parameters: {'look_back': 100, 'num_hidden_layers': 4, 'learning_rate': 0.0003219969691105734, 'batch_size': 175, 'epochs': 86, 'lstm_units_layer_0': 93, 'lstm_units_layer_1': 149, 'lstm_units_layer_2': 25, 'lstm_units_layer_3': 65}. Best is trial 18 with value: 0.02716251930629508.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17925, 1, 63)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:09:26,754] Trial 20 finished with value: 0.02796386846049875 and parameters: {'look_back': 73, 'num_hidden_layers': 5, 'learning_rate': 0.010923540737289121, 'batch_size': 164, 'epochs': 87, 'lstm_units_layer_0': 92, 'lstm_units_layer_1': 128, 'lstm_units_layer_2': 37, 'lstm_units_layer_3': 67, 'lstm_units_layer_4': 9}. Best is trial 18 with value: 0.02716251930629508.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17908, 1, 80)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:11:12,167] Trial 21 finished with value: 0.02704734002912457 and parameters: {'look_back': 63, 'num_hidden_layers': 5, 'learning_rate': 0.008189974916828852, 'batch_size': 165, 'epochs': 85, 'lstm_units_layer_0': 35, 'lstm_units_layer_1': 112, 'lstm_units_layer_2': 38, 'lstm_units_layer_3': 75, 'lstm_units_layer_4': 8}. Best is trial 21 with value: 0.02704734002912457.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17925, 1, 63)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:11:42,546] Trial 22 finished with value: 0.029855317677404312 and parameters: {'look_back': 80, 'num_hidden_layers': 5, 'learning_rate': 0.006973884712129682, 'batch_size': 166, 'epochs': 86, 'lstm_units_layer_0': 80, 'lstm_units_layer_1': 125, 'lstm_units_layer_2': 33, 'lstm_units_layer_3': 75, 'lstm_units_layer_4': 9}. Best is trial 21 with value: 0.02704734002912457.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17982, 1, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:13:52,045] Trial 23 finished with value: 0.02711454055464425 and parameters: {'look_back': 63, 'num_hidden_layers': 5, 'learning_rate': 0.009054615165283723, 'batch_size': 155, 'epochs': 85, 'lstm_units_layer_0': 38, 'lstm_units_layer_1': 99, 'lstm_units_layer_2': 39, 'lstm_units_layer_3': 80, 'lstm_units_layer_4': 8}. Best is trial 21 with value: 0.02704734002912457.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17980, 1, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 11:14:36,081] Trial 24 finished with value: 0.03198378328433407 and parameters: {'look_back': 6, 'num_hidden_layers': 6, 'learning_rate': 0.017355802990766545, 'batch_size': 144, 'epochs': 88, 'lstm_units_layer_0': 37, 'lstm_units_layer_1': 103, 'lstm_units_layer_2': 8, 'lstm_units_layer_3': 36, 'lstm_units_layer_4': 16, 'lstm_units_layer_5': 247}. Best is trial 21 with value: 0.02704734002912457.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17932, 1, 56)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:15:58,177] Trial 26 finished with value: 0.02686025211281613 and parameters: {'look_back': 56, 'num_hidden_layers': 3, 'learning_rate': 0.0056240033221932485, 'batch_size': 193, 'epochs': 78, 'lstm_units_layer_0': 37, 'lstm_units_layer_1': 82, 'lstm_units_layer_2': 49}. Best is trial 26 with value: 0.02686025211281613.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17928, 1, 60)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:16:48,074] Trial 25 finished with value: 0.03399414493013098 and parameters: {'look_back': 8, 'num_hidden_layers': 6, 'learning_rate': 0.03441932919871872, 'batch_size': 136, 'epochs': 80, 'lstm_units_layer_0': 36, 'lstm_units_layer_1': 87, 'lstm_units_layer_2': 49, 'lstm_units_layer_3': 32, 'lstm_units_layer_4': 17, 'lstm_units_layer_5': 254}. Best is trial 26 with value: 0.02686025211281613.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17933, 1, 55)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 11:16:52,176] Trial 27 finished with value: 0.02909870248957507 and parameters: {'look_back': 60, 'num_hidden_layers': 3, 'learning_rate': 0.03383266226383165, 'batch_size': 202, 'epochs': 51, 'lstm_units_layer_0': 34, 'lstm_units_layer_1': 78, 'lstm_units_layer_2': 48}. Best is trial 26 with value: 0.02686025211281613.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17955, 1, 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 11:17:48,398] Trial 28 finished with value: 0.05241592987372683 and parameters: {'look_back': 55, 'num_hidden_layers': 3, 'learning_rate': 0.0912349521660902, 'batch_size': 193, 'epochs': 55, 'lstm_units_layer_0': 31, 'lstm_units_layer_1': 68, 'lstm_units_layer_2': 57}. Best is trial 26 with value: 0.02686025211281613.\n",
            "<ipython-input-10-3656848860d2>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17861, 1, 127)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 6\n",
        "\n",
        "# Hyper Parameters tuning\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "valid_size = int(len(train) * 0.1)\n",
        "temp_train = train[:-valid_size]\n",
        "valid = train[-valid_size:]\n",
        "print(len(train), len(temp_train), len(valid), len(test))\n",
        "\n",
        "def objective(trial):\n",
        "    # define search space for hyperparameters\n",
        "    look_back = trial.suggest_int('look_back', 5, 300)\n",
        "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 10)\n",
        "\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
        "    batch_size = trial.suggest_int('batch_size', 5, 300)\n",
        "    epochs = trial.suggest_int('epochs', 10, 100)\n",
        "\n",
        "    # reshape into X=t and Y=t+1\n",
        "    trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "    validX, validY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "    # reshape input to be [samples, time steps, features]\n",
        "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "    validX = np.reshape(validX, (validX.shape[0], 1, validX.shape[1]))\n",
        "    print(trainX.shape)\n",
        "\n",
        "    # Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "    model = Sequential()\n",
        "    for i in range(num_hidden_layers):\n",
        "        num_units = trial.suggest_int(f'lstm_units_layer_{i}', 8, 256, log=True)\n",
        "        return_sequences = (i < num_hidden_layers - 1)\n",
        "        model.add(LSTM(units=num_units, return_sequences=return_sequences))\n",
        "    model.add(Dense(horizon))\n",
        "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    validPredict = model.predict(validX, verbose=0)\n",
        "\n",
        "    # calculate root mean squared error\n",
        "    validScore = np.sqrt(mean_squared_error(validY, validPredict))\n",
        "\n",
        "    return validScore\n",
        "\n",
        "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "start_time = time.time()\n",
        "\n",
        "study.optimize(objective, n_trials=30, n_jobs=-1)\n",
        "\n",
        "# Record the ending time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print('done')\n",
        "print(\"LSTM HyperParameters Tuning Elapsed Time : %.5f\" % (elapsed_time), \"seconds\")\n",
        "\n",
        "best_params = study.best_params\n",
        "best_error = study.best_value\n",
        "\n",
        "print(\"Done\")\n",
        "print(f\"best_params: {best_params}, best_error: {best_error}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtAUxAxMUXyw",
        "outputId": "0af6bb48-4fa9-488c-d464-99535acc223c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 12:22:41,542] A new study created in memory with name: no-name-d052f972-fc21-4b9c-87eb-05b08c061fdb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 16192 1799 2000\n",
            "(17741, 1, 245)\n",
            "(17754, 1, 232)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 12:23:21,457] Trial 0 finished with value: 0.04374347727548258 and parameters: {'look_back': 245, 'num_hidden_layers': 1, 'learning_rate': 9.600928263181503e-05, 'batch_size': 278, 'epochs': 74, 'lstm_units_layer_0': 22}. Best is trial 0 with value: 0.04374347727548258.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17960, 1, 26)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 12:23:35,366] Trial 1 finished with value: 0.04641520316491024 and parameters: {'look_back': 232, 'num_hidden_layers': 2, 'learning_rate': 7.087517373904725e-05, 'batch_size': 207, 'epochs': 24, 'lstm_units_layer_0': 169, 'lstm_units_layer_1': 38}. Best is trial 0 with value: 0.04374347727548258.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17729, 1, 257)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 12:25:02,326] Trial 2 finished with value: 0.03520079684511613 and parameters: {'look_back': 26, 'num_hidden_layers': 5, 'learning_rate': 0.00049300787909773, 'batch_size': 241, 'epochs': 96, 'lstm_units_layer_0': 45, 'lstm_units_layer_1': 17, 'lstm_units_layer_2': 19, 'lstm_units_layer_3': 46, 'lstm_units_layer_4': 8}. Best is trial 2 with value: 0.03520079684511613.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17948, 1, 38)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 12:27:03,927] Trial 3 finished with value: 0.05223471232112049 and parameters: {'look_back': 257, 'num_hidden_layers': 8, 'learning_rate': 3.476517671816912e-05, 'batch_size': 296, 'epochs': 81, 'lstm_units_layer_0': 115, 'lstm_units_layer_1': 82, 'lstm_units_layer_2': 42, 'lstm_units_layer_3': 121, 'lstm_units_layer_4': 25, 'lstm_units_layer_5': 43, 'lstm_units_layer_6': 10, 'lstm_units_layer_7': 110}. Best is trial 2 with value: 0.03520079684511613.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17965, 1, 21)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 12:28:33,725] Trial 4 finished with value: 0.03474802914714052 and parameters: {'look_back': 38, 'num_hidden_layers': 7, 'learning_rate': 0.00025108144551751335, 'batch_size': 137, 'epochs': 55, 'lstm_units_layer_0': 163, 'lstm_units_layer_1': 31, 'lstm_units_layer_2': 82, 'lstm_units_layer_3': 171, 'lstm_units_layer_4': 16, 'lstm_units_layer_5': 95, 'lstm_units_layer_6': 8}. Best is trial 4 with value: 0.03474802914714052.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17868, 1, 118)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 12:29:21,221] Trial 6 finished with value: 0.051480604319046164 and parameters: {'look_back': 118, 'num_hidden_layers': 2, 'learning_rate': 1.806213434059755e-05, 'batch_size': 208, 'epochs': 75, 'lstm_units_layer_0': 9, 'lstm_units_layer_1': 33}. Best is trial 4 with value: 0.03474802914714052.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17895, 1, 91)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 12:30:23,746] Trial 5 finished with value: 0.05312312326340334 and parameters: {'look_back': 21, 'num_hidden_layers': 9, 'learning_rate': 0.012716956141657762, 'batch_size': 75, 'epochs': 27, 'lstm_units_layer_0': 139, 'lstm_units_layer_1': 40, 'lstm_units_layer_2': 142, 'lstm_units_layer_3': 27, 'lstm_units_layer_4': 27, 'lstm_units_layer_5': 109, 'lstm_units_layer_6': 196, 'lstm_units_layer_7': 45, 'lstm_units_layer_8': 57}. Best is trial 4 with value: 0.03474802914714052.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17831, 1, 155)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 12:34:26,509] Trial 7 finished with value: 0.03235792738580053 and parameters: {'look_back': 91, 'num_hidden_layers': 6, 'learning_rate': 0.0005124476844228014, 'batch_size': 50, 'epochs': 73, 'lstm_units_layer_0': 50, 'lstm_units_layer_1': 28, 'lstm_units_layer_2': 82, 'lstm_units_layer_3': 33, 'lstm_units_layer_4': 22, 'lstm_units_layer_5': 15}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17803, 1, 183)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 12:35:14,589] Trial 8 finished with value: 0.052027078143983524 and parameters: {'look_back': 155, 'num_hidden_layers': 9, 'learning_rate': 0.02862755155069649, 'batch_size': 176, 'epochs': 77, 'lstm_units_layer_0': 11, 'lstm_units_layer_1': 183, 'lstm_units_layer_2': 110, 'lstm_units_layer_3': 17, 'lstm_units_layer_4': 103, 'lstm_units_layer_5': 171, 'lstm_units_layer_6': 21, 'lstm_units_layer_7': 10, 'lstm_units_layer_8': 31}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17715, 1, 271)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 12:35:46,276] Trial 10 finished with value: 0.05221671934119106 and parameters: {'look_back': 271, 'num_hidden_layers': 2, 'learning_rate': 0.07252565014764073, 'batch_size': 273, 'epochs': 65, 'lstm_units_layer_0': 27, 'lstm_units_layer_1': 8}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17878, 1, 108)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 12:36:29,796] Trial 9 finished with value: 0.03766997440579126 and parameters: {'look_back': 183, 'num_hidden_layers': 4, 'learning_rate': 9.090818023045428e-05, 'batch_size': 238, 'epochs': 75, 'lstm_units_layer_0': 140, 'lstm_units_layer_1': 107, 'lstm_units_layer_2': 20, 'lstm_units_layer_3': 16}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17915, 1, 71)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 12:53:39,590] Trial 12 finished with value: 0.03352365682684349 and parameters: {'look_back': 71, 'num_hidden_layers': 7, 'learning_rate': 0.0010890761554213412, 'batch_size': 10, 'epochs': 47, 'lstm_units_layer_0': 65, 'lstm_units_layer_1': 22, 'lstm_units_layer_2': 88, 'lstm_units_layer_3': 224, 'lstm_units_layer_4': 14, 'lstm_units_layer_5': 11, 'lstm_units_layer_6': 8}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17902, 1, 84)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 12:59:07,911] Trial 11 finished with value: 0.034380770574326446 and parameters: {'look_back': 108, 'num_hidden_layers': 5, 'learning_rate': 0.001800897465663111, 'batch_size': 5, 'epochs': 49, 'lstm_units_layer_0': 69, 'lstm_units_layer_1': 95, 'lstm_units_layer_2': 8, 'lstm_units_layer_3': 8, 'lstm_units_layer_4': 77}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17909, 1, 77)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:08:18,470] Trial 13 finished with value: 0.03406309209994054 and parameters: {'look_back': 84, 'num_hidden_layers': 6, 'learning_rate': 0.0018408284978712094, 'batch_size': 11, 'epochs': 40, 'lstm_units_layer_0': 68, 'lstm_units_layer_1': 16, 'lstm_units_layer_2': 256, 'lstm_units_layer_3': 69, 'lstm_units_layer_4': 60, 'lstm_units_layer_5': 10}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17915, 1, 71)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:13:05,173] Trial 15 finished with value: 0.034740557342639804 and parameters: {'look_back': 71, 'num_hidden_layers': 7, 'learning_rate': 0.004159539540213323, 'batch_size': 62, 'epochs': 39, 'lstm_units_layer_0': 72, 'lstm_units_layer_1': 20, 'lstm_units_layer_2': 63, 'lstm_units_layer_3': 238, 'lstm_units_layer_4': 228, 'lstm_units_layer_5': 10, 'lstm_units_layer_6': 48}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17921, 1, 65)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:14:17,156] Trial 16 finished with value: 0.035067129226149324 and parameters: {'look_back': 65, 'num_hidden_layers': 4, 'learning_rate': 0.00048792397791249005, 'batch_size': 61, 'epochs': 14, 'lstm_units_layer_0': 250, 'lstm_units_layer_1': 58, 'lstm_units_layer_2': 46, 'lstm_units_layer_3': 90}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17849, 1, 137)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:22:16,517] Trial 17 finished with value: 0.05167431700009798 and parameters: {'look_back': 137, 'num_hidden_layers': 10, 'learning_rate': 0.0037942936175813075, 'batch_size': 112, 'epochs': 94, 'lstm_units_layer_0': 40, 'lstm_units_layer_1': 24, 'lstm_units_layer_2': 149, 'lstm_units_layer_3': 67, 'lstm_units_layer_4': 11, 'lstm_units_layer_5': 19, 'lstm_units_layer_6': 29, 'lstm_units_layer_7': 253, 'lstm_units_layer_8': 8, 'lstm_units_layer_9': 43}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17786, 1, 200)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:29:27,010] Trial 18 finished with value: 0.0343257153620793 and parameters: {'look_back': 200, 'num_hidden_layers': 7, 'learning_rate': 0.0004499350282236412, 'batch_size': 46, 'epochs': 63, 'lstm_units_layer_0': 89, 'lstm_units_layer_1': 12, 'lstm_units_layer_2': 79, 'lstm_units_layer_3': 228, 'lstm_units_layer_4': 17, 'lstm_units_layer_5': 19, 'lstm_units_layer_6': 8}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17894, 1, 92)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:32:49,681] Trial 19 finished with value: 0.03657856163875877 and parameters: {'look_back': 92, 'num_hidden_layers': 6, 'learning_rate': 0.0009744914945241962, 'batch_size': 108, 'epochs': 48, 'lstm_units_layer_0': 54, 'lstm_units_layer_1': 54, 'lstm_units_layer_2': 225, 'lstm_units_layer_3': 133, 'lstm_units_layer_4': 37, 'lstm_units_layer_5': 8}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17935, 1, 51)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:33:32,897] Trial 14 finished with value: 0.03482678788561545 and parameters: {'look_back': 77, 'num_hidden_layers': 7, 'learning_rate': 0.001960004866046546, 'batch_size': 7, 'epochs': 41, 'lstm_units_layer_0': 69, 'lstm_units_layer_1': 18, 'lstm_units_layer_2': 245, 'lstm_units_layer_3': 234, 'lstm_units_layer_4': 208, 'lstm_units_layer_5': 9, 'lstm_units_layer_6': 44}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17686, 1, 300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:38:16,410] Trial 20 finished with value: 0.03312528375929147 and parameters: {'look_back': 51, 'num_hidden_layers': 4, 'learning_rate': 0.0001955889606990217, 'batch_size': 27, 'epochs': 63, 'lstm_units_layer_0': 36, 'lstm_units_layer_1': 25, 'lstm_units_layer_2': 115, 'lstm_units_layer_3': 40}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17938, 1, 48)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 13:39:35,466] Trial 21 finished with value: 0.033717806737039384 and parameters: {'look_back': 300, 'num_hidden_layers': 4, 'learning_rate': 0.00023126058562826987, 'batch_size': 36, 'epochs': 89, 'lstm_units_layer_0': 31, 'lstm_units_layer_1': 27, 'lstm_units_layer_2': 114, 'lstm_units_layer_3': 43}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17927, 1, 59)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:41:10,658] Trial 23 finished with value: 0.03363857278188205 and parameters: {'look_back': 59, 'num_hidden_layers': 3, 'learning_rate': 0.000214244318918904, 'batch_size': 86, 'epochs': 62, 'lstm_units_layer_0': 42, 'lstm_units_layer_1': 26, 'lstm_units_layer_2': 85}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17978, 1, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:43:07,067] Trial 22 finished with value: 0.03409969517268982 and parameters: {'look_back': 48, 'num_hidden_layers': 4, 'learning_rate': 0.0001801321344357591, 'batch_size': 31, 'epochs': 61, 'lstm_units_layer_0': 32, 'lstm_units_layer_1': 31, 'lstm_units_layer_2': 109, 'lstm_units_layer_3': 38}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17978, 1, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:46:06,622] Trial 24 finished with value: 0.03533411363297896 and parameters: {'look_back': 8, 'num_hidden_layers': 5, 'learning_rate': 0.0008128449874552114, 'batch_size': 34, 'epochs': 56, 'lstm_units_layer_0': 19, 'lstm_units_layer_1': 14, 'lstm_units_layer_2': 151, 'lstm_units_layer_3': 37, 'lstm_units_layer_4': 15}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17873, 1, 113)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:46:30,676] Trial 25 finished with value: 0.036021475722549254 and parameters: {'look_back': 8, 'num_hidden_layers': 6, 'learning_rate': 0.0007670626246863704, 'batch_size': 94, 'epochs': 68, 'lstm_units_layer_0': 20, 'lstm_units_layer_1': 14, 'lstm_units_layer_2': 151, 'lstm_units_layer_3': 66, 'lstm_units_layer_4': 15, 'lstm_units_layer_5': 22}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17873, 1, 113)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:49:43,640] Trial 26 finished with value: 0.051966979774275605 and parameters: {'look_back': 113, 'num_hidden_layers': 6, 'learning_rate': 1.1464731108725592e-05, 'batch_size': 99, 'epochs': 85, 'lstm_units_layer_0': 53, 'lstm_units_layer_1': 46, 'lstm_units_layer_2': 58, 'lstm_units_layer_3': 63, 'lstm_units_layer_4': 8, 'lstm_units_layer_5': 19}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17844, 1, 142)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:49:52,455] Trial 27 finished with value: 0.039891745896084845 and parameters: {'look_back': 113, 'num_hidden_layers': 8, 'learning_rate': 4.179702941733285e-05, 'batch_size': 155, 'epochs': 84, 'lstm_units_layer_0': 53, 'lstm_units_layer_1': 24, 'lstm_units_layer_2': 58, 'lstm_units_layer_3': 100, 'lstm_units_layer_4': 9, 'lstm_units_layer_5': 30, 'lstm_units_layer_6': 16, 'lstm_units_layer_7': 8}. Best is trial 7 with value: 0.03235792738580053.\n",
            "<ipython-input-9-51f619b15016>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17839, 1, 147)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 13:52:06,989] Trial 29 finished with value: 0.03660628651507756 and parameters: {'look_back': 147, 'num_hidden_layers': 3, 'learning_rate': 0.00037981574617246614, 'batch_size': 52, 'epochs': 48, 'lstm_units_layer_0': 85, 'lstm_units_layer_1': 11, 'lstm_units_layer_2': 91}. Best is trial 7 with value: 0.03235792738580053.\n",
            "[I 2023-09-05 13:52:14,073] Trial 28 finished with value: 0.05184725187415518 and parameters: {'look_back': 142, 'num_hidden_layers': 8, 'learning_rate': 4.6509611864338884e-05, 'batch_size': 142, 'epochs': 48, 'lstm_units_layer_0': 94, 'lstm_units_layer_1': 23, 'lstm_units_layer_2': 63, 'lstm_units_layer_3': 117, 'lstm_units_layer_4': 32, 'lstm_units_layer_5': 36, 'lstm_units_layer_6': 16, 'lstm_units_layer_7': 8}. Best is trial 7 with value: 0.03235792738580053.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "LSTM HyperParameters Tuning Elapsed Time : 5372.53182 seconds\n",
            "Done\n",
            "best_params: {'look_back': 91, 'num_hidden_layers': 6, 'learning_rate': 0.0005124476844228014, 'batch_size': 50, 'epochs': 73, 'lstm_units_layer_0': 50, 'lstm_units_layer_1': 28, 'lstm_units_layer_2': 82, 'lstm_units_layer_3': 33, 'lstm_units_layer_4': 22, 'lstm_units_layer_5': 15}, best_error: 0.03235792738580053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 8\n",
        "\n",
        "# Hyper Parameters tuning\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "valid_size = int(len(train) * 0.1)\n",
        "temp_train = train[:-valid_size]\n",
        "valid = train[-valid_size:]\n",
        "print(len(train), len(temp_train), len(valid), len(test))\n",
        "\n",
        "def objective(trial):\n",
        "    # define search space for hyperparameters\n",
        "    look_back = trial.suggest_int('look_back', 5, 300)\n",
        "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 10)\n",
        "\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
        "    batch_size = trial.suggest_int('batch_size', 5, 300)\n",
        "    epochs = trial.suggest_int('epochs', 10, 100)\n",
        "\n",
        "    # reshape into X=t and Y=t+1\n",
        "    trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "    validX, validY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "    # reshape input to be [samples, time steps, features]\n",
        "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "    validX = np.reshape(validX, (validX.shape[0], 1, validX.shape[1]))\n",
        "    print(trainX.shape)\n",
        "\n",
        "    # Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "    model = Sequential()\n",
        "    for i in range(num_hidden_layers):\n",
        "        num_units = trial.suggest_int(f'lstm_units_layer_{i}', 8, 256, log=True)\n",
        "        return_sequences = (i < num_hidden_layers - 1)\n",
        "        model.add(LSTM(units=num_units, return_sequences=return_sequences))\n",
        "    model.add(Dense(horizon))\n",
        "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    validPredict = model.predict(validX, verbose=0)\n",
        "\n",
        "    # calculate root mean squared error\n",
        "    validScore = np.sqrt(mean_squared_error(validY, validPredict))\n",
        "\n",
        "    return validScore\n",
        "\n",
        "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "start_time = time.time()\n",
        "\n",
        "study.optimize(objective, n_trials=30, n_jobs=-1)\n",
        "\n",
        "# Record the ending time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print('done')\n",
        "print(\"LSTM HyperParameters Tuning Elapsed Time : %.5f\" % (elapsed_time), \"seconds\")\n",
        "\n",
        "best_params = study.best_params\n",
        "best_error = study.best_value\n",
        "\n",
        "print(\"Done\")\n",
        "print(f\"best_params: {best_params}, best_error: {best_error}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCLJcJIJUcfd",
        "outputId": "505d387a-d4b6-4343-e4c5-bc8b4e786c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:52:14,106] A new study created in memory with name: no-name-a4a20d05-f317-4b92-b047-c37579da0655\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 16192 1799 2000\n",
            "(17964, 1, 20)\n",
            "(17826, 1, 158)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:53:15,349] Trial 1 finished with value: 0.0386272033080371 and parameters: {'look_back': 20, 'num_hidden_layers': 4, 'learning_rate': 0.05522253569453423, 'batch_size': 219, 'epochs': 58, 'lstm_units_layer_0': 25, 'lstm_units_layer_1': 14, 'lstm_units_layer_2': 56, 'lstm_units_layer_3': 17}. Best is trial 1 with value: 0.0386272033080371.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17739, 1, 245)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:53:28,378] Trial 0 finished with value: 0.03907573793513375 and parameters: {'look_back': 158, 'num_hidden_layers': 1, 'learning_rate': 5.018014771015471e-05, 'batch_size': 75, 'epochs': 53, 'lstm_units_layer_0': 140}. Best is trial 1 with value: 0.0386272033080371.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17890, 1, 94)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:55:50,425] Trial 2 finished with value: 0.03997434153831244 and parameters: {'look_back': 245, 'num_hidden_layers': 6, 'learning_rate': 0.0021666788598993392, 'batch_size': 300, 'epochs': 76, 'lstm_units_layer_0': 32, 'lstm_units_layer_1': 144, 'lstm_units_layer_2': 106, 'lstm_units_layer_3': 30, 'lstm_units_layer_4': 24, 'lstm_units_layer_5': 19}. Best is trial 1 with value: 0.0386272033080371.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17746, 1, 238)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 13:59:27,239] Trial 3 finished with value: 0.056675245947000324 and parameters: {'look_back': 94, 'num_hidden_layers': 5, 'learning_rate': 0.07895880099337778, 'batch_size': 177, 'epochs': 96, 'lstm_units_layer_0': 216, 'lstm_units_layer_1': 206, 'lstm_units_layer_2': 12, 'lstm_units_layer_3': 48, 'lstm_units_layer_4': 232}. Best is trial 1 with value: 0.0386272033080371.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17891, 1, 93)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:00:17,240] Trial 4 finished with value: 0.03891434501520577 and parameters: {'look_back': 238, 'num_hidden_layers': 8, 'learning_rate': 0.00039790275969453644, 'batch_size': 175, 'epochs': 92, 'lstm_units_layer_0': 46, 'lstm_units_layer_1': 24, 'lstm_units_layer_2': 92, 'lstm_units_layer_3': 28, 'lstm_units_layer_4': 92, 'lstm_units_layer_5': 51, 'lstm_units_layer_6': 14, 'lstm_units_layer_7': 22}. Best is trial 1 with value: 0.0386272033080371.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17847, 1, 137)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:00:56,617] Trial 5 finished with value: 0.03800370993082163 and parameters: {'look_back': 93, 'num_hidden_layers': 2, 'learning_rate': 0.0001199629963821474, 'batch_size': 45, 'epochs': 27, 'lstm_units_layer_0': 66, 'lstm_units_layer_1': 188}. Best is trial 5 with value: 0.03800370993082163.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17868, 1, 116)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:02:06,116] Trial 6 finished with value: 0.052274726588182434 and parameters: {'look_back': 137, 'num_hidden_layers': 7, 'learning_rate': 3.4037103007218775e-05, 'batch_size': 245, 'epochs': 31, 'lstm_units_layer_0': 124, 'lstm_units_layer_1': 140, 'lstm_units_layer_2': 119, 'lstm_units_layer_3': 61, 'lstm_units_layer_4': 9, 'lstm_units_layer_5': 123, 'lstm_units_layer_6': 9}. Best is trial 5 with value: 0.03800370993082163.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17966, 1, 18)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:05:14,876] Trial 7 finished with value: 0.05271722160446241 and parameters: {'look_back': 116, 'num_hidden_layers': 10, 'learning_rate': 0.004477050350011086, 'batch_size': 288, 'epochs': 59, 'lstm_units_layer_0': 126, 'lstm_units_layer_1': 12, 'lstm_units_layer_2': 107, 'lstm_units_layer_3': 96, 'lstm_units_layer_4': 8, 'lstm_units_layer_5': 159, 'lstm_units_layer_6': 109, 'lstm_units_layer_7': 210, 'lstm_units_layer_8': 72, 'lstm_units_layer_9': 22}. Best is trial 5 with value: 0.03800370993082163.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17877, 1, 107)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:05:52,740] Trial 9 finished with value: 0.21308454450138456 and parameters: {'look_back': 107, 'num_hidden_layers': 5, 'learning_rate': 2.3939203444211915e-05, 'batch_size': 194, 'epochs': 13, 'lstm_units_layer_0': 74, 'lstm_units_layer_1': 121, 'lstm_units_layer_2': 9, 'lstm_units_layer_3': 9, 'lstm_units_layer_4': 15}. Best is trial 5 with value: 0.03800370993082163.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17941, 1, 43)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:07:01,625] Trial 10 finished with value: 0.03756443747777241 and parameters: {'look_back': 43, 'num_hidden_layers': 1, 'learning_rate': 0.001077960229002474, 'batch_size': 80, 'epochs': 76, 'lstm_units_layer_0': 76}. Best is trial 10 with value: 0.03756443747777241.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17970, 1, 14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:10:03,560] Trial 11 finished with value: 0.03730568702775874 and parameters: {'look_back': 14, 'num_hidden_layers': 3, 'learning_rate': 0.0005284739397558105, 'batch_size': 104, 'epochs': 78, 'lstm_units_layer_0': 12, 'lstm_units_layer_1': 53, 'lstm_units_layer_2': 210}. Best is trial 11 with value: 0.03730568702775874.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17976, 1, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:10:54,141] Trial 8 finished with value: 0.052042937785578725 and parameters: {'look_back': 18, 'num_hidden_layers': 9, 'learning_rate': 0.012535154991702695, 'batch_size': 143, 'epochs': 86, 'lstm_units_layer_0': 186, 'lstm_units_layer_1': 226, 'lstm_units_layer_2': 57, 'lstm_units_layer_3': 48, 'lstm_units_layer_4': 216, 'lstm_units_layer_5': 17, 'lstm_units_layer_6': 18, 'lstm_units_layer_7': 8, 'lstm_units_layer_8': 219}. Best is trial 11 with value: 0.03730568702775874.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17932, 1, 52)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:13:08,760] Trial 12 finished with value: 0.038953541136323065 and parameters: {'look_back': 8, 'num_hidden_layers': 3, 'learning_rate': 0.0005030249037460174, 'batch_size': 100, 'epochs': 76, 'lstm_units_layer_0': 8, 'lstm_units_layer_1': 54, 'lstm_units_layer_2': 223}. Best is trial 11 with value: 0.03730568702775874.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17924, 1, 60)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:14:05,136] Trial 13 finished with value: 0.037405424293558275 and parameters: {'look_back': 52, 'num_hidden_layers': 3, 'learning_rate': 0.0004709576490499541, 'batch_size': 107, 'epochs': 74, 'lstm_units_layer_0': 8, 'lstm_units_layer_1': 64, 'lstm_units_layer_2': 253}. Best is trial 11 with value: 0.03730568702775874.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17919, 1, 65)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:15:10,658] Trial 14 finished with value: 0.03693483360729994 and parameters: {'look_back': 60, 'num_hidden_layers': 1, 'learning_rate': 0.0009156400281163598, 'batch_size': 26, 'epochs': 69, 'lstm_units_layer_0': 15}. Best is trial 14 with value: 0.03693483360729994.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17782, 1, 202)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:25:07,955] Trial 16 finished with value: 0.037859592893539566 and parameters: {'look_back': 202, 'num_hidden_layers': 3, 'learning_rate': 0.0002274877805392506, 'batch_size': 7, 'epochs': 46, 'lstm_units_layer_0': 16, 'lstm_units_layer_1': 27, 'lstm_units_layer_2': 32}. Best is trial 14 with value: 0.03693483360729994.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17684, 1, 300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 14:30:44,236] Trial 17 finished with value: 0.03759578578382192 and parameters: {'look_back': 300, 'num_hidden_layers': 1, 'learning_rate': 0.00010649680925985988, 'batch_size': 8, 'epochs': 63, 'lstm_units_layer_0': 14}. Best is trial 14 with value: 0.03693483360729994.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17924, 1, 60)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:31:40,552] Trial 15 finished with value: 0.036570963652281366 and parameters: {'look_back': 65, 'num_hidden_layers': 3, 'learning_rate': 0.00019511832310862206, 'batch_size': 8, 'epochs': 66, 'lstm_units_layer_0': 8, 'lstm_units_layer_1': 63, 'lstm_units_layer_2': 240}. Best is trial 15 with value: 0.036570963652281366.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17914, 1, 70)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:31:43,880] Trial 18 finished with value: 0.05130741220742003 and parameters: {'look_back': 60, 'num_hidden_layers': 2, 'learning_rate': 1.4815188356751813e-05, 'batch_size': 133, 'epochs': 68, 'lstm_units_layer_0': 14, 'lstm_units_layer_1': 79}. Best is trial 15 with value: 0.036570963652281366.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17820, 1, 164)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 14:33:06,695] Trial 19 finished with value: 0.04990513752887446 and parameters: {'look_back': 70, 'num_hidden_layers': 2, 'learning_rate': 1.1440310806852664e-05, 'batch_size': 43, 'epochs': 42, 'lstm_units_layer_0': 20, 'lstm_units_layer_1': 82}. Best is trial 15 with value: 0.036570963652281366.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17811, 1, 173)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:35:25,536] Trial 20 finished with value: 0.03937889759974276 and parameters: {'look_back': 164, 'num_hidden_layers': 4, 'learning_rate': 0.00012674150861684876, 'batch_size': 38, 'epochs': 43, 'lstm_units_layer_0': 20, 'lstm_units_layer_1': 36, 'lstm_units_layer_2': 28, 'lstm_units_layer_3': 239}. Best is trial 15 with value: 0.036570963652281366.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17944, 1, 40)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:36:29,858] Trial 21 finished with value: 0.03792682785028139 and parameters: {'look_back': 173, 'num_hidden_layers': 4, 'learning_rate': 0.0014726226200405554, 'batch_size': 44, 'epochs': 46, 'lstm_units_layer_0': 10, 'lstm_units_layer_1': 34, 'lstm_units_layer_2': 29, 'lstm_units_layer_3': 218}. Best is trial 15 with value: 0.036570963652281366.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17947, 1, 37)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:38:06,268] Trial 23 finished with value: 0.03671774736741248 and parameters: {'look_back': 37, 'num_hidden_layers': 2, 'learning_rate': 0.0006665881529019888, 'batch_size': 64, 'epochs': 84, 'lstm_units_layer_0': 11, 'lstm_units_layer_1': 46}. Best is trial 15 with value: 0.036570963652281366.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17907, 1, 77)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:41:45,699] Trial 24 finished with value: 0.037404418638362705 and parameters: {'look_back': 77, 'num_hidden_layers': 2, 'learning_rate': 0.0027448156838724763, 'batch_size': 22, 'epochs': 85, 'lstm_units_layer_0': 10, 'lstm_units_layer_1': 8}. Best is trial 15 with value: 0.036570963652281366.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17948, 1, 36)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:42:30,053] Trial 25 finished with value: 0.03657708852594775 and parameters: {'look_back': 36, 'num_hidden_layers': 1, 'learning_rate': 0.0011552756240605906, 'batch_size': 67, 'epochs': 68, 'lstm_units_layer_0': 8}. Best is trial 15 with value: 0.036570963652281366.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17949, 1, 35)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:42:42,346] Trial 22 finished with value: 0.03849938844870803 and parameters: {'look_back': 40, 'num_hidden_layers': 4, 'learning_rate': 0.0009934286431653053, 'batch_size': 30, 'epochs': 85, 'lstm_units_layer_0': 10, 'lstm_units_layer_1': 41, 'lstm_units_layer_2': 183, 'lstm_units_layer_3': 8}. Best is trial 15 with value: 0.036570963652281366.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17855, 1, 129)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:44:19,489] Trial 26 finished with value: 0.037506097509514696 and parameters: {'look_back': 35, 'num_hidden_layers': 2, 'learning_rate': 0.00019337062909844, 'batch_size': 70, 'epochs': 85, 'lstm_units_layer_0': 8, 'lstm_units_layer_1': 88}. Best is trial 15 with value: 0.036570963652281366.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17864, 1, 120)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:44:55,587] Trial 27 finished with value: 0.03794942128953343 and parameters: {'look_back': 129, 'num_hidden_layers': 2, 'learning_rate': 0.00022050666465151084, 'batch_size': 70, 'epochs': 100, 'lstm_units_layer_0': 8, 'lstm_units_layer_1': 94}. Best is trial 15 with value: 0.036570963652281366.\n",
            "<ipython-input-10-27a80ae14fd5>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17892, 1, 92)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:50:18,510] Trial 29 finished with value: 0.03920423091814834 and parameters: {'look_back': 92, 'num_hidden_layers': 6, 'learning_rate': 0.005204574620974465, 'batch_size': 58, 'epochs': 65, 'lstm_units_layer_0': 19, 'lstm_units_layer_1': 53, 'lstm_units_layer_2': 142, 'lstm_units_layer_3': 115, 'lstm_units_layer_4': 51, 'lstm_units_layer_5': 8}. Best is trial 15 with value: 0.036570963652281366.\n",
            "[I 2023-09-05 14:51:22,369] Trial 28 finished with value: 0.03966708093695119 and parameters: {'look_back': 120, 'num_hidden_layers': 6, 'learning_rate': 6.04615387402686e-05, 'batch_size': 62, 'epochs': 99, 'lstm_units_layer_0': 11, 'lstm_units_layer_1': 56, 'lstm_units_layer_2': 154, 'lstm_units_layer_3': 114, 'lstm_units_layer_4': 53, 'lstm_units_layer_5': 8}. Best is trial 15 with value: 0.036570963652281366.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "LSTM HyperParameters Tuning Elapsed Time : 3548.26525 seconds\n",
            "Done\n",
            "best_params: {'look_back': 65, 'num_hidden_layers': 3, 'learning_rate': 0.00019511832310862206, 'batch_size': 8, 'epochs': 66, 'lstm_units_layer_0': 8, 'lstm_units_layer_1': 63, 'lstm_units_layer_2': 240}, best_error: 0.036570963652281366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 10\n",
        "\n",
        "# Hyper Parameters tuning\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "valid_size = int(len(train) * 0.1)\n",
        "temp_train = train[:-valid_size]\n",
        "valid = train[-valid_size:]\n",
        "print(len(train), len(temp_train), len(valid), len(test))\n",
        "\n",
        "def objective(trial):\n",
        "    # define search space for hyperparameters\n",
        "    look_back = trial.suggest_int('look_back', 5, 300)\n",
        "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 10)\n",
        "\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
        "    batch_size = trial.suggest_int('batch_size', 5, 300)\n",
        "    epochs = trial.suggest_int('epochs', 10, 100)\n",
        "\n",
        "    # reshape into X=t and Y=t+1\n",
        "    trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "    validX, validY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "    # reshape input to be [samples, time steps, features]\n",
        "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "    validX = np.reshape(validX, (validX.shape[0], 1, validX.shape[1]))\n",
        "    print(trainX.shape)\n",
        "\n",
        "    # Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "    model = Sequential()\n",
        "    for i in range(num_hidden_layers):\n",
        "        num_units = trial.suggest_int(f'lstm_units_layer_{i}', 8, 256, log=True)\n",
        "        return_sequences = (i < num_hidden_layers - 1)\n",
        "        model.add(LSTM(units=num_units, return_sequences=return_sequences))\n",
        "    model.add(Dense(horizon))\n",
        "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    validPredict = model.predict(validX, verbose=0)\n",
        "\n",
        "    # calculate root mean squared error\n",
        "    validScore = np.sqrt(mean_squared_error(validY, validPredict))\n",
        "\n",
        "    return validScore\n",
        "\n",
        "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "start_time = time.time()\n",
        "\n",
        "study.optimize(objective, n_trials=30, n_jobs=-1)\n",
        "\n",
        "# Record the ending time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print('done')\n",
        "print(\"LSTM HyperParameters Tuning Elapsed Time : %.5f\" % (elapsed_time), \"seconds\")\n",
        "\n",
        "best_params = study.best_params\n",
        "best_error = study.best_value\n",
        "\n",
        "print(\"Done\")\n",
        "print(f\"best_params: {best_params}, best_error: {best_error}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8drsfHIMUeWL",
        "outputId": "cd178090-34a6-429f-e0fc-d4a19b162d28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:51:22,396] A new study created in memory with name: no-name-374783c5-c86e-4449-9c7c-01550d573f12\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 16192 1799 2000\n",
            "(17948, 1, 34)\n",
            "(17758, 1, 224)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:53:55,114] Trial 1 finished with value: 0.05231549474925235 and parameters: {'look_back': 224, 'num_hidden_layers': 10, 'learning_rate': 0.024089948392788878, 'batch_size': 297, 'epochs': 48, 'lstm_units_layer_0': 159, 'lstm_units_layer_1': 10, 'lstm_units_layer_2': 27, 'lstm_units_layer_3': 58, 'lstm_units_layer_4': 21, 'lstm_units_layer_5': 149, 'lstm_units_layer_6': 20, 'lstm_units_layer_7': 85, 'lstm_units_layer_8': 25, 'lstm_units_layer_9': 41}. Best is trial 1 with value: 0.05231549474925235.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17881, 1, 101)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 14:55:39,703] Trial 2 finished with value: 0.040439191104931216 and parameters: {'look_back': 101, 'num_hidden_layers': 2, 'learning_rate': 0.00038407156136456767, 'batch_size': 100, 'epochs': 93, 'lstm_units_layer_0': 39, 'lstm_units_layer_1': 76}. Best is trial 2 with value: 0.040439191104931216.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17934, 1, 48)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:07:46,705] Trial 0 finished with value: 0.04427409643835151 and parameters: {'look_back': 34, 'num_hidden_layers': 5, 'learning_rate': 1.2370367931514962e-05, 'batch_size': 6, 'epochs': 31, 'lstm_units_layer_0': 119, 'lstm_units_layer_1': 150, 'lstm_units_layer_2': 20, 'lstm_units_layer_3': 153, 'lstm_units_layer_4': 29}. Best is trial 2 with value: 0.040439191104931216.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17812, 1, 170)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:09:53,744] Trial 4 finished with value: 0.04311815017894067 and parameters: {'look_back': 170, 'num_hidden_layers': 4, 'learning_rate': 0.02009252671344957, 'batch_size': 192, 'epochs': 29, 'lstm_units_layer_0': 126, 'lstm_units_layer_1': 237, 'lstm_units_layer_2': 251, 'lstm_units_layer_3': 222}. Best is trial 2 with value: 0.040439191104931216.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17867, 1, 115)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:11:22,741] Trial 3 finished with value: 0.0410366445578038 and parameters: {'look_back': 48, 'num_hidden_layers': 10, 'learning_rate': 0.0001374510651357315, 'batch_size': 37, 'epochs': 89, 'lstm_units_layer_0': 205, 'lstm_units_layer_1': 31, 'lstm_units_layer_2': 64, 'lstm_units_layer_3': 42, 'lstm_units_layer_4': 151, 'lstm_units_layer_5': 11, 'lstm_units_layer_6': 45, 'lstm_units_layer_7': 24, 'lstm_units_layer_8': 13, 'lstm_units_layer_9': 18}. Best is trial 2 with value: 0.040439191104931216.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17773, 1, 209)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:13:09,616] Trial 6 finished with value: 0.05213974348654261 and parameters: {'look_back': 209, 'num_hidden_layers': 4, 'learning_rate': 1.880773278440929e-05, 'batch_size': 167, 'epochs': 81, 'lstm_units_layer_0': 92, 'lstm_units_layer_1': 13, 'lstm_units_layer_2': 8, 'lstm_units_layer_3': 22}. Best is trial 2 with value: 0.040439191104931216.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17868, 1, 114)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:13:56,317] Trial 5 finished with value: 0.042022973169923684 and parameters: {'look_back': 115, 'num_hidden_layers': 9, 'learning_rate': 0.00028819338671381815, 'batch_size': 141, 'epochs': 53, 'lstm_units_layer_0': 18, 'lstm_units_layer_1': 107, 'lstm_units_layer_2': 27, 'lstm_units_layer_3': 9, 'lstm_units_layer_4': 182, 'lstm_units_layer_5': 8, 'lstm_units_layer_6': 108, 'lstm_units_layer_7': 94, 'lstm_units_layer_8': 137}. Best is trial 2 with value: 0.040439191104931216.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17739, 1, 243)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:14:02,879] Trial 7 finished with value: 0.041146270691814237 and parameters: {'look_back': 114, 'num_hidden_layers': 2, 'learning_rate': 0.013757971560170728, 'batch_size': 269, 'epochs': 83, 'lstm_units_layer_0': 34, 'lstm_units_layer_1': 78}. Best is trial 2 with value: 0.040439191104931216.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17900, 1, 82)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 15:14:52,016] Trial 9 finished with value: 0.04421892433676143 and parameters: {'look_back': 82, 'num_hidden_layers': 3, 'learning_rate': 0.0002595552168913265, 'batch_size': 169, 'epochs': 14, 'lstm_units_layer_0': 123, 'lstm_units_layer_1': 242, 'lstm_units_layer_2': 43}. Best is trial 2 with value: 0.040439191104931216.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17844, 1, 138)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:18:56,428] Trial 10 finished with value: 0.040780014520024215 and parameters: {'look_back': 138, 'num_hidden_layers': 3, 'learning_rate': 0.00256641300315283, 'batch_size': 62, 'epochs': 96, 'lstm_units_layer_0': 97, 'lstm_units_layer_1': 14, 'lstm_units_layer_2': 108}. Best is trial 2 with value: 0.040439191104931216.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17723, 1, 259)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:19:35,197] Trial 11 finished with value: 0.0414969855407299 and parameters: {'look_back': 259, 'num_hidden_layers': 1, 'learning_rate': 0.0020975950546756102, 'batch_size': 87, 'epochs': 70, 'lstm_units_layer_0': 10}. Best is trial 2 with value: 0.040439191104931216.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17833, 1, 149)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:20:52,331] Trial 12 finished with value: 0.04012357832510543 and parameters: {'look_back': 149, 'num_hidden_layers': 1, 'learning_rate': 0.0024519997428423335, 'batch_size': 83, 'epochs': 99, 'lstm_units_layer_0': 51}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17806, 1, 176)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:21:39,021] Trial 13 finished with value: 0.04082951288928602 and parameters: {'look_back': 176, 'num_hidden_layers': 1, 'learning_rate': 0.0011441743608859875, 'batch_size': 108, 'epochs': 69, 'lstm_units_layer_0': 48}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17905, 1, 77)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:22:10,189] Trial 8 finished with value: 0.048585989902662705 and parameters: {'look_back': 243, 'num_hidden_layers': 8, 'learning_rate': 1.7953132054559642e-05, 'batch_size': 116, 'epochs': 100, 'lstm_units_layer_0': 34, 'lstm_units_layer_1': 191, 'lstm_units_layer_2': 116, 'lstm_units_layer_3': 52, 'lstm_units_layer_4': 22, 'lstm_units_layer_5': 202, 'lstm_units_layer_6': 41, 'lstm_units_layer_7': 19}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17913, 1, 69)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:24:06,130] Trial 15 finished with value: 0.04143430640077509 and parameters: {'look_back': 69, 'num_hidden_layers': 7, 'learning_rate': 0.005792139292434996, 'batch_size': 212, 'epochs': 70, 'lstm_units_layer_0': 47, 'lstm_units_layer_1': 47, 'lstm_units_layer_2': 8, 'lstm_units_layer_3': 9, 'lstm_units_layer_4': 9, 'lstm_units_layer_5': 39, 'lstm_units_layer_6': 8}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17876, 1, 106)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:25:45,114] Trial 14 finished with value: 0.040781145883840374 and parameters: {'look_back': 77, 'num_hidden_layers': 6, 'learning_rate': 0.005044398438922294, 'batch_size': 121, 'epochs': 98, 'lstm_units_layer_0': 42, 'lstm_units_layer_1': 50, 'lstm_units_layer_2': 8, 'lstm_units_layer_3': 8, 'lstm_units_layer_4': 10, 'lstm_units_layer_5': 190}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17689, 1, 293)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:27:52,295] Trial 17 finished with value: 0.05209603811890099 and parameters: {'look_back': 293, 'num_hidden_layers': 2, 'learning_rate': 0.08674751315982235, 'batch_size': 68, 'epochs': 81, 'lstm_units_layer_0': 66, 'lstm_units_layer_1': 27}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17965, 1, 17)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:29:29,168] Trial 18 finished with value: 0.04076499435563457 and parameters: {'look_back': 17, 'num_hidden_layers': 1, 'learning_rate': 0.000470843857028879, 'batch_size': 29, 'epochs': 61, 'lstm_units_layer_0': 23}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17846, 1, 136)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:30:57,027] Trial 19 finished with value: 0.04722228392121268 and parameters: {'look_back': 136, 'num_hidden_layers': 3, 'learning_rate': 8.661958377616106e-05, 'batch_size': 223, 'epochs': 41, 'lstm_units_layer_0': 71, 'lstm_units_layer_1': 81, 'lstm_units_layer_2': 251}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17796, 1, 186)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:33:56,514] Trial 16 finished with value: 0.07082392168633607 and parameters: {'look_back': 106, 'num_hidden_layers': 6, 'learning_rate': 0.08691541887013433, 'batch_size': 65, 'epochs': 89, 'lstm_units_layer_0': 50, 'lstm_units_layer_1': 63, 'lstm_units_layer_2': 233, 'lstm_units_layer_3': 118, 'lstm_units_layer_4': 89, 'lstm_units_layer_5': 51}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17791, 1, 191)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:35:33,290] Trial 20 finished with value: 0.05532489322066444 and parameters: {'look_back': 186, 'num_hidden_layers': 2, 'learning_rate': 0.0007287841339600217, 'batch_size': 82, 'epochs': 90, 'lstm_units_layer_0': 232, 'lstm_units_layer_1': 136}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17948, 1, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 15:36:10,335] Trial 21 finished with value: 0.04109738213862066 and parameters: {'look_back': 191, 'num_hidden_layers': 2, 'learning_rate': 0.0007591100102265098, 'batch_size': 93, 'epochs': 90, 'lstm_units_layer_0': 22, 'lstm_units_layer_1': 127}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17967, 1, 15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "[I 2023-09-05 15:37:46,618] Trial 22 finished with value: 0.040248551922817295 and parameters: {'look_back': 34, 'num_hidden_layers': 1, 'learning_rate': 0.000552439587093688, 'batch_size': 24, 'epochs': 64, 'lstm_units_layer_0': 25}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17972, 1, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:38:15,714] Trial 23 finished with value: 0.04135016494082625 and parameters: {'look_back': 15, 'num_hidden_layers': 1, 'learning_rate': 0.0004641180268476142, 'batch_size': 24, 'epochs': 59, 'lstm_units_layer_0': 25}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17920, 1, 62)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:39:24,925] Trial 24 finished with value: 0.040716996821403964 and parameters: {'look_back': 10, 'num_hidden_layers': 1, 'learning_rate': 0.0013619237506193082, 'batch_size': 37, 'epochs': 75, 'lstm_units_layer_0': 26}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17925, 1, 57)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:40:52,755] Trial 26 finished with value: 0.04068915680579142 and parameters: {'look_back': 57, 'num_hidden_layers': 4, 'learning_rate': 0.002127338223220126, 'batch_size': 135, 'epochs': 76, 'lstm_units_layer_0': 14, 'lstm_units_layer_1': 29, 'lstm_units_layer_2': 13, 'lstm_units_layer_3': 19}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17891, 1, 91)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:41:57,813] Trial 25 finished with value: 0.040609272142573906 and parameters: {'look_back': 62, 'num_hidden_layers': 4, 'learning_rate': 0.0014014671258148741, 'batch_size': 45, 'epochs': 76, 'lstm_units_layer_0': 15, 'lstm_units_layer_1': 33, 'lstm_units_layer_2': 14, 'lstm_units_layer_3': 23}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17887, 1, 95)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:45:01,056] Trial 27 finished with value: 0.0419701187012747 and parameters: {'look_back': 91, 'num_hidden_layers': 3, 'learning_rate': 0.0001214914017461513, 'batch_size': 51, 'epochs': 100, 'lstm_units_layer_0': 34, 'lstm_units_layer_1': 98, 'lstm_units_layer_2': 50}. Best is trial 12 with value: 0.04012357832510543.\n",
            "<ipython-input-11-025fe578aeba>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17831, 1, 151)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 15:53:18,855] Trial 28 finished with value: 0.04106130498361404 and parameters: {'look_back': 95, 'num_hidden_layers': 2, 'learning_rate': 0.0001060791054558371, 'batch_size': 7, 'epochs': 63, 'lstm_units_layer_0': 31, 'lstm_units_layer_1': 95}. Best is trial 12 with value: 0.04012357832510543.\n",
            "[I 2023-09-05 15:55:35,596] Trial 29 finished with value: 0.04168800493164721 and parameters: {'look_back': 151, 'num_hidden_layers': 2, 'learning_rate': 5.6776166755040616e-05, 'batch_size': 6, 'epochs': 65, 'lstm_units_layer_0': 62, 'lstm_units_layer_1': 20}. Best is trial 12 with value: 0.04012357832510543.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "LSTM HyperParameters Tuning Elapsed Time : 3853.20098 seconds\n",
            "Done\n",
            "best_params: {'look_back': 149, 'num_hidden_layers': 1, 'learning_rate': 0.0024519997428423335, 'batch_size': 83, 'epochs': 99, 'lstm_units_layer_0': 51}, best_error: 0.04012357832510543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "look_back = 52\n",
        "num_hidden_layers = 1\n",
        "learning_rate = 0.0025060720938911716\n",
        "batch_size = 217\n",
        "epochs = 99\n",
        "lstm_units_layer_0 = 48"
      ],
      "metadata": {
        "id": "fC5bHP_kKXRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 2"
      ],
      "metadata": {
        "id": "HatcrmWOsJjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "\n",
        "look_back = 51\n",
        "num_hidden_layers = 2\n",
        "learning_rate = 0.001519335526892746\n",
        "batch_size = 51\n",
        "epochs = 82\n",
        "lstm_units_layer_0 = 96\n",
        "lstm_units_layer_1 = 208\n",
        "# best_error: 0.020879777783223082"
      ],
      "metadata": {
        "id": "sxM2whWzKxXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 2\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=lstm_units_layer_0, return_sequences=True))\n",
        "model.add(LSTM(units=lstm_units_layer_1))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "\n",
        "# calculate root mean squared error\n",
        "LSTM_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "LSTM_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"LSTM Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"LSTM Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('LSTM Test RMSE : %.5f' % (LSTM_RMSE))\n",
        "print('LSTM Test MAE : %.5f' % (LSTM_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjc9TJnBsHoe",
        "outputId": "c7d5ea56-a781-4e6d-ae2f-28ca13feae66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17841, 1, 149)\n",
            "Epoch 1/99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 [==============================] - 5s 6ms/step - loss: 0.0039\n",
            "Epoch 2/99\n",
            "215/215 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 3/99\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 9.7484e-04\n",
            "Epoch 4/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 8.0636e-04\n",
            "Epoch 5/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 7.5102e-04\n",
            "Epoch 6/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 6.9489e-04\n",
            "Epoch 7/99\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 7.1108e-04\n",
            "Epoch 8/99\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 6.7232e-04\n",
            "Epoch 9/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 6.5830e-04\n",
            "Epoch 10/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 6.4791e-04\n",
            "Epoch 11/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 6.4895e-04\n",
            "Epoch 12/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 6.1880e-04\n",
            "Epoch 13/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 6.3479e-04\n",
            "Epoch 14/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 6.6352e-04\n",
            "Epoch 15/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 6.0108e-04\n",
            "Epoch 16/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.7801e-04\n",
            "Epoch 17/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.6545e-04\n",
            "Epoch 18/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 6.0314e-04\n",
            "Epoch 19/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.7903e-04\n",
            "Epoch 20/99\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 5.7016e-04\n",
            "Epoch 21/99\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 5.9499e-04\n",
            "Epoch 22/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.8622e-04\n",
            "Epoch 23/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.6174e-04\n",
            "Epoch 24/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.4056e-04\n",
            "Epoch 25/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.6550e-04\n",
            "Epoch 26/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.5993e-04\n",
            "Epoch 27/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.5779e-04\n",
            "Epoch 28/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.3717e-04\n",
            "Epoch 29/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.5437e-04\n",
            "Epoch 30/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.2649e-04\n",
            "Epoch 31/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.3806e-04\n",
            "Epoch 32/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.2025e-04\n",
            "Epoch 33/99\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 5.3058e-04\n",
            "Epoch 34/99\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 5.4321e-04\n",
            "Epoch 35/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.4657e-04\n",
            "Epoch 36/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.9747e-04\n",
            "Epoch 37/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.1421e-04\n",
            "Epoch 38/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.9846e-04\n",
            "Epoch 39/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.4752e-04\n",
            "Epoch 40/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.1102e-04\n",
            "Epoch 41/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.1824e-04\n",
            "Epoch 42/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.0644e-04\n",
            "Epoch 43/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.0907e-04\n",
            "Epoch 44/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.0193e-04\n",
            "Epoch 45/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.3953e-04\n",
            "Epoch 46/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.9760e-04\n",
            "Epoch 47/99\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 5.1883e-04\n",
            "Epoch 48/99\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 5.3153e-04\n",
            "Epoch 49/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.0093e-04\n",
            "Epoch 50/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.0068e-04\n",
            "Epoch 51/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.0407e-04\n",
            "Epoch 52/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.0416e-04\n",
            "Epoch 53/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.8672e-04\n",
            "Epoch 54/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.1305e-04\n",
            "Epoch 55/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.9003e-04\n",
            "Epoch 56/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.7434e-04\n",
            "Epoch 57/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.0714e-04\n",
            "Epoch 58/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.8017e-04\n",
            "Epoch 59/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.8199e-04\n",
            "Epoch 60/99\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 4.9511e-04\n",
            "Epoch 61/99\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 5.2240e-04\n",
            "Epoch 62/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.8062e-04\n",
            "Epoch 63/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.3107e-04\n",
            "Epoch 64/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.8252e-04\n",
            "Epoch 65/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.7238e-04\n",
            "Epoch 66/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.0559e-04\n",
            "Epoch 67/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.7656e-04\n",
            "Epoch 68/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.8522e-04\n",
            "Epoch 69/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.8732e-04\n",
            "Epoch 70/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.6848e-04\n",
            "Epoch 71/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.6353e-04\n",
            "Epoch 72/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.1097e-04\n",
            "Epoch 73/99\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 4.8180e-04\n",
            "Epoch 74/99\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 4.9406e-04\n",
            "Epoch 75/99\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 4.6882e-04\n",
            "Epoch 76/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.9419e-04\n",
            "Epoch 77/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.6339e-04\n",
            "Epoch 78/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.6965e-04\n",
            "Epoch 79/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.7962e-04\n",
            "Epoch 80/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.6955e-04\n",
            "Epoch 81/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.0104e-04\n",
            "Epoch 82/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.7998e-04\n",
            "Epoch 83/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 5.0543e-04\n",
            "Epoch 84/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.6680e-04\n",
            "Epoch 85/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.6317e-04\n",
            "Epoch 86/99\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 4.7904e-04\n",
            "Epoch 87/99\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 4.7248e-04\n",
            "Epoch 88/99\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 4.5824e-04\n",
            "Epoch 89/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.7453e-04\n",
            "Epoch 90/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.8549e-04\n",
            "Epoch 91/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.8739e-04\n",
            "Epoch 92/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.8532e-04\n",
            "Epoch 93/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.5825e-04\n",
            "Epoch 94/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.5698e-04\n",
            "Epoch 95/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.6069e-04\n",
            "Epoch 96/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.5415e-04\n",
            "Epoch 97/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.6383e-04\n",
            "Epoch 98/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.5587e-04\n",
            "Epoch 99/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 4.5877e-04\n",
            "LSTM Model Training Elapsed Time : 94.78146 seconds\n",
            "LSTM Model Generate predictions Elapsed Time : 0.99485 seconds\n",
            "LSTM Test RMSE : 0.02241\n",
            "LSTM Test MAE : 0.01800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 4"
      ],
      "metadata": {
        "id": "d5dQfPCNswmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "\n",
        "# 0.02686025211281613\n",
        "look_back = 56\n",
        "num_hidden_layers = 3\n",
        "learning_rate = 0.0056240033221932485\n",
        "batch_size = 193\n",
        "epochs = 78\n",
        "lstm_units_layer_0 = 37\n",
        "lstm_units_layer_1 = 82\n",
        "lstm_units_layer_2 = 49"
      ],
      "metadata": {
        "id": "bhkeAaPaLanx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 4\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=lstm_units_layer_0, return_sequences=True))\n",
        "model.add(LSTM(units=lstm_units_layer_1, return_sequences=True))\n",
        "model.add(LSTM(units=lstm_units_layer_2))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "# testPredict = np.reshape(testPredict, (testPredict.shape[0], testPredict.shape[2]))\n",
        "# calculate root mean squared error\n",
        "LSTM_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "LSTM_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"LSTM Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"LSTM Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('LSTM Test RMSE : %.5f' % (LSTM_RMSE))\n",
        "print('LSTM Test MAE : %.5f' % (LSTM_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQqsU0DFszFS",
        "outputId": "f4de375d-5304-454f-caca-1511038c4a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17932, 1, 56)\n",
            "Epoch 1/78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93/93 [==============================] - 6s 7ms/step - loss: 0.0077\n",
            "Epoch 2/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.0022\n",
            "Epoch 3/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "Epoch 4/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 0.0012\n",
            "Epoch 5/78\n",
            "93/93 [==============================] - 1s 13ms/step - loss: 0.0011\n",
            "Epoch 6/78\n",
            "93/93 [==============================] - 1s 13ms/step - loss: 9.9254e-04\n",
            "Epoch 7/78\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0010\n",
            "Epoch 8/78\n",
            "93/93 [==============================] - 1s 14ms/step - loss: 9.2369e-04\n",
            "Epoch 9/78\n",
            "93/93 [==============================] - 1s 15ms/step - loss: 8.8778e-04\n",
            "Epoch 10/78\n",
            "93/93 [==============================] - 1s 16ms/step - loss: 9.1964e-04\n",
            "Epoch 11/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 8.8205e-04\n",
            "Epoch 12/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 8.6774e-04\n",
            "Epoch 13/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 8.4888e-04\n",
            "Epoch 14/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 8.9909e-04\n",
            "Epoch 15/78\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 8.7182e-04\n",
            "Epoch 16/78\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 8.4616e-04\n",
            "Epoch 17/78\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 8.3536e-04\n",
            "Epoch 18/78\n",
            "93/93 [==============================] - 1s 13ms/step - loss: 8.3006e-04\n",
            "Epoch 19/78\n",
            "93/93 [==============================] - 1s 13ms/step - loss: 8.2080e-04\n",
            "Epoch 20/78\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 7.9015e-04\n",
            "Epoch 21/78\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 8.0098e-04\n",
            "Epoch 22/78\n",
            "93/93 [==============================] - 1s 16ms/step - loss: 8.0792e-04\n",
            "Epoch 23/78\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 8.3468e-04\n",
            "Epoch 24/78\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 8.0441e-04\n",
            "Epoch 25/78\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 7.9159e-04\n",
            "Epoch 26/78\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 7.8405e-04\n",
            "Epoch 27/78\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 7.7980e-04\n",
            "Epoch 28/78\n",
            "93/93 [==============================] - 1s 14ms/step - loss: 8.0073e-04\n",
            "Epoch 29/78\n",
            "93/93 [==============================] - 1s 14ms/step - loss: 8.1628e-04\n",
            "Epoch 30/78\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 7.5378e-04\n",
            "Epoch 31/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.9284e-04\n",
            "Epoch 32/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 8.0694e-04\n",
            "Epoch 33/78\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 8.0018e-04\n",
            "Epoch 34/78\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 7.7614e-04\n",
            "Epoch 35/78\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 7.8506e-04\n",
            "Epoch 36/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 8.2469e-04\n",
            "Epoch 37/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.8354e-04\n",
            "Epoch 38/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.7436e-04\n",
            "Epoch 39/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.6867e-04\n",
            "Epoch 40/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.5829e-04\n",
            "Epoch 41/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.4780e-04\n",
            "Epoch 42/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.7662e-04\n",
            "Epoch 43/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.7076e-04\n",
            "Epoch 44/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.2930e-04\n",
            "Epoch 45/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.7532e-04\n",
            "Epoch 46/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.3992e-04\n",
            "Epoch 47/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.4966e-04\n",
            "Epoch 48/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.6499e-04\n",
            "Epoch 49/78\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 7.5209e-04\n",
            "Epoch 50/78\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 7.8175e-04\n",
            "Epoch 51/78\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 7.3099e-04\n",
            "Epoch 52/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.4194e-04\n",
            "Epoch 53/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.2936e-04\n",
            "Epoch 54/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.2715e-04\n",
            "Epoch 55/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.2019e-04\n",
            "Epoch 56/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.5089e-04\n",
            "Epoch 57/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.6090e-04\n",
            "Epoch 58/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.7439e-04\n",
            "Epoch 59/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.4321e-04\n",
            "Epoch 60/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.2467e-04\n",
            "Epoch 61/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.2694e-04\n",
            "Epoch 62/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.2777e-04\n",
            "Epoch 63/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.3734e-04\n",
            "Epoch 64/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.4032e-04\n",
            "Epoch 65/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.1985e-04\n",
            "Epoch 66/78\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 7.2438e-04\n",
            "Epoch 67/78\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 7.6914e-04\n",
            "Epoch 68/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.4088e-04\n",
            "Epoch 69/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.3367e-04\n",
            "Epoch 70/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.3567e-04\n",
            "Epoch 71/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.3295e-04\n",
            "Epoch 72/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.4829e-04\n",
            "Epoch 73/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.4220e-04\n",
            "Epoch 74/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.6401e-04\n",
            "Epoch 75/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.2342e-04\n",
            "Epoch 76/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.2480e-04\n",
            "Epoch 77/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.1947e-04\n",
            "Epoch 78/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.0681e-04\n",
            "LSTM Model Training Elapsed Time : 72.08377 seconds\n",
            "LSTM Model Generate predictions Elapsed Time : 1.91246 seconds\n",
            "LSTM Test RMSE : 0.02686\n",
            "LSTM Test MAE : 0.02135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 6"
      ],
      "metadata": {
        "id": "Ksu6Ed2xsoUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "\n",
        "look_back = 91\n",
        "num_hidden_layers = 6\n",
        "learning_rate = 0.0005124476844228014\n",
        "batch_size = 50\n",
        "epochs = 73\n",
        "lstm_units_layer_0 = 50\n",
        "lstm_units_layer_1 = 28\n",
        "lstm_units_layer_2 = 82\n",
        "lstm_units_layer_3 = 33\n",
        "lstm_units_layer_4 = 22\n",
        "lstm_units_layer_5 = 15\n",
        "# best_error: 0.03235792738580053\n"
      ],
      "metadata": {
        "id": "RB24TDZtL1pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 6\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=lstm_units_layer_0, return_sequences=True))\n",
        "model.add(LSTM(units=lstm_units_layer_1, return_sequences=True))\n",
        "model.add(LSTM(units=lstm_units_layer_2, return_sequences=True))\n",
        "model.add(LSTM(units=lstm_units_layer_3, return_sequences=True))\n",
        "model.add(LSTM(units=lstm_units_layer_4, return_sequences=True))\n",
        "model.add(LSTM(units=lstm_units_layer_5))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "# testPredict = np.reshape(testPredict, (testPredict.shape[0], testPredict.shape[2]))\n",
        "# calculate root mean squared error\n",
        "LSTM_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "LSTM_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"LSTM Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"LSTM Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('LSTM Test RMSE : %.5f' % (LSTM_RMSE))\n",
        "print('LSTM Test MAE : %.5f' % (LSTM_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNK7wTm3snr-",
        "outputId": "7f538673-a996-4afa-c549-80055f0b7a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17895, 1, 91)\n",
            "Epoch 1/73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "358/358 [==============================] - 19s 11ms/step - loss: 0.0204\n",
            "Epoch 2/73\n",
            "358/358 [==============================] - 5s 13ms/step - loss: 0.0023\n",
            "Epoch 3/73\n",
            "358/358 [==============================] - 4s 11ms/step - loss: 0.0022\n",
            "Epoch 4/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0017\n",
            "Epoch 5/73\n",
            "358/358 [==============================] - 4s 12ms/step - loss: 0.0015\n",
            "Epoch 6/73\n",
            "358/358 [==============================] - 4s 12ms/step - loss: 0.0014\n",
            "Epoch 7/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0014\n",
            "Epoch 8/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0013\n",
            "Epoch 9/73\n",
            "358/358 [==============================] - 5s 13ms/step - loss: 0.0013\n",
            "Epoch 10/73\n",
            "358/358 [==============================] - 4s 11ms/step - loss: 0.0013\n",
            "Epoch 11/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0012\n",
            "Epoch 12/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0012\n",
            "Epoch 13/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0012\n",
            "Epoch 14/73\n",
            "358/358 [==============================] - 3s 7ms/step - loss: 0.0012\n",
            "Epoch 15/73\n",
            "358/358 [==============================] - 5s 13ms/step - loss: 0.0012\n",
            "Epoch 16/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0012\n",
            "Epoch 17/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0012\n",
            "Epoch 18/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0012\n",
            "Epoch 19/73\n",
            "358/358 [==============================] - 4s 12ms/step - loss: 0.0012\n",
            "Epoch 20/73\n",
            "358/358 [==============================] - 3s 7ms/step - loss: 0.0012\n",
            "Epoch 21/73\n",
            "358/358 [==============================] - 3s 7ms/step - loss: 0.0012\n",
            "Epoch 22/73\n",
            "358/358 [==============================] - 3s 7ms/step - loss: 0.0011\n",
            "Epoch 23/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0011\n",
            "Epoch 24/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0011\n",
            "Epoch 25/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0011\n",
            "Epoch 26/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0011\n",
            "Epoch 27/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0011\n",
            "Epoch 28/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0011\n",
            "Epoch 29/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0011\n",
            "Epoch 30/73\n",
            "358/358 [==============================] - 4s 11ms/step - loss: 0.0011\n",
            "Epoch 31/73\n",
            "358/358 [==============================] - 5s 13ms/step - loss: 0.0011\n",
            "Epoch 32/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0011\n",
            "Epoch 33/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0011\n",
            "Epoch 34/73\n",
            "358/358 [==============================] - 5s 13ms/step - loss: 0.0011\n",
            "Epoch 35/73\n",
            "358/358 [==============================] - 3s 10ms/step - loss: 0.0011\n",
            "Epoch 36/73\n",
            "358/358 [==============================] - 4s 11ms/step - loss: 0.0011\n",
            "Epoch 37/73\n",
            "358/358 [==============================] - 4s 12ms/step - loss: 0.0011\n",
            "Epoch 38/73\n",
            "358/358 [==============================] - 5s 14ms/step - loss: 0.0011\n",
            "Epoch 39/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0011\n",
            "Epoch 40/73\n",
            "358/358 [==============================] - 4s 11ms/step - loss: 0.0010\n",
            "Epoch 41/73\n",
            "358/358 [==============================] - 4s 12ms/step - loss: 0.0011\n",
            "Epoch 42/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0011\n",
            "Epoch 43/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 44/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 45/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0011\n",
            "Epoch 46/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0010\n",
            "Epoch 47/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0011\n",
            "Epoch 48/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0011\n",
            "Epoch 49/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0010\n",
            "Epoch 50/73\n",
            "358/358 [==============================] - 3s 7ms/step - loss: 0.0010\n",
            "Epoch 51/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0010\n",
            "Epoch 52/73\n",
            "358/358 [==============================] - 3s 7ms/step - loss: 0.0010\n",
            "Epoch 53/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 54/73\n",
            "358/358 [==============================] - 2s 6ms/step - loss: 0.0010\n",
            "Epoch 55/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0010\n",
            "Epoch 56/73\n",
            "358/358 [==============================] - 3s 7ms/step - loss: 0.0010\n",
            "Epoch 57/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 58/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 59/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0010\n",
            "Epoch 60/73\n",
            "358/358 [==============================] - 3s 7ms/step - loss: 0.0010\n",
            "Epoch 61/73\n",
            "358/358 [==============================] - 3s 7ms/step - loss: 0.0010\n",
            "Epoch 62/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 63/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0010\n",
            "Epoch 64/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0010\n",
            "Epoch 65/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 66/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0010\n",
            "Epoch 67/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0010\n",
            "Epoch 68/73\n",
            "358/358 [==============================] - 3s 7ms/step - loss: 0.0010\n",
            "Epoch 69/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0010\n",
            "Epoch 70/73\n",
            "358/358 [==============================] - 3s 7ms/step - loss: 9.9571e-04\n",
            "Epoch 71/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0010\n",
            "Epoch 72/73\n",
            "358/358 [==============================] - 3s 7ms/step - loss: 0.0010\n",
            "Epoch 73/73\n",
            "358/358 [==============================] - 3s 7ms/step - loss: 0.0010\n",
            "LSTM Model Training Elapsed Time : 250.09103 seconds\n",
            "LSTM Model Generate predictions Elapsed Time : 1.66337 seconds\n",
            "LSTM Test RMSE : 0.03228\n",
            "LSTM Test MAE : 0.02534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 8"
      ],
      "metadata": {
        "id": "k49hKjgUs1tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "look_back = 65\n",
        "num_hidden_layers = 3\n",
        "learning_rate = 0.00019511832310862206\n",
        "batch_size = 8\n",
        "epochs = 66\n",
        "lstm_units_layer_0 = 8\n",
        "lstm_units_layer_1 = 63\n",
        "lstm_units_layer_2 = 240\n",
        "# best_error: 0.036570963652281366\n"
      ],
      "metadata": {
        "id": "m4dfKgYdM4kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 8\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=lstm_units_layer_0, return_sequences=True))\n",
        "model.add(LSTM(units=lstm_units_layer_1, return_sequences=True))\n",
        "model.add(LSTM(units=lstm_units_layer_2))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "# testPredict = np.reshape(testPredict, (testPredict.shape[0], testPredict.shape[2]))\n",
        "# calculate root mean squared error\n",
        "LSTM_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "LSTM_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"LSTM Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"LSTM Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('LSTM Test RMSE : %.5f' % (LSTM_RMSE))\n",
        "print('LSTM Test MAE : %.5f' % (LSTM_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sITfQ24Psswa",
        "outputId": "308ee3c2-965e-4a8a-8fde-bfb5ad88aa0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17919, 1, 65)\n",
            "Epoch 1/66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2240/2240 [==============================] - 17s 5ms/step - loss: 0.0052\n",
            "Epoch 2/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0022\n",
            "Epoch 3/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0019\n",
            "Epoch 4/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0018\n",
            "Epoch 5/66\n",
            "2240/2240 [==============================] - 13s 6ms/step - loss: 0.0017\n",
            "Epoch 6/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0016\n",
            "Epoch 7/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0016\n",
            "Epoch 8/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0015\n",
            "Epoch 9/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0015\n",
            "Epoch 10/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 11/66\n",
            "2240/2240 [==============================] - 12s 6ms/step - loss: 0.0014\n",
            "Epoch 12/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 13/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 14/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 15/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 16/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 17/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 18/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 19/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 20/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 21/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 22/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 23/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 24/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 25/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 26/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 27/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 28/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 29/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 30/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 31/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 32/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 33/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 34/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 35/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 36/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 37/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 38/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 39/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 40/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 41/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 42/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 43/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 44/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 45/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 46/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 47/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 48/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 49/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 50/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 51/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 52/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 53/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 54/66\n",
            "2240/2240 [==============================] - 12s 6ms/step - loss: 0.0013\n",
            "Epoch 55/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 56/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 57/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 58/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 59/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0012\n",
            "Epoch 60/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0012\n",
            "Epoch 61/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0012\n",
            "Epoch 62/66\n",
            "2240/2240 [==============================] - 12s 6ms/step - loss: 0.0012\n",
            "Epoch 63/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0012\n",
            "Epoch 64/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0012\n",
            "Epoch 65/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0012\n",
            "Epoch 66/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0012\n",
            "LSTM Model Training Elapsed Time : 801.79241 seconds\n",
            "LSTM Model Generate predictions Elapsed Time : 1.15127 seconds\n",
            "LSTM Test RMSE : 0.03621\n",
            "LSTM Test MAE : 0.02818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 10"
      ],
      "metadata": {
        "id": "HbdZnj6Zs48a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "\n",
        "look_back = 149\n",
        "num_hidden_layers = 1\n",
        "learning_rate = 0.0024519997428423335\n",
        "batch_size = 83\n",
        "epochs = 99\n",
        "lstm_units_layer_0 = 51\n",
        "# best_error: 0.04012357832510543"
      ],
      "metadata": {
        "id": "VQFJ_gsiNUwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 10\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=lstm_units_layer_0))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "# testPredict = np.reshape(testPredict, (testPredict.shape[0], testPredict.shape[2]))\n",
        "# calculate root mean squared error\n",
        "LSTM_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "LSTM_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"LSTM Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"LSTM Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('LSTM Test RMSE : %.5f' % (LSTM_RMSE))\n",
        "print('LSTM Test MAE : %.5f' % (LSTM_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIauC4Kis4bL",
        "outputId": "a074cbef-a3f3-4e9a-e091-293af6f8f4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17833, 1, 149)\n",
            "Epoch 1/99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 [==============================] - 3s 2ms/step - loss: 0.0037\n",
            "Epoch 2/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0021\n",
            "Epoch 3/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Epoch 4/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Epoch 5/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Epoch 6/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Epoch 7/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Epoch 8/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0017\n",
            "Epoch 9/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 10/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 11/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 12/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Epoch 13/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Epoch 14/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Epoch 15/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Epoch 16/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 17/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 18/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 19/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 20/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Epoch 21/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Epoch 22/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 23/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 24/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 25/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Epoch 26/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 27/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 28/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 29/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 30/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 31/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 32/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0015\n",
            "Epoch 33/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 34/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 35/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 36/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 37/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 38/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 39/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 40/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 41/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 42/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 43/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 44/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 45/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 46/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 47/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 48/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 49/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 50/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 51/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 52/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 53/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 54/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 55/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 56/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 57/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 58/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 59/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 60/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 61/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 62/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 63/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 64/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 65/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 66/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 67/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 68/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 69/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 70/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 71/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 72/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 73/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 74/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 75/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 76/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0014\n",
            "Epoch 77/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 78/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 79/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 80/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 81/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 82/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 83/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 84/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 85/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 86/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 87/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 88/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 89/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 90/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 91/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 92/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 93/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 94/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 95/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Epoch 96/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 97/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0014\n",
            "Epoch 98/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0014\n",
            "Epoch 99/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "LSTM Model Training Elapsed Time : 56.67461 seconds\n",
            "LSTM Model Generate predictions Elapsed Time : 0.41469 seconds\n",
            "LSTM Test RMSE : 0.03959\n",
            "LSTM Test MAE : 0.03069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rOqMEk4URaXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU Model"
      ],
      "metadata": {
        "id": "MQuC0IEHS-eW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 2"
      ],
      "metadata": {
        "id": "NGIBvfWOTAon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "\n",
        "look_back = 51\n",
        "num_hidden_layers = 2\n",
        "learning_rate = 0.001519335526892746\n",
        "batch_size = 51\n",
        "epochs = 82\n",
        "lstm_units_layer_0 = 96\n",
        "lstm_units_layer_1 = 208\n",
        "# best_error: 0.020879777783223082"
      ],
      "metadata": {
        "id": "nuzUbseNTAoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 2\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(GRU(units=lstm_units_layer_0, return_sequences=True))\n",
        "model.add(GRU(units=lstm_units_layer_1))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "\n",
        "# calculate root mean squared error\n",
        "GRU_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "GRU_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"GRU Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"GRU Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('GRU Test RMSE : %.5f' % (GRU_RMSE))\n",
        "print('GRU Test MAE : %.5f' % (GRU_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a09925-29ad-465e-cbe1-5b3f27094667",
        "id": "hmYwsePQTAoo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17939, 1, 51)\n",
            "Epoch 1/82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "352/352 [==============================] - 5s 6ms/step - loss: 0.0022\n",
            "Epoch 2/82\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 6.8747e-04\n",
            "Epoch 3/82\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 6.3316e-04\n",
            "Epoch 4/82\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 5.6578e-04\n",
            "Epoch 5/82\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 5.8433e-04\n",
            "Epoch 6/82\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 5.5823e-04\n",
            "Epoch 7/82\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 5.4890e-04\n",
            "Epoch 8/82\n",
            "352/352 [==============================] - 4s 12ms/step - loss: 5.5914e-04\n",
            "Epoch 9/82\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 5.3398e-04\n",
            "Epoch 10/82\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 5.7062e-04\n",
            "Epoch 11/82\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 5.3441e-04\n",
            "Epoch 12/82\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 5.2813e-04\n",
            "Epoch 13/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 5.0574e-04\n",
            "Epoch 14/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 5.0826e-04\n",
            "Epoch 15/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 5.0608e-04\n",
            "Epoch 16/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 5.0744e-04\n",
            "Epoch 17/82\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 5.0437e-04\n",
            "Epoch 18/82\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 5.2644e-04\n",
            "Epoch 19/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.9199e-04\n",
            "Epoch 20/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.9152e-04\n",
            "Epoch 21/82\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 5.1022e-04\n",
            "Epoch 22/82\n",
            "352/352 [==============================] - 4s 12ms/step - loss: 5.0521e-04\n",
            "Epoch 23/82\n",
            "352/352 [==============================] - 4s 10ms/step - loss: 4.7329e-04\n",
            "Epoch 24/82\n",
            "352/352 [==============================] - 3s 8ms/step - loss: 4.9352e-04\n",
            "Epoch 25/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.7843e-04\n",
            "Epoch 26/82\n",
            "352/352 [==============================] - 2s 7ms/step - loss: 4.6881e-04\n",
            "Epoch 27/82\n",
            "352/352 [==============================] - 4s 10ms/step - loss: 4.7441e-04\n",
            "Epoch 28/82\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 5.0119e-04\n",
            "Epoch 29/82\n",
            "352/352 [==============================] - 3s 8ms/step - loss: 4.8795e-04\n",
            "Epoch 30/82\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 4.7890e-04\n",
            "Epoch 31/82\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 4.8747e-04\n",
            "Epoch 32/82\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 4.8400e-04\n",
            "Epoch 33/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.8571e-04\n",
            "Epoch 34/82\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 4.6894e-04\n",
            "Epoch 35/82\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 4.7762e-04\n",
            "Epoch 36/82\n",
            "352/352 [==============================] - 3s 8ms/step - loss: 4.5982e-04\n",
            "Epoch 37/82\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 4.8924e-04\n",
            "Epoch 38/82\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 4.5672e-04\n",
            "Epoch 39/82\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 4.7103e-04\n",
            "Epoch 40/82\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 4.8742e-04\n",
            "Epoch 41/82\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 4.6174e-04\n",
            "Epoch 42/82\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 4.7754e-04\n",
            "Epoch 43/82\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 4.6666e-04\n",
            "Epoch 44/82\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 4.6116e-04\n",
            "Epoch 45/82\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 4.6177e-04\n",
            "Epoch 46/82\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 4.7206e-04\n",
            "Epoch 47/82\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 4.5165e-04\n",
            "Epoch 48/82\n",
            "352/352 [==============================] - 4s 10ms/step - loss: 4.6721e-04\n",
            "Epoch 49/82\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 4.6252e-04\n",
            "Epoch 50/82\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 4.5681e-04\n",
            "Epoch 51/82\n",
            "352/352 [==============================] - 3s 8ms/step - loss: 4.5152e-04\n",
            "Epoch 52/82\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 4.7387e-04\n",
            "Epoch 53/82\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 4.6906e-04\n",
            "Epoch 54/82\n",
            "352/352 [==============================] - 4s 12ms/step - loss: 4.5095e-04\n",
            "Epoch 55/82\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 4.6939e-04\n",
            "Epoch 56/82\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 4.4739e-04\n",
            "Epoch 57/82\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 4.5354e-04\n",
            "Epoch 58/82\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 4.5924e-04\n",
            "Epoch 59/82\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 4.4298e-04\n",
            "Epoch 60/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.4607e-04\n",
            "Epoch 61/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.3780e-04\n",
            "Epoch 62/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.5282e-04\n",
            "Epoch 63/82\n",
            "352/352 [==============================] - 3s 8ms/step - loss: 4.4812e-04\n",
            "Epoch 64/82\n",
            "352/352 [==============================] - 2s 7ms/step - loss: 4.5104e-04\n",
            "Epoch 65/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.4269e-04\n",
            "Epoch 66/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.5161e-04\n",
            "Epoch 67/82\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 4.4733e-04\n",
            "Epoch 68/82\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 4.5947e-04\n",
            "Epoch 69/82\n",
            "352/352 [==============================] - 3s 7ms/step - loss: 4.3682e-04\n",
            "Epoch 70/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.4894e-04\n",
            "Epoch 71/82\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 4.4212e-04\n",
            "Epoch 72/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.4333e-04\n",
            "Epoch 73/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.4791e-04\n",
            "Epoch 74/82\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 4.6763e-04\n",
            "Epoch 75/82\n",
            "352/352 [==============================] - 2s 7ms/step - loss: 4.4738e-04\n",
            "Epoch 76/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.5237e-04\n",
            "Epoch 77/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.4054e-04\n",
            "Epoch 78/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.4518e-04\n",
            "Epoch 79/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.4075e-04\n",
            "Epoch 80/82\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 4.4155e-04\n",
            "Epoch 81/82\n",
            "352/352 [==============================] - 2s 7ms/step - loss: 4.4235e-04\n",
            "Epoch 82/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.5033e-04\n",
            "GRU Model Training Elapsed Time : 227.07894 seconds\n",
            "GRU Model Generate predictions Elapsed Time : 0.62042 seconds\n",
            "GRU Test RMSE : 0.02094\n",
            "GRU Test MAE : 0.01703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 4"
      ],
      "metadata": {
        "id": "QgPgs9K7TAop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "\n",
        "# 0.02686025211281613\n",
        "look_back = 56\n",
        "num_hidden_layers = 3\n",
        "learning_rate = 0.0056240033221932485\n",
        "batch_size = 193\n",
        "epochs = 78\n",
        "lstm_units_layer_0 = 37\n",
        "lstm_units_layer_1 = 82\n",
        "lstm_units_layer_2 = 49"
      ],
      "metadata": {
        "id": "HKWFOCibTAop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 4\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(GRU(units=lstm_units_layer_0, return_sequences=True))\n",
        "model.add(GRU(units=lstm_units_layer_1, return_sequences=True))\n",
        "model.add(GRU(units=lstm_units_layer_2))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "# testPredict = np.reshape(testPredict, (testPredict.shape[0], testPredict.shape[2]))\n",
        "# calculate root mean squared error\n",
        "GRU_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "GRU_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"GRU Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"GRU Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('GRU Test RMSE : %.5f' % (GRU_RMSE))\n",
        "print('GRU Test MAE : %.5f' % (GRU_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b00d79-0a84-42a1-ef16-de62afae0ce9",
        "id": "ob2yGyGBTAop"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17932, 1, 56)\n",
            "Epoch 1/78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93/93 [==============================] - 5s 6ms/step - loss: 0.0049\n",
            "Epoch 2/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0014\n",
            "Epoch 3/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0011\n",
            "Epoch 4/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 0.0010\n",
            "Epoch 5/78\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 9.6307e-04\n",
            "Epoch 6/78\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 9.6427e-04\n",
            "Epoch 7/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 9.1065e-04\n",
            "Epoch 8/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 9.2314e-04\n",
            "Epoch 9/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 8.7524e-04\n",
            "Epoch 10/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 8.8676e-04\n",
            "Epoch 11/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.6934e-04\n",
            "Epoch 12/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.9678e-04\n",
            "Epoch 13/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 8.4428e-04\n",
            "Epoch 14/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.7212e-04\n",
            "Epoch 15/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 9.0524e-04\n",
            "Epoch 16/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.2933e-04\n",
            "Epoch 17/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.2283e-04\n",
            "Epoch 18/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.2938e-04\n",
            "Epoch 19/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.8768e-04\n",
            "Epoch 20/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.8978e-04\n",
            "Epoch 21/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.9734e-04\n",
            "Epoch 22/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.0663e-04\n",
            "Epoch 23/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 8.4102e-04\n",
            "Epoch 24/78\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 8.0980e-04\n",
            "Epoch 25/78\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 7.8869e-04\n",
            "Epoch 26/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.9995e-04\n",
            "Epoch 27/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.7525e-04\n",
            "Epoch 28/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.1052e-04\n",
            "Epoch 29/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.1873e-04\n",
            "Epoch 30/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.5475e-04\n",
            "Epoch 31/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.6914e-04\n",
            "Epoch 32/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.9622e-04\n",
            "Epoch 33/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.9219e-04\n",
            "Epoch 34/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.7832e-04\n",
            "Epoch 35/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.0642e-04\n",
            "Epoch 36/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.0307e-04\n",
            "Epoch 37/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.9051e-04\n",
            "Epoch 38/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.7692e-04\n",
            "Epoch 39/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.6523e-04\n",
            "Epoch 40/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.7414e-04\n",
            "Epoch 41/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.5100e-04\n",
            "Epoch 42/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.8798e-04\n",
            "Epoch 43/78\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 7.9227e-04\n",
            "Epoch 44/78\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 7.2520e-04\n",
            "Epoch 45/78\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 8.1756e-04\n",
            "Epoch 46/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.3869e-04\n",
            "Epoch 47/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.4531e-04\n",
            "Epoch 48/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.6419e-04\n",
            "Epoch 49/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.4733e-04\n",
            "Epoch 50/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.9431e-04\n",
            "Epoch 51/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.4317e-04\n",
            "Epoch 52/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.4822e-04\n",
            "Epoch 53/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.2499e-04\n",
            "Epoch 54/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.4078e-04\n",
            "Epoch 55/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.1646e-04\n",
            "Epoch 56/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.5396e-04\n",
            "Epoch 57/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.7446e-04\n",
            "Epoch 58/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.7550e-04\n",
            "Epoch 59/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.4102e-04\n",
            "Epoch 60/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.2388e-04\n",
            "Epoch 61/78\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 7.2411e-04\n",
            "Epoch 62/78\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 7.4175e-04\n",
            "Epoch 63/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.3440e-04\n",
            "Epoch 64/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.3728e-04\n",
            "Epoch 65/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.3402e-04\n",
            "Epoch 66/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.3802e-04\n",
            "Epoch 67/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.5544e-04\n",
            "Epoch 68/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.5141e-04\n",
            "Epoch 69/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.3407e-04\n",
            "Epoch 70/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.4981e-04\n",
            "Epoch 71/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.4377e-04\n",
            "Epoch 72/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.2508e-04\n",
            "Epoch 73/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.3954e-04\n",
            "Epoch 74/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.4417e-04\n",
            "Epoch 75/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.1795e-04\n",
            "Epoch 76/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.1536e-04\n",
            "Epoch 77/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.3129e-04\n",
            "Epoch 78/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 7.3300e-04\n",
            "GRU Model Training Elapsed Time : 53.80654 seconds\n",
            "GRU Model Generate predictions Elapsed Time : 1.06034 seconds\n",
            "GRU Test RMSE : 0.02763\n",
            "GRU Test MAE : 0.02202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 6"
      ],
      "metadata": {
        "id": "v7mq-c23TAoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "\n",
        "look_back = 91\n",
        "num_hidden_layers = 6\n",
        "learning_rate = 0.0005124476844228014\n",
        "batch_size = 50\n",
        "epochs = 73\n",
        "lstm_units_layer_0 = 50\n",
        "lstm_units_layer_1 = 28\n",
        "lstm_units_layer_2 = 82\n",
        "lstm_units_layer_3 = 33\n",
        "lstm_units_layer_4 = 22\n",
        "lstm_units_layer_5 = 15\n",
        "# best_error: 0.03235792738580053\n"
      ],
      "metadata": {
        "id": "L2zjl0bRTAoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 6\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(GRU(units=lstm_units_layer_0, return_sequences=True))\n",
        "model.add(GRU(units=lstm_units_layer_1, return_sequences=True))\n",
        "model.add(GRU(units=lstm_units_layer_2, return_sequences=True))\n",
        "model.add(GRU(units=lstm_units_layer_3, return_sequences=True))\n",
        "model.add(GRU(units=lstm_units_layer_4, return_sequences=True))\n",
        "model.add(GRU(units=lstm_units_layer_5))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "# testPredict = np.reshape(testPredict, (testPredict.shape[0], testPredict.shape[2]))\n",
        "# calculate root mean squared error\n",
        "GRU_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "GRU_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"GRU Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"GRU Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('GRU Test RMSE : %.5f' % (GRU_RMSE))\n",
        "print('GRU Test MAE : %.5f' % (GRU_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d18bdc8e-dad1-4a6f-de89-8e83aa9f0bc8",
        "id": "qyuSWiZ1TAoq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17895, 1, 91)\n",
            "Epoch 1/73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "358/358 [==============================] - 12s 9ms/step - loss: 0.0091\n",
            "Epoch 2/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0022\n",
            "Epoch 3/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0017\n",
            "Epoch 4/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0015\n",
            "Epoch 5/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0013\n",
            "Epoch 6/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0013\n",
            "Epoch 7/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0013\n",
            "Epoch 8/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0012\n",
            "Epoch 9/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0012\n",
            "Epoch 10/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0012\n",
            "Epoch 11/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0012\n",
            "Epoch 12/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0011\n",
            "Epoch 13/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0012\n",
            "Epoch 14/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0011\n",
            "Epoch 15/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0011\n",
            "Epoch 16/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0011\n",
            "Epoch 17/73\n",
            "358/358 [==============================] - 3s 10ms/step - loss: 0.0011\n",
            "Epoch 18/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0011\n",
            "Epoch 19/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0011\n",
            "Epoch 20/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0011\n",
            "Epoch 21/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0011\n",
            "Epoch 22/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0011\n",
            "Epoch 23/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0011\n",
            "Epoch 24/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0011\n",
            "Epoch 25/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0011\n",
            "Epoch 26/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0011\n",
            "Epoch 27/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0011\n",
            "Epoch 28/73\n",
            "358/358 [==============================] - 3s 10ms/step - loss: 0.0011\n",
            "Epoch 29/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0011\n",
            "Epoch 30/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 31/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 32/73\n",
            "358/358 [==============================] - 3s 10ms/step - loss: 0.0011\n",
            "Epoch 33/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 34/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 35/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 36/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0010\n",
            "Epoch 37/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 38/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 39/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0010\n",
            "Epoch 40/73\n",
            "358/358 [==============================] - 3s 10ms/step - loss: 0.0010\n",
            "Epoch 41/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 42/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 43/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0010\n",
            "Epoch 44/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0010\n",
            "Epoch 45/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0010\n",
            "Epoch 46/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 47/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0010\n",
            "Epoch 48/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0010\n",
            "Epoch 49/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 50/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 51/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0010\n",
            "Epoch 52/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0010\n",
            "Epoch 53/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 9.9630e-04\n",
            "Epoch 54/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 55/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 0.0010\n",
            "Epoch 56/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 9.9070e-04\n",
            "Epoch 57/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 0.0010\n",
            "Epoch 58/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 9.9032e-04\n",
            "Epoch 59/73\n",
            "358/358 [==============================] - 4s 10ms/step - loss: 9.8904e-04\n",
            "Epoch 60/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 9.7956e-04\n",
            "Epoch 61/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 0.0010\n",
            "Epoch 62/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 9.9477e-04\n",
            "Epoch 63/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 9.8568e-04\n",
            "Epoch 64/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 9.8703e-04\n",
            "Epoch 65/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 9.8887e-04\n",
            "Epoch 66/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 9.8897e-04\n",
            "Epoch 67/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 9.8139e-04\n",
            "Epoch 68/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 9.8428e-04\n",
            "Epoch 69/73\n",
            "358/358 [==============================] - 3s 8ms/step - loss: 9.8439e-04\n",
            "Epoch 70/73\n",
            "358/358 [==============================] - 3s 10ms/step - loss: 9.6387e-04\n",
            "Epoch 71/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 9.8029e-04\n",
            "Epoch 72/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 9.8207e-04\n",
            "Epoch 73/73\n",
            "358/358 [==============================] - 3s 9ms/step - loss: 9.7529e-04\n",
            "GRU Model Training Elapsed Time : 269.90927 seconds\n",
            "GRU Model Generate predictions Elapsed Time : 1.51047 seconds\n",
            "GRU Test RMSE : 0.03163\n",
            "GRU Test MAE : 0.02483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 8"
      ],
      "metadata": {
        "id": "EDIMlTLkTAor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "look_back = 65\n",
        "num_hidden_layers = 3\n",
        "learning_rate = 0.00019511832310862206\n",
        "batch_size = 8\n",
        "epochs = 66\n",
        "lstm_units_layer_0 = 8\n",
        "lstm_units_layer_1 = 63\n",
        "lstm_units_layer_2 = 240\n",
        "# best_error: 0.036570963652281366"
      ],
      "metadata": {
        "id": "uJ486eSPTAor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 8\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(GRU(units=lstm_units_layer_0, return_sequences=True))\n",
        "model.add(GRU(units=lstm_units_layer_1, return_sequences=True))\n",
        "model.add(GRU(units=lstm_units_layer_2))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "# testPredict = np.reshape(testPredict, (testPredict.shape[0], testPredict.shape[2]))\n",
        "# calculate root mean squared error\n",
        "GRU_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "GRU_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"GRU Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"GRU Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('GRU Test RMSE : %.5f' % (GRU_RMSE))\n",
        "print('GRU Test MAE : %.5f' % (GRU_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea803261-aa95-4d3e-e447-f66facdc3832",
        "id": "kq-xilDpTAor"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17919, 1, 65)\n",
            "Epoch 1/66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2240/2240 [==============================] - 17s 5ms/step - loss: 0.0039\n",
            "Epoch 2/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0020\n",
            "Epoch 3/66\n",
            "2240/2240 [==============================] - 12s 6ms/step - loss: 0.0018\n",
            "Epoch 4/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0016\n",
            "Epoch 5/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0015\n",
            "Epoch 6/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0015\n",
            "Epoch 7/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0015\n",
            "Epoch 8/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 9/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 10/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 11/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 12/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 13/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 14/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 15/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 16/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 17/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0014\n",
            "Epoch 18/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 19/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 20/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 21/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 22/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 23/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 24/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 25/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 26/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 27/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 28/66\n",
            "2240/2240 [==============================] - 12s 6ms/step - loss: 0.0013\n",
            "Epoch 29/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 30/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 31/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 32/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 33/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 34/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 35/66\n",
            "2240/2240 [==============================] - 12s 6ms/step - loss: 0.0013\n",
            "Epoch 36/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 37/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 38/66\n",
            "2240/2240 [==============================] - 12s 6ms/step - loss: 0.0013\n",
            "Epoch 39/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 40/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 41/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 42/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 43/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 44/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 45/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 46/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 47/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 48/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 49/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 50/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 51/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 52/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 53/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 54/66\n",
            "2240/2240 [==============================] - 12s 6ms/step - loss: 0.0013\n",
            "Epoch 55/66\n",
            "2240/2240 [==============================] - 13s 6ms/step - loss: 0.0013\n",
            "Epoch 56/66\n",
            "2240/2240 [==============================] - 12s 6ms/step - loss: 0.0013\n",
            "Epoch 57/66\n",
            "2240/2240 [==============================] - 13s 6ms/step - loss: 0.0013\n",
            "Epoch 58/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 59/66\n",
            "2240/2240 [==============================] - 12s 6ms/step - loss: 0.0013\n",
            "Epoch 60/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 61/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 62/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 63/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 64/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 65/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "Epoch 66/66\n",
            "2240/2240 [==============================] - 12s 5ms/step - loss: 0.0013\n",
            "GRU Model Training Elapsed Time : 802.84731 seconds\n",
            "GRU Model Generate predictions Elapsed Time : 0.85968 seconds\n",
            "GRU Test RMSE : 0.03645\n",
            "GRU Test MAE : 0.02841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 10"
      ],
      "metadata": {
        "id": "jCvGlURqTAos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "\n",
        "look_back = 149\n",
        "num_hidden_layers = 1\n",
        "learning_rate = 0.0024519997428423335\n",
        "batch_size = 83\n",
        "epochs = 99\n",
        "lstm_units_layer_0 = 51\n",
        "# best_error: 0.04012357832510543"
      ],
      "metadata": {
        "id": "O6VKJQ0UTAos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 10\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(GRU(units=lstm_units_layer_0))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "# testPredict = np.reshape(testPredict, (testPredict.shape[0], testPredict.shape[2]))\n",
        "# calculate root mean squared error\n",
        "GRU_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "GRU_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"GRU Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"GRU Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('GRU Test RMSE : %.5f' % (GRU_RMSE))\n",
        "print('GRU Test MAE : %.5f' % (GRU_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb4eff9-7378-47f6-98ae-e86df08360b9",
        "id": "UBVp57R_TAos"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17833, 1, 149)\n",
            "Epoch 1/99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 [==============================] - 2s 3ms/step - loss: 0.0046\n",
            "Epoch 2/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0022\n",
            "Epoch 3/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0020\n",
            "Epoch 4/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0019\n",
            "Epoch 5/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0018\n",
            "Epoch 6/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0018\n",
            "Epoch 7/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0017\n",
            "Epoch 8/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Epoch 9/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0017\n",
            "Epoch 10/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 11/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0016\n",
            "Epoch 12/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0016\n",
            "Epoch 13/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 14/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 15/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 16/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 17/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 18/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 19/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 20/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 21/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 22/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 23/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 24/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 25/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 26/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 27/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 28/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 29/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 30/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0015\n",
            "Epoch 31/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0016\n",
            "Epoch 32/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 33/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 34/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 35/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 36/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 37/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 38/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 39/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 40/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 41/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 42/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 43/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 44/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 45/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 46/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 47/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 48/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 49/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 50/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0015\n",
            "Epoch 51/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0015\n",
            "Epoch 52/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 53/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 54/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 55/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 56/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 57/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 58/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 59/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 60/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 61/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 62/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 63/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 64/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 65/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 66/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 67/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 68/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 69/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0014\n",
            "Epoch 70/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0015\n",
            "Epoch 71/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 72/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 73/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 74/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 75/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 76/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 77/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 78/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 79/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 80/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 81/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 82/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 83/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 84/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 85/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 86/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 87/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 88/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 89/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0014\n",
            "Epoch 90/99\n",
            "215/215 [==============================] - 1s 4ms/step - loss: 0.0014\n",
            "Epoch 91/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 92/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 93/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 94/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 95/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 96/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 97/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 98/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 99/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "GRU Model Training Elapsed Time : 62.30974 seconds\n",
            "GRU Model Generate predictions Elapsed Time : 0.36032 seconds\n",
            "GRU Test RMSE : 0.03955\n",
            "GRU Test MAE : 0.03064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mp3j0E1zT3q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Model"
      ],
      "metadata": {
        "id": "6t9VRL9lZjAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 2"
      ],
      "metadata": {
        "id": "Vt3N-GYVZjA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "\n",
        "look_back = 51\n",
        "num_hidden_layers = 2\n",
        "learning_rate = 0.001519335526892746\n",
        "batch_size = 51\n",
        "epochs = 82\n",
        "lstm_units_layer_0 = 96\n",
        "lstm_units_layer_1 = 208\n",
        "# best_error: 0.020879777783223082"
      ],
      "metadata": {
        "id": "9nwDxFmLZjA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 2\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=lstm_units_layer_0, return_sequences=True))\n",
        "model.add(SimpleRNN(units=lstm_units_layer_1))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "\n",
        "# calculate root mean squared error\n",
        "RNN_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "RNN_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"RNN Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"RNN Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('RNN Test RMSE : %.5f' % (RNN_RMSE))\n",
        "print('RNN Test MAE : %.5f' % (RNN_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14278620-e782-4dfa-e7cb-396ad29e4244",
        "id": "ajQ9N2HEZjA-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17939, 1, 51)\n",
            "Epoch 1/82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "352/352 [==============================] - 2s 3ms/step - loss: 0.0030\n",
            "Epoch 2/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 6.6877e-04\n",
            "Epoch 3/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.9263e-04\n",
            "Epoch 4/82\n",
            "352/352 [==============================] - 2s 4ms/step - loss: 5.9260e-04\n",
            "Epoch 5/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.9019e-04\n",
            "Epoch 6/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.3943e-04\n",
            "Epoch 7/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.6368e-04\n",
            "Epoch 8/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.5624e-04\n",
            "Epoch 9/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.4106e-04\n",
            "Epoch 10/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.7856e-04\n",
            "Epoch 11/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.3231e-04\n",
            "Epoch 12/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.5125e-04\n",
            "Epoch 13/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.3848e-04\n",
            "Epoch 14/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.4402e-04\n",
            "Epoch 15/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 5.2972e-04\n",
            "Epoch 16/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.2964e-04\n",
            "Epoch 17/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.1880e-04\n",
            "Epoch 18/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.2153e-04\n",
            "Epoch 19/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.2177e-04\n",
            "Epoch 20/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.0877e-04\n",
            "Epoch 21/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.4646e-04\n",
            "Epoch 22/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.2916e-04\n",
            "Epoch 23/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.9518e-04\n",
            "Epoch 24/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.0981e-04\n",
            "Epoch 25/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.0480e-04\n",
            "Epoch 26/82\n",
            "352/352 [==============================] - 2s 4ms/step - loss: 4.9062e-04\n",
            "Epoch 27/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.0082e-04\n",
            "Epoch 28/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.1991e-04\n",
            "Epoch 29/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.0911e-04\n",
            "Epoch 30/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.9681e-04\n",
            "Epoch 31/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.9964e-04\n",
            "Epoch 32/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 5.0411e-04\n",
            "Epoch 33/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.7340e-04\n",
            "Epoch 34/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.8142e-04\n",
            "Epoch 35/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.7382e-04\n",
            "Epoch 36/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6906e-04\n",
            "Epoch 37/82\n",
            "352/352 [==============================] - 1s 4ms/step - loss: 4.9235e-04\n",
            "Epoch 38/82\n",
            "352/352 [==============================] - 1s 4ms/step - loss: 4.7319e-04\n",
            "Epoch 39/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.8377e-04\n",
            "Epoch 40/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.8625e-04\n",
            "Epoch 41/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.7360e-04\n",
            "Epoch 42/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.9154e-04\n",
            "Epoch 43/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.7001e-04\n",
            "Epoch 44/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.8026e-04\n",
            "Epoch 45/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.8303e-04\n",
            "Epoch 46/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.7137e-04\n",
            "Epoch 47/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6327e-04\n",
            "Epoch 48/82\n",
            "352/352 [==============================] - 2s 4ms/step - loss: 4.8002e-04\n",
            "Epoch 49/82\n",
            "352/352 [==============================] - 1s 4ms/step - loss: 4.8370e-04\n",
            "Epoch 50/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6642e-04\n",
            "Epoch 51/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.5419e-04\n",
            "Epoch 52/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.7231e-04\n",
            "Epoch 53/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.5721e-04\n",
            "Epoch 54/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6522e-04\n",
            "Epoch 55/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.7501e-04\n",
            "Epoch 56/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6453e-04\n",
            "Epoch 57/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6473e-04\n",
            "Epoch 58/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6185e-04\n",
            "Epoch 59/82\n",
            "352/352 [==============================] - 1s 4ms/step - loss: 4.5949e-04\n",
            "Epoch 60/82\n",
            "352/352 [==============================] - 1s 4ms/step - loss: 4.4911e-04\n",
            "Epoch 61/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.4982e-04\n",
            "Epoch 62/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6894e-04\n",
            "Epoch 63/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6046e-04\n",
            "Epoch 64/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6205e-04\n",
            "Epoch 65/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.5606e-04\n",
            "Epoch 66/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.5912e-04\n",
            "Epoch 67/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.5824e-04\n",
            "Epoch 68/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6028e-04\n",
            "Epoch 69/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.4549e-04\n",
            "Epoch 70/82\n",
            "352/352 [==============================] - 1s 4ms/step - loss: 4.6696e-04\n",
            "Epoch 71/82\n",
            "352/352 [==============================] - 1s 4ms/step - loss: 4.5325e-04\n",
            "Epoch 72/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.5192e-04\n",
            "Epoch 73/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6757e-04\n",
            "Epoch 74/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6952e-04\n",
            "Epoch 75/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.5436e-04\n",
            "Epoch 76/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.5924e-04\n",
            "Epoch 77/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.4566e-04\n",
            "Epoch 78/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.4774e-04\n",
            "Epoch 79/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.6204e-04\n",
            "Epoch 80/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.4110e-04\n",
            "Epoch 81/82\n",
            "352/352 [==============================] - 1s 3ms/step - loss: 4.4352e-04\n",
            "Epoch 82/82\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 4.5501e-04\n",
            "RNN Model Training Elapsed Time : 143.01586 seconds\n",
            "RNN Model Generate predictions Elapsed Time : 0.28063 seconds\n",
            "RNN Test RMSE : 0.02076\n",
            "RNN Test MAE : 0.01686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 4"
      ],
      "metadata": {
        "id": "IRSDyZPbZjA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "\n",
        "# 0.02686025211281613\n",
        "look_back = 56\n",
        "num_hidden_layers = 3\n",
        "learning_rate = 0.0056240033221932485\n",
        "batch_size = 193\n",
        "epochs = 78\n",
        "lstm_units_layer_0 = 37\n",
        "lstm_units_layer_1 = 82\n",
        "lstm_units_layer_2 = 49"
      ],
      "metadata": {
        "id": "bvaJDE3fZjA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 4\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=lstm_units_layer_0, return_sequences=True))\n",
        "model.add(SimpleRNN(units=lstm_units_layer_1, return_sequences=True))\n",
        "model.add(SimpleRNN(units=lstm_units_layer_2))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "# testPredict = np.reshape(testPredict, (testPredict.shape[0], testPredict.shape[2]))\n",
        "# calculate root mean squared error\n",
        "RNN_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "RNN_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"RNN Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"RNN Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('RNN Test RMSE : %.5f' % (RNN_RMSE))\n",
        "print('RNN Test MAE : %.5f' % (RNN_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6476839-f6a3-4ad0-b6c5-528fb1957e75",
        "id": "s2Pt1It3ZjBA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17932, 1, 56)\n",
            "Epoch 1/78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93/93 [==============================] - 4s 6ms/step - loss: 0.0183\n",
            "Epoch 2/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 0.0013\n",
            "Epoch 3/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0011\n",
            "Epoch 4/78\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 9.7712e-04\n",
            "Epoch 5/78\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 9.6193e-04\n",
            "Epoch 6/78\n",
            "93/93 [==============================] - 0s 3ms/step - loss: 9.3606e-04\n",
            "Epoch 7/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 8.9329e-04\n",
            "Epoch 8/78\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 9.4824e-04\n",
            "Epoch 9/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 8.9808e-04\n",
            "Epoch 10/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 8.6781e-04\n",
            "Epoch 11/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 8.3249e-04\n",
            "Epoch 12/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 8.5906e-04\n",
            "Epoch 13/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 8.4572e-04\n",
            "Epoch 14/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.2773e-04\n",
            "Epoch 15/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.9791e-04\n",
            "Epoch 16/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.1417e-04\n",
            "Epoch 17/78\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 8.5826e-04\n",
            "Epoch 18/78\n",
            "93/93 [==============================] - 0s 4ms/step - loss: 8.2195e-04\n",
            "Epoch 19/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.0133e-04\n",
            "Epoch 20/78\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 8.1154e-04\n",
            "Epoch 21/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.0212e-04\n",
            "Epoch 22/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 8.6366e-04\n",
            "Epoch 23/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.3066e-04\n",
            "Epoch 24/78\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 8.0880e-04\n",
            "Epoch 25/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.1029e-04\n",
            "Epoch 26/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.8905e-04\n",
            "Epoch 27/78\n",
            "93/93 [==============================] - 0s 4ms/step - loss: 7.8897e-04\n",
            "Epoch 28/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 8.8065e-04\n",
            "Epoch 29/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.4428e-04\n",
            "Epoch 30/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.7398e-04\n",
            "Epoch 31/78\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 7.9526e-04\n",
            "Epoch 32/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.8606e-04\n",
            "Epoch 33/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.7949e-04\n",
            "Epoch 34/78\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 8.0435e-04\n",
            "Epoch 35/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.2536e-04\n",
            "Epoch 36/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.2258e-04\n",
            "Epoch 37/78\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 8.0393e-04\n",
            "Epoch 38/78\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 8.0994e-04\n",
            "Epoch 39/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 8.0942e-04\n",
            "Epoch 40/78\n",
            "93/93 [==============================] - 0s 4ms/step - loss: 7.8365e-04\n",
            "Epoch 41/78\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 7.6337e-04\n",
            "Epoch 42/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.6392e-04\n",
            "Epoch 43/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.5609e-04\n",
            "Epoch 44/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.5107e-04\n",
            "Epoch 45/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.3963e-04\n",
            "Epoch 46/78\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 7.7683e-04\n",
            "Epoch 47/78\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 7.5837e-04\n",
            "Epoch 48/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.7504e-04\n",
            "Epoch 49/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 8.0682e-04\n",
            "Epoch 50/78\n",
            "93/93 [==============================] - 0s 4ms/step - loss: 8.3995e-04\n",
            "Epoch 51/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.5893e-04\n",
            "Epoch 52/78\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 7.6393e-04\n",
            "Epoch 53/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.3301e-04\n",
            "Epoch 54/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.6879e-04\n",
            "Epoch 55/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.6670e-04\n",
            "Epoch 56/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.7513e-04\n",
            "Epoch 57/78\n",
            "93/93 [==============================] - 0s 4ms/step - loss: 7.7863e-04\n",
            "Epoch 58/78\n",
            "93/93 [==============================] - 0s 4ms/step - loss: 7.7902e-04\n",
            "Epoch 59/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.4818e-04\n",
            "Epoch 60/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.7072e-04\n",
            "Epoch 61/78\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 7.4928e-04\n",
            "Epoch 62/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.5106e-04\n",
            "Epoch 63/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.7730e-04\n",
            "Epoch 64/78\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 8.0344e-04\n",
            "Epoch 65/78\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 7.6055e-04\n",
            "Epoch 66/78\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 7.8475e-04\n",
            "Epoch 67/78\n",
            "93/93 [==============================] - 0s 4ms/step - loss: 7.5552e-04\n",
            "Epoch 68/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.6479e-04\n",
            "Epoch 69/78\n",
            "93/93 [==============================] - 0s 5ms/step - loss: 7.6315e-04\n",
            "Epoch 70/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.7713e-04\n",
            "Epoch 71/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.4540e-04\n",
            "Epoch 72/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.7872e-04\n",
            "Epoch 73/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.4741e-04\n",
            "Epoch 74/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.5287e-04\n",
            "Epoch 75/78\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 7.4811e-04\n",
            "Epoch 76/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.4645e-04\n",
            "Epoch 77/78\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 7.6384e-04\n",
            "Epoch 78/78\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 7.5720e-04\n",
            "RNN Model Training Elapsed Time : 47.93892 seconds\n",
            "RNN Model Generate predictions Elapsed Time : 0.45329 seconds\n",
            "RNN Test RMSE : 0.02788\n",
            "RNN Test MAE : 0.02198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 6"
      ],
      "metadata": {
        "id": "TMamAP_NZjBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "\n",
        "look_back = 91\n",
        "num_hidden_layers = 6\n",
        "learning_rate = 0.0005124476844228014\n",
        "batch_size = 50\n",
        "epochs = 73\n",
        "lstm_units_layer_0 = 50\n",
        "lstm_units_layer_1 = 28\n",
        "lstm_units_layer_2 = 82\n",
        "lstm_units_layer_3 = 33\n",
        "lstm_units_layer_4 = 22\n",
        "lstm_units_layer_5 = 15\n",
        "# best_error: 0.03235792738580053\n"
      ],
      "metadata": {
        "id": "DiuFXL-gZjBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 6\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=lstm_units_layer_0, return_sequences=True))\n",
        "model.add(SimpleRNN(units=lstm_units_layer_1, return_sequences=True))\n",
        "model.add(SimpleRNN(units=lstm_units_layer_2, return_sequences=True))\n",
        "model.add(SimpleRNN(units=lstm_units_layer_3, return_sequences=True))\n",
        "model.add(SimpleRNN(units=lstm_units_layer_4, return_sequences=True))\n",
        "model.add(SimpleRNN(units=lstm_units_layer_5))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "# testPredict = np.reshape(testPredict, (testPredict.shape[0], testPredict.shape[2]))\n",
        "# calculate root mean squared error\n",
        "RNN_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "RNN_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"RNN Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"RNN Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('RNN Test RMSE : %.5f' % (RNN_RMSE))\n",
        "print('RNN Test MAE : %.5f' % (RNN_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b37db8-d5b2-4c38-ecba-490efad2b999",
        "id": "H0KfkDB4ZjBC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17895, 1, 91)\n",
            "Epoch 1/73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "358/358 [==============================] - 7s 5ms/step - loss: 0.0036\n",
            "Epoch 2/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0018\n",
            "Epoch 3/73\n",
            "358/358 [==============================] - 2s 5ms/step - loss: 0.0016\n",
            "Epoch 4/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 0.0014\n",
            "Epoch 5/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0014\n",
            "Epoch 6/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0013\n",
            "Epoch 7/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0013\n",
            "Epoch 8/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 9/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 10/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 11/73\n",
            "358/358 [==============================] - 2s 6ms/step - loss: 0.0012\n",
            "Epoch 12/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 13/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 14/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0011\n",
            "Epoch 15/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 16/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 0.0011\n",
            "Epoch 17/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 0.0011\n",
            "Epoch 18/73\n",
            "358/358 [==============================] - 2s 5ms/step - loss: 0.0011\n",
            "Epoch 19/73\n",
            "358/358 [==============================] - 2s 5ms/step - loss: 0.0011\n",
            "Epoch 20/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0011\n",
            "Epoch 21/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0011\n",
            "Epoch 22/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0011\n",
            "Epoch 23/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0011\n",
            "Epoch 24/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0011\n",
            "Epoch 25/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 0.0011\n",
            "Epoch 26/73\n",
            "358/358 [==============================] - 2s 6ms/step - loss: 0.0011\n",
            "Epoch 27/73\n",
            "358/358 [==============================] - 2s 5ms/step - loss: 0.0011\n",
            "Epoch 28/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 0.0011\n",
            "Epoch 29/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 0.0010\n",
            "Epoch 30/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0011\n",
            "Epoch 31/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 0.0011\n",
            "Epoch 32/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 0.0010\n",
            "Epoch 33/73\n",
            "358/358 [==============================] - 2s 7ms/step - loss: 0.0011\n",
            "Epoch 34/73\n",
            "358/358 [==============================] - 2s 5ms/step - loss: 0.0010\n",
            "Epoch 35/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 0.0010\n",
            "Epoch 36/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 37/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 38/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 39/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 40/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 41/73\n",
            "358/358 [==============================] - 2s 6ms/step - loss: 0.0010\n",
            "Epoch 42/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 43/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 44/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 45/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 0.0010\n",
            "Epoch 46/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 9.9929e-04\n",
            "Epoch 47/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 0.0010\n",
            "Epoch 48/73\n",
            "358/358 [==============================] - 2s 5ms/step - loss: 0.0010\n",
            "Epoch 49/73\n",
            "358/358 [==============================] - 2s 5ms/step - loss: 0.0010\n",
            "Epoch 50/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 51/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 52/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 53/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 9.9250e-04\n",
            "Epoch 54/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 55/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 0.0010\n",
            "Epoch 56/73\n",
            "358/358 [==============================] - 2s 6ms/step - loss: 9.9152e-04\n",
            "Epoch 57/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 0.0010\n",
            "Epoch 58/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 9.8931e-04\n",
            "Epoch 59/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 9.8592e-04\n",
            "Epoch 60/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 9.7078e-04\n",
            "Epoch 61/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 62/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 9.8472e-04\n",
            "Epoch 63/73\n",
            "358/358 [==============================] - 2s 5ms/step - loss: 9.8135e-04\n",
            "Epoch 64/73\n",
            "358/358 [==============================] - 2s 5ms/step - loss: 9.8324e-04\n",
            "Epoch 65/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 9.8127e-04\n",
            "Epoch 66/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 9.6807e-04\n",
            "Epoch 67/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 9.8145e-04\n",
            "Epoch 68/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 9.7693e-04\n",
            "Epoch 69/73\n",
            "358/358 [==============================] - 2s 4ms/step - loss: 9.8153e-04\n",
            "Epoch 70/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 9.5885e-04\n",
            "Epoch 71/73\n",
            "358/358 [==============================] - 2s 6ms/step - loss: 9.6651e-04\n",
            "Epoch 72/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 9.8481e-04\n",
            "Epoch 73/73\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 9.8062e-04\n",
            "RNN Model Training Elapsed Time : 122.15408 seconds\n",
            "RNN Model Generate predictions Elapsed Time : 0.61484 seconds\n",
            "RNN Test RMSE : 0.03201\n",
            "RNN Test MAE : 0.02526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 8"
      ],
      "metadata": {
        "id": "kDpFQ5fRZjBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "look_back = 65\n",
        "num_hidden_layers = 3\n",
        "learning_rate = 0.00019511832310862206\n",
        "batch_size = 8\n",
        "epochs = 66\n",
        "lstm_units_layer_0 = 8\n",
        "lstm_units_layer_1 = 63\n",
        "lstm_units_layer_2 = 240\n",
        "# best_error: 0.036570963652281366"
      ],
      "metadata": {
        "id": "W4Hmkd1JZjBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 8\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=lstm_units_layer_0, return_sequences=True))\n",
        "model.add(SimpleRNN(units=lstm_units_layer_1, return_sequences=True))\n",
        "model.add(SimpleRNN(units=lstm_units_layer_2))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "# testPredict = np.reshape(testPredict, (testPredict.shape[0], testPredict.shape[2]))\n",
        "# calculate root mean squared error\n",
        "RNN_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "RNN_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"RNN Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"RNN Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('RNN Test RMSE : %.5f' % (RNN_RMSE))\n",
        "print('RNN Test MAE : %.5f' % (RNN_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68664b8d-e166-43d7-8ddd-f4a255405834",
        "id": "k0s4z39fZjBD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17919, 1, 65)\n",
            "Epoch 1/66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2240/2240 [==============================] - 8s 3ms/step - loss: 0.0024\n",
            "Epoch 2/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0017\n",
            "Epoch 3/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0016\n",
            "Epoch 4/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0015\n",
            "Epoch 5/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0015\n",
            "Epoch 6/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0015\n",
            "Epoch 7/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0014\n",
            "Epoch 8/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0014\n",
            "Epoch 9/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0014\n",
            "Epoch 10/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0014\n",
            "Epoch 11/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0014\n",
            "Epoch 12/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0014\n",
            "Epoch 13/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0014\n",
            "Epoch 14/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0014\n",
            "Epoch 15/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0014\n",
            "Epoch 16/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0014\n",
            "Epoch 17/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0014\n",
            "Epoch 18/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 19/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0013\n",
            "Epoch 20/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 21/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 22/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 23/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 24/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0013\n",
            "Epoch 25/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 26/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0013\n",
            "Epoch 27/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 28/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 29/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 30/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 31/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 32/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 33/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 34/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 35/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 36/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 37/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 38/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 39/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 40/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 41/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 42/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 43/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 44/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0013\n",
            "Epoch 45/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 46/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 47/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 48/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0013\n",
            "Epoch 49/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 50/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 51/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 52/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 53/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0013\n",
            "Epoch 54/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 55/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0013\n",
            "Epoch 56/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 57/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0013\n",
            "Epoch 58/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0013\n",
            "Epoch 59/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0012\n",
            "Epoch 60/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0012\n",
            "Epoch 61/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0012\n",
            "Epoch 62/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0012\n",
            "Epoch 63/66\n",
            "2240/2240 [==============================] - 7s 3ms/step - loss: 0.0012\n",
            "Epoch 64/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0012\n",
            "Epoch 65/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0012\n",
            "Epoch 66/66\n",
            "2240/2240 [==============================] - 6s 3ms/step - loss: 0.0012\n",
            "RNN Model Training Elapsed Time : 413.02616 seconds\n",
            "RNN Model Generate predictions Elapsed Time : 0.39325 seconds\n",
            "RNN Test RMSE : 0.03629\n",
            "RNN Test MAE : 0.02826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 10"
      ],
      "metadata": {
        "id": "9EpecFmfZjBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "\n",
        "look_back = 149\n",
        "num_hidden_layers = 1\n",
        "learning_rate = 0.0024519997428423335\n",
        "batch_size = 83\n",
        "epochs = 99\n",
        "lstm_units_layer_0 = 51\n",
        "# best_error: 0.04012357832510543"
      ],
      "metadata": {
        "id": "6QEUlan_ZjBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 10\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape)\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=lstm_units_layer_0))\n",
        "model.add(Dense(horizon))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "testPredict = model.predict(testX, verbose=0)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "# testPredict = np.reshape(testPredict, (testPredict.shape[0], testPredict.shape[2]))\n",
        "# calculate root mean squared error\n",
        "RNN_RMSE = np.sqrt(mean_squared_error(testY, testPredict))\n",
        "RNN_MAE = mean_absolute_error(testY, testPredict)\n",
        "\n",
        "print(\"RNN Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"RNN Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('RNN Test RMSE : %.5f' % (RNN_RMSE))\n",
        "print('RNN Test MAE : %.5f' % (RNN_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c28c9c2b-69ab-48a5-e619-fc589b4fecc7",
        "id": "IrGzlbtqZjBF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "(17833, 1, 149)\n",
            "Epoch 1/99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0085\n",
            "Epoch 2/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Epoch 3/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 4/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 5/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 6/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 7/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 8/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 9/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 10/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 11/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 12/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 13/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 14/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 15/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 16/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 17/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 18/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 19/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 20/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 21/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 22/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 23/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 24/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 25/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 26/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 27/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 28/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Epoch 29/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 30/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Epoch 31/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 32/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 33/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 34/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 35/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 36/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 37/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 38/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 39/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 40/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 41/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 42/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 43/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 44/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 45/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 46/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 47/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 48/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 49/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 50/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 51/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 52/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 53/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 54/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 55/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 56/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 57/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 58/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Epoch 59/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 60/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 61/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Epoch 62/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 63/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 64/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 65/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 66/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 67/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 68/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 69/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 70/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 71/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 72/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 73/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 74/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 75/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 76/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 77/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 78/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 79/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 80/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 81/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 82/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 83/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 84/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 85/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 86/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 87/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 88/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 89/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 90/99\n",
            "215/215 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Epoch 91/99\n",
            "215/215 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 92/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 93/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 94/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 95/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 96/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 97/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 98/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 99/99\n",
            "215/215 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "RNN Model Training Elapsed Time : 41.79832 seconds\n",
            "RNN Model Generate predictions Elapsed Time : 0.18965 seconds\n",
            "RNN Test RMSE : 0.04055\n",
            "RNN Test MAE : 0.03147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ESN Model"
      ],
      "metadata": {
        "id": "RAY9D4WajXlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Finding Best Hyperparameters"
      ],
      "metadata": {
        "id": "KAvFdOcPjvwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 2\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "valid_size = int(len(train) * 0.1)\n",
        "temp_train = train[:-valid_size]\n",
        "valid = train[-valid_size:]\n",
        "print(len(train), len(temp_train), len(valid), len(test))\n",
        "\n",
        "# Define the objective function for Optuna optimization\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters to search\n",
        "\n",
        "    n_reservoir = trial.suggest_int('n_reservoir', 10, 1000)   # -\n",
        "    sparsity = trial.suggest_categorical('sparsity', [0.01, 0.1, 0.2, 0.3, 0.4, 0.5])   # -\n",
        "    spectral_radius = trial.suggest_categorical('spectral_radius', [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.25, 10.0])   # - spectral radius of W\n",
        "    noise = trial.suggest_categorical('noise', [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009])       # - Noise Set\n",
        "    look_back = trial.suggest_int('look_back', 10, 500)\n",
        "\n",
        "    # reshape into X=t and Y=t+1\n",
        "    trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "    validX, validY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "    # reshape input to be [samples, time steps, features]\n",
        "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1]))\n",
        "    validX = np.reshape(validX, (validX.shape[0], validX.shape[1]))\n",
        "\n",
        "    model = ESN(n_inputs = look_back,\n",
        "            n_outputs = horizon,\n",
        "            n_reservoir = n_reservoir,\n",
        "            sparsity=sparsity,\n",
        "            random_state=1234,\n",
        "            spectral_radius=spectral_radius,\n",
        "            noise = noise,\n",
        "            silent = False,\n",
        "            teacher_scaling = 10,\n",
        "    )\n",
        "\n",
        "    pred_train = model.fit(trainX, trainY)\n",
        "\n",
        "    # make predictions\n",
        "    testPredict = model.predict(validX)\n",
        "    testPredict = np.array(testPredict)\n",
        "\n",
        "    testRMSE = np.sqrt(mean_squared_error(validY, testPredict[:len(validY), :]))\n",
        "\n",
        "    return testRMSE\n",
        "\n",
        "\n",
        "# Create the Optuna study\n",
        "study = optuna.create_study(direction='minimize')\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "start_time = time.time()\n",
        "\n",
        "# Run the optimization\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Record the ending time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print('done')\n",
        "print(\"ESN HyperParameters Tuning Elapsed Time : %.5f\" % (elapsed_time), \"seconds\")\n",
        "\n",
        "# Print the best parameters and corresponding loss\n",
        "best_params = study.best_params\n",
        "best_loss = study.best_value\n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Best Loss: \", best_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hNn9X6pae71",
        "outputId": "5371479c-64a4-4450-dd04-43863f38d560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:18:35,699] A new study created in memory with name: no-name-ddc9082b-727e-4ba8-a494-5efd51d9f595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 16192 1799 2000\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.010833627462823557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:18:44,487] Trial 0 finished with value: 0.021963175200418433 and parameters: {'n_reservoir': 776, 'sparsity': 0.2, 'spectral_radius': 0.9, 'noise': 0.004, 'look_back': 129}. Best is trial 0 with value: 0.021963175200418433.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.011537171838086738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:18:54,159] Trial 1 finished with value: 0.022200259535445747 and parameters: {'n_reservoir': 808, 'sparsity': 0.4, 'spectral_radius': 0.9, 'noise': 0.007, 'look_back': 36}. Best is trial 0 with value: 0.021963175200418433.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.010760734709673437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:19:06,110] Trial 2 finished with value: 0.020592423955412432 and parameters: {'n_reservoir': 787, 'sparsity': 0.01, 'spectral_radius': 0.2, 'noise': 0.007, 'look_back': 167}. Best is trial 2 with value: 0.020592423955412432.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.011398855102870763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:19:16,361] Trial 3 finished with value: 0.020830770063226017 and parameters: {'n_reservoir': 663, 'sparsity': 0.1, 'spectral_radius': 0.4, 'noise': 0.009, 'look_back': 44}. Best is trial 2 with value: 0.020592423955412432.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.010851265525167264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:19:26,519] Trial 4 finished with value: 0.024507781600122843 and parameters: {'n_reservoir': 529, 'sparsity': 0.01, 'spectral_radius': 1, 'noise': 0.004, 'look_back': 227}. Best is trial 2 with value: 0.020592423955412432.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.010851832315677656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:19:37,760] Trial 5 finished with value: 0.020388544925829116 and parameters: {'n_reservoir': 666, 'sparsity': 0.1, 'spectral_radius': 0.2, 'noise': 0.009, 'look_back': 131}. Best is trial 5 with value: 0.020388544925829116.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.010701538449747486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:19:50,120] Trial 6 finished with value: 0.021880718635221962 and parameters: {'n_reservoir': 831, 'sparsity': 0.2, 'spectral_radius': 1.25, 'noise': 0.003, 'look_back': 358}. Best is trial 5 with value: 0.020388544925829116.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.010642239303138398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:01,931] Trial 7 finished with value: 0.022095039243012128 and parameters: {'n_reservoir': 945, 'sparsity': 0.3, 'spectral_radius': 0.5, 'noise': 0.003, 'look_back': 213}. Best is trial 5 with value: 0.020388544925829116.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.010885922155525774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:07,790] Trial 8 finished with value: 0.02231394115030618 and parameters: {'n_reservoir': 497, 'sparsity': 0.4, 'spectral_radius': 1.1, 'noise': 0.003, 'look_back': 297}. Best is trial 5 with value: 0.020388544925829116.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:11,870] Trial 9 finished with value: 0.021578041703977847 and parameters: {'n_reservoir': 394, 'sparsity': 0.2, 'spectral_radius': 0.6, 'noise': 0.003, 'look_back': 308}. Best is trial 5 with value: 0.020388544925829116.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.010818684966863888\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:14,154] Trial 10 finished with value: 0.021770673819436728 and parameters: {'n_reservoir': 71, 'sparsity': 0.1, 'spectral_radius': 0.2, 'noise': 0.005, 'look_back': 499}. Best is trial 5 with value: 0.020388544925829116.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01224502860382366\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.010710936711682058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:26,948] Trial 11 finished with value: 0.02121712161609372 and parameters: {'n_reservoir': 999, 'sparsity': 0.01, 'spectral_radius': 0.2, 'noise': 0.007, 'look_back': 141}. Best is trial 5 with value: 0.020388544925829116.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.010858474131744594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:33,437] Trial 12 finished with value: 0.020569592112539788 and parameters: {'n_reservoir': 645, 'sparsity': 0.5, 'spectral_radius': 0.1, 'noise': 0.009, 'look_back': 139}. Best is trial 5 with value: 0.020388544925829116.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:35,538] Trial 13 finished with value: 0.020184792290603375 and parameters: {'n_reservoir': 306, 'sparsity': 0.5, 'spectral_radius': 0.1, 'noise': 0.009, 'look_back': 84}. Best is trial 13 with value: 0.020184792290603375.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011151055747467309\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:37,211] Trial 14 finished with value: 0.020689852407187223 and parameters: {'n_reservoir': 245, 'sparsity': 0.5, 'spectral_radius': 0.1, 'noise': 0.009, 'look_back': 72}. Best is trial 13 with value: 0.020184792290603375.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011269215809124367\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:38,921] Trial 15 finished with value: 0.02036790756281834 and parameters: {'n_reservoir': 244, 'sparsity': 0.5, 'spectral_radius': 0.7, 'noise': 0.001, 'look_back': 89}. Best is trial 13 with value: 0.020184792290603375.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011188168908407863\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:40,555] Trial 16 finished with value: 0.02013676945466315 and parameters: {'n_reservoir': 228, 'sparsity': 0.5, 'spectral_radius': 0.7, 'noise': 0.008, 'look_back': 89}. Best is trial 16 with value: 0.02013676945466315.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011196717168014198\n",
            "harvesting states...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:41,009] Trial 17 finished with value: 0.021677676767631764 and parameters: {'n_reservoir': 62, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.008, 'look_back': 26}. Best is trial 16 with value: 0.02013676945466315.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fitting...\n",
            "training error:\n",
            "0.012163208445385482\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:44,699] Trial 18 finished with value: 0.025819183211905752 and parameters: {'n_reservoir': 215, 'sparsity': 0.5, 'spectral_radius': 0.7, 'noise': 0.002, 'look_back': 412}. Best is trial 16 with value: 0.02013676945466315.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011335737740253487\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:47,895] Trial 19 finished with value: 0.021123630983476995 and parameters: {'n_reservoir': 370, 'sparsity': 0.3, 'spectral_radius': 0.8, 'noise': 0.008, 'look_back': 198}. Best is trial 16 with value: 0.02013676945466315.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.010926453487738451\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:49,061] Trial 20 finished with value: 0.020172853265648643 and parameters: {'n_reservoir': 154, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 82}. Best is trial 16 with value: 0.02013676945466315.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.019315645388880216\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:50,174] Trial 21 finished with value: 0.02012954661144583 and parameters: {'n_reservoir': 149, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 83}. Best is trial 21 with value: 0.02012954661144583.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01937364277313557\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:51,270] Trial 22 finished with value: 0.020083043584968874 and parameters: {'n_reservoir': 143, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 78}. Best is trial 22 with value: 0.020083043584968874.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.019486735177616735\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:52,958] Trial 23 finished with value: 0.01967277001416509 and parameters: {'n_reservoir': 136, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 259}. Best is trial 23 with value: 0.01967277001416509.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.0186903437498544\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:53,873] Trial 24 finished with value: 0.019453132900758018 and parameters: {'n_reservoir': 23, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 281}. Best is trial 24 with value: 0.019453132900758018.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018795490761576484\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:55,031] Trial 25 finished with value: 0.01953271755354697 and parameters: {'n_reservoir': 16, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 259}. Best is trial 24 with value: 0.019453132900758018.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01885485386177469\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:56,330] Trial 26 finished with value: 0.01957475995937908 and parameters: {'n_reservoir': 13, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 267}. Best is trial 24 with value: 0.019453132900758018.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01880913834277535\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:57,432] Trial 27 finished with value: 0.019430134162601396 and parameters: {'n_reservoir': 15, 'sparsity': 0.4, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 351}. Best is trial 27 with value: 0.019430134162601396.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018687467302184946\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:20:58,610] Trial 28 finished with value: 0.019466663564512043 and parameters: {'n_reservoir': 19, 'sparsity': 0.4, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 366}. Best is trial 27 with value: 0.019430134162601396.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01864951770304473\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:00,383] Trial 29 finished with value: 0.019573019752642404 and parameters: {'n_reservoir': 71, 'sparsity': 0.4, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 388}. Best is trial 27 with value: 0.019430134162601396.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01859079787654221\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:04,752] Trial 30 finished with value: 0.022190185605346014 and parameters: {'n_reservoir': 325, 'sparsity': 0.4, 'spectral_radius': 1.25, 'noise': 0.001, 'look_back': 446}. Best is trial 27 with value: 0.019430134162601396.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011222247444937519\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:06,128] Trial 31 finished with value: 0.01940765148704362 and parameters: {'n_reservoir': 36, 'sparsity': 0.4, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 337}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018703623869039575\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:08,621] Trial 32 finished with value: 0.02150555538924041 and parameters: {'n_reservoir': 84, 'sparsity': 0.4, 'spectral_radius': 0.9, 'noise': 0.006, 'look_back': 335}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011984084199841384\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:09,828] Trial 33 finished with value: 0.01945382592024683 and parameters: {'n_reservoir': 20, 'sparsity': 0.4, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 360}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018673053046940218\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:11,455] Trial 34 finished with value: 0.019522511264788697 and parameters: {'n_reservoir': 99, 'sparsity': 0.4, 'spectral_radius': 10.0, 'noise': 0.005, 'look_back': 310}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018609126338733035\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:14,107] Trial 35 finished with value: 0.03042246940077518 and parameters: {'n_reservoir': 158, 'sparsity': 0.4, 'spectral_radius': 0.4, 'noise': 0.002, 'look_back': 428}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011565124934548636\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:16,513] Trial 36 finished with value: 0.02427655947736427 and parameters: {'n_reservoir': 189, 'sparsity': 0.4, 'spectral_radius': 1, 'noise': 0.004, 'look_back': 336}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01138507125154281\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.01102639795268405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:20,061] Trial 37 finished with value: 0.021111318367723005 and parameters: {'n_reservoir': 288, 'sparsity': 0.4, 'spectral_radius': 1.1, 'noise': 0.006, 'look_back': 284}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:22,330] Trial 38 finished with value: 0.022320046815368196 and parameters: {'n_reservoir': 99, 'sparsity': 0.2, 'spectral_radius': 0.6, 'noise': 0.006, 'look_back': 376}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011743776005369262\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:27,116] Trial 39 finished with value: 0.02192229369704534 and parameters: {'n_reservoir': 448, 'sparsity': 0.01, 'spectral_radius': 0.5, 'noise': 0.007, 'look_back': 336}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.010819273295226726\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:28,675] Trial 40 finished with value: 0.021623020095363314 and parameters: {'n_reservoir': 16, 'sparsity': 0.3, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 458}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.016551274509557106\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:30,150] Trial 41 finished with value: 0.019483954779244157 and parameters: {'n_reservoir': 44, 'sparsity': 0.4, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 364}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018652231559176714\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:33,115] Trial 42 finished with value: 0.019613385561849963 and parameters: {'n_reservoir': 115, 'sparsity': 0.4, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 400}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01853743522677699\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:33,856] Trial 43 finished with value: 0.022910388612393133 and parameters: {'n_reservoir': 13, 'sparsity': 0.4, 'spectral_radius': 0.8, 'noise': 0.006, 'look_back': 228}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.014542706036351867\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.018072466801814683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:40,420] Trial 44 finished with value: 0.02014910813505181 and parameters: {'n_reservoir': 567, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 354}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:41,818] Trial 45 finished with value: 0.02172235123872743 and parameters: {'n_reservoir': 54, 'sparsity': 0.4, 'spectral_radius': 0.9, 'noise': 0.005, 'look_back': 315}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.012022285294818552\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:44,571] Trial 46 finished with value: 0.019677996826329373 and parameters: {'n_reservoir': 194, 'sparsity': 0.4, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 288}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01859174283166019\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.010757226874936106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:21:54,127] Trial 47 finished with value: 0.02124842775797789 and parameters: {'n_reservoir': 824, 'sparsity': 0.2, 'spectral_radius': 1.25, 'noise': 0.003, 'look_back': 236}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.010732916720843142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:04,828] Trial 48 finished with value: 0.021725711386974992 and parameters: {'n_reservoir': 885, 'sparsity': 0.01, 'spectral_radius': 1, 'noise': 0.007, 'look_back': 181}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:06,708] Trial 49 finished with value: 0.023720264668461128 and parameters: {'n_reservoir': 110, 'sparsity': 0.1, 'spectral_radius': 0.4, 'noise': 0.001, 'look_back': 350}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01152727869477042\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:09,393] Trial 50 finished with value: 0.019612937819204316 and parameters: {'n_reservoir': 52, 'sparsity': 0.4, 'spectral_radius': 10.0, 'noise': 0.002, 'look_back': 475}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018533827943075273\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:10,999] Trial 51 finished with value: 0.01948008614772735 and parameters: {'n_reservoir': 50, 'sparsity': 0.4, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 369}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018649019405555873\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:12,686] Trial 52 finished with value: 0.019492617094935015 and parameters: {'n_reservoir': 54, 'sparsity': 0.4, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 389}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018642567161921048\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:14,790] Trial 53 finished with value: 0.02135885813595106 and parameters: {'n_reservoir': 95, 'sparsity': 0.4, 'spectral_radius': 0.5, 'noise': 0.006, 'look_back': 413}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.0123915786731057\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.010674587749650438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:24,285] Trial 54 finished with value: 0.022512562029050994 and parameters: {'n_reservoir': 733, 'sparsity': 0.4, 'spectral_radius': 0.2, 'noise': 0.006, 'look_back': 324}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:26,856] Trial 55 finished with value: 0.022226238139864952 and parameters: {'n_reservoir': 182, 'sparsity': 0.3, 'spectral_radius': 1.1, 'noise': 0.006, 'look_back': 371}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01159190158432744\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:28,312] Trial 56 finished with value: 0.019492189095315148 and parameters: {'n_reservoir': 49, 'sparsity': 0.4, 'spectral_radius': 10.0, 'noise': 0.009, 'look_back': 348}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018624845590758072\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:31,086] Trial 57 finished with value: 0.021198297840708313 and parameters: {'n_reservoir': 257, 'sparsity': 0.4, 'spectral_radius': 0.6, 'noise': 0.008, 'look_back': 301}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.010969678739575111\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:32,159] Trial 58 finished with value: 0.019472146781549377 and parameters: {'n_reservoir': 17, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 278}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018805401253349275\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:34,655] Trial 59 finished with value: 0.023306934780707337 and parameters: {'n_reservoir': 124, 'sparsity': 0.2, 'spectral_radius': 0.1, 'noise': 0.003, 'look_back': 283}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011321525001596324\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:36,634] Trial 60 finished with value: 0.01966052443231785 and parameters: {'n_reservoir': 165, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.004, 'look_back': 275}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01858450403058781\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:38,096] Trial 61 finished with value: 0.019515025446356492 and parameters: {'n_reservoir': 27, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 381}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01863030752490436\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:39,418] Trial 62 finished with value: 0.019579460562761004 and parameters: {'n_reservoir': 81, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 250}. Best is trial 31 with value: 0.01940765148704362.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018727634068405736\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:40,461] Trial 63 finished with value: 0.019390472901838596 and parameters: {'n_reservoir': 13, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 328}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018728522009361006\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:41,391] Trial 64 finished with value: 0.01943750526553027 and parameters: {'n_reservoir': 12, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 303}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018738376343537116\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:43,041] Trial 65 finished with value: 0.021962655418241214 and parameters: {'n_reservoir': 86, 'sparsity': 0.01, 'spectral_radius': 0.3, 'noise': 0.006, 'look_back': 324}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011696758652985419\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:45,220] Trial 66 finished with value: 0.021171306586724854 and parameters: {'n_reservoir': 116, 'sparsity': 0.01, 'spectral_radius': 0.7, 'noise': 0.005, 'look_back': 303}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011409922529258818\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:46,642] Trial 67 finished with value: 0.02192982940804253 and parameters: {'n_reservoir': 10, 'sparsity': 0.01, 'spectral_radius': 0.8, 'noise': 0.006, 'look_back': 324}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.014153932730991121\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:49,045] Trial 68 finished with value: 0.01966758280242457 and parameters: {'n_reservoir': 136, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 401}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018471869965254482\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:50,628] Trial 69 finished with value: 0.01950894350470696 and parameters: {'n_reservoir': 69, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 343}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018631318790025626\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.018221095366754616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:56,858] Trial 70 finished with value: 0.020422145767521764 and parameters: {'n_reservoir': 588, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.002, 'look_back': 248}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:58,705] Trial 71 finished with value: 0.019450806205459335 and parameters: {'n_reservoir': 35, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 296}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01873978376216501\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:22:59,973] Trial 72 finished with value: 0.019409313731918506 and parameters: {'n_reservoir': 41, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 314}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018730173245626383\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:01,247] Trial 73 finished with value: 0.019499401423311243 and parameters: {'n_reservoir': 43, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 314}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018683075365763753\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:02,709] Trial 74 finished with value: 0.021048383607820912 and parameters: {'n_reservoir': 80, 'sparsity': 0.1, 'spectral_radius': 0.9, 'noise': 0.006, 'look_back': 296}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.012215890072867523\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:04,050] Trial 75 finished with value: 0.02188763809765167 and parameters: {'n_reservoir': 37, 'sparsity': 0.1, 'spectral_radius': 0.2, 'noise': 0.007, 'look_back': 331}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.0129588340015342\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:05,736] Trial 76 finished with value: 0.019653261104451328 and parameters: {'n_reservoir': 131, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.009, 'look_back': 264}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01871562099836539\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:07,424] Trial 77 finished with value: 0.022339229723161484 and parameters: {'n_reservoir': 79, 'sparsity': 0.1, 'spectral_radius': 1.25, 'noise': 0.008, 'look_back': 357}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.012360373906814465\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:10,604] Trial 78 finished with value: 0.021429510558398274 and parameters: {'n_reservoir': 221, 'sparsity': 0.1, 'spectral_radius': 0.4, 'noise': 0.006, 'look_back': 297}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011110927847208033\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:12,443] Trial 79 finished with value: 0.021643738713791407 and parameters: {'n_reservoir': 103, 'sparsity': 0.01, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 341}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011736794082732512\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:16,261] Trial 80 finished with value: 0.020005735662715903 and parameters: {'n_reservoir': 423, 'sparsity': 0.3, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 205}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018433743339002853\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:17,540] Trial 81 finished with value: 0.019433593447467038 and parameters: {'n_reservoir': 35, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 319}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018722339041265294\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:18,841] Trial 82 finished with value: 0.01952071239098535 and parameters: {'n_reservoir': 37, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 311}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018699179105042887\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:19,856] Trial 83 finished with value: 0.01973393929793554 and parameters: {'n_reservoir': 10, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 322}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018525252906774405\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:21,607] Trial 84 finished with value: 0.019445923451900833 and parameters: {'n_reservoir': 62, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 292}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018715032273916042\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:23,503] Trial 85 finished with value: 0.020529580731527573 and parameters: {'n_reservoir': 70, 'sparsity': 0.5, 'spectral_radius': 0.1, 'noise': 0.006, 'look_back': 272}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011469350488880396\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:25,614] Trial 86 finished with value: 0.021257299578167976 and parameters: {'n_reservoir': 175, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.004, 'look_back': 290}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01165864080676266\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:26,983] Trial 87 finished with value: 0.020562888994451696 and parameters: {'n_reservoir': 63, 'sparsity': 0.5, 'spectral_radius': 1.1, 'noise': 0.006, 'look_back': 305}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01199876488537078\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:28,171] Trial 88 finished with value: 0.023399269865980538 and parameters: {'n_reservoir': 37, 'sparsity': 0.5, 'spectral_radius': 0.6, 'noise': 0.003, 'look_back': 289}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01404328322363697\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:29,879] Trial 89 finished with value: 0.019659225875465265 and parameters: {'n_reservoir': 151, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 238}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01872893602620062\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:31,384] Trial 90 finished with value: 0.02044126897848945 and parameters: {'n_reservoir': 110, 'sparsity': 0.1, 'spectral_radius': 0.7, 'noise': 0.006, 'look_back': 260}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011465376008793313\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:32,901] Trial 91 finished with value: 0.019428895167654062 and parameters: {'n_reservoir': 33, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 355}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018665947615452635\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:35,203] Trial 92 finished with value: 0.01949276661924648 and parameters: {'n_reservoir': 36, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 334}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018682634393125126\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:36,682] Trial 93 finished with value: 0.0195179804685757 and parameters: {'n_reservoir': 64, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 316}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018693410390670462\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:38,433] Trial 94 finished with value: 0.019550124152313265 and parameters: {'n_reservoir': 93, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.005, 'look_back': 349}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018623051024092285\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:39,745] Trial 95 finished with value: 0.019473999087526193 and parameters: {'n_reservoir': 36, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 329}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01870601285828201\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:41,317] Trial 96 finished with value: 0.021297953877955706 and parameters: {'n_reservoir': 60, 'sparsity': 0.01, 'spectral_radius': 0.3, 'noise': 0.001, 'look_back': 361}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011748631979430662\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:42,541] Trial 97 finished with value: 0.019485314083661217 and parameters: {'n_reservoir': 11, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 380}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018674490297960523\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:44,205] Trial 98 finished with value: 0.02065796669612801 and parameters: {'n_reservoir': 123, 'sparsity': 0.5, 'spectral_radius': 0.8, 'noise': 0.007, 'look_back': 274}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011174924886488273\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:45,846] Trial 99 finished with value: 0.019474154392976388 and parameters: {'n_reservoir': 31, 'sparsity': 0.3, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 307}. Best is trial 63 with value: 0.019390472901838596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018723914456959683\n",
            "done\n",
            "ESN HyperParameters Tuning Elapsed Time : 310.14912 seconds\n",
            "Best Parameters:  {'n_reservoir': 13, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 328}\n",
            "Best Loss:  0.019390472901838596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 4\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "valid_size = int(len(train) * 0.1)\n",
        "temp_train = train[:-valid_size]\n",
        "valid = train[-valid_size:]\n",
        "print(len(train), len(temp_train), len(valid), len(test))\n",
        "\n",
        "# Define the objective function for Optuna optimization\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters to search\n",
        "\n",
        "    n_reservoir = trial.suggest_int('n_reservoir', 10, 1000)   # -\n",
        "    sparsity = trial.suggest_categorical('sparsity', [0.01, 0.1, 0.2, 0.3, 0.4, 0.5])   # -\n",
        "    spectral_radius = trial.suggest_categorical('spectral_radius', [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.25, 10.0])   # - spectral radius of W\n",
        "    noise = trial.suggest_categorical('noise', [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009])       # - Noise Set\n",
        "    look_back = trial.suggest_int('look_back', 10, 500)\n",
        "\n",
        "    # reshape into X=t and Y=t+1\n",
        "    trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "    validX, validY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "    # reshape input to be [samples, time steps, features]\n",
        "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1]))\n",
        "    validX = np.reshape(validX, (validX.shape[0], validX.shape[1]))\n",
        "\n",
        "    model = ESN(n_inputs = look_back,\n",
        "            n_outputs = horizon,\n",
        "            n_reservoir = n_reservoir,\n",
        "            sparsity=sparsity,\n",
        "            random_state=1234,\n",
        "            spectral_radius=spectral_radius,\n",
        "            noise = noise,\n",
        "            silent = False,\n",
        "            teacher_scaling = 10,\n",
        "    )\n",
        "\n",
        "    pred_train = model.fit(trainX, trainY)\n",
        "\n",
        "    # make predictions\n",
        "    testPredict = model.predict(validX)\n",
        "    testPredict = np.array(testPredict)\n",
        "\n",
        "    testRMSE = np.sqrt(mean_squared_error(validY, testPredict[:len(validY), :]))\n",
        "\n",
        "    return testRMSE\n",
        "\n",
        "\n",
        "# Create the Optuna study\n",
        "study = optuna.create_study(direction='minimize')\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "start_time = time.time()\n",
        "\n",
        "# Run the optimization\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Record the ending time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print('done')\n",
        "print(\"ESN HyperParameters Tuning Elapsed Time : %.5f\" % (elapsed_time), \"seconds\")\n",
        "\n",
        "# Print the best parameters and corresponding loss\n",
        "best_params = study.best_params\n",
        "best_loss = study.best_value\n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Best Loss: \", best_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akn508Cwkaya",
        "outputId": "3b7f2700-87af-4cb5-8e3b-5fbda98070b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:45,915] A new study created in memory with name: no-name-6c83b9dd-b020-45b5-b1dd-768d0201cefb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 16192 1799 2000\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:49,994] Trial 0 finished with value: 0.026487567172234574 and parameters: {'n_reservoir': 460, 'sparsity': 0.3, 'spectral_radius': 0.2, 'noise': 0.007, 'look_back': 121}. Best is trial 0 with value: 0.026487567172234574.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007830198652219306\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.007796594803344706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:54,817] Trial 1 finished with value: 0.027518801577407607 and parameters: {'n_reservoir': 526, 'sparsity': 0.4, 'spectral_radius': 0.5, 'noise': 0.005, 'look_back': 186}. Best is trial 0 with value: 0.026487567172234574.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:55,457] Trial 2 finished with value: 0.028340640712299952 and parameters: {'n_reservoir': 69, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.007, 'look_back': 104}. Best is trial 0 with value: 0.026487567172234574.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008459574418236283\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:23:59,777] Trial 3 finished with value: 0.03096284083698 and parameters: {'n_reservoir': 387, 'sparsity': 0.01, 'spectral_radius': 0.7, 'noise': 0.003, 'look_back': 236}. Best is trial 0 with value: 0.026487567172234574.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007965823156714241\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:24:01,353] Trial 4 finished with value: 0.02642473142645996 and parameters: {'n_reservoir': 249, 'sparsity': 0.5, 'spectral_radius': 1.25, 'noise': 0.007, 'look_back': 45}. Best is trial 4 with value: 0.02642473142645996.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008276856595846756\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:24:03,512] Trial 5 finished with value: 0.03262616462261729 and parameters: {'n_reservoir': 85, 'sparsity': 0.01, 'spectral_radius': 0.8, 'noise': 0.003, 'look_back': 449}. Best is trial 4 with value: 0.02642473142645996.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01118707073506402\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.007857084897321873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:24:11,378] Trial 6 finished with value: 0.038815755718630385 and parameters: {'n_reservoir': 803, 'sparsity': 0.2, 'spectral_radius': 1, 'noise': 0.008, 'look_back': 71}. Best is trial 4 with value: 0.02642473142645996.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:24:14,420] Trial 7 finished with value: 0.030316516285633803 and parameters: {'n_reservoir': 219, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.008, 'look_back': 385}. Best is trial 4 with value: 0.02642473142645996.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008614621993806113\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:24:16,115] Trial 8 finished with value: 0.02584232542419906 and parameters: {'n_reservoir': 106, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 317}. Best is trial 8 with value: 0.02584232542419906.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02369417948607409\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.02077843972007285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:24:25,420] Trial 9 finished with value: 0.028620828396209316 and parameters: {'n_reservoir': 684, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.009, 'look_back': 270}. Best is trial 8 with value: 0.02584232542419906.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:24:26,649] Trial 10 finished with value: 0.025592632003548346 and parameters: {'n_reservoir': 21, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 350}. Best is trial 10 with value: 0.025592632003548346.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023863012602683732\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:24:27,926] Trial 11 finished with value: 0.02523306460854507 and parameters: {'n_reservoir': 28, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 335}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.024063165240132543\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.007660365508377022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:24:43,064] Trial 12 finished with value: 0.03479372521059721 and parameters: {'n_reservoir': 994, 'sparsity': 0.1, 'spectral_radius': 0.6, 'noise': 0.006, 'look_back': 389}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:24:44,934] Trial 13 finished with value: 0.028627853084574467 and parameters: {'n_reservoir': 21, 'sparsity': 0.1, 'spectral_radius': 1.1, 'noise': 0.006, 'look_back': 494}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.020432482796295757\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.022692068460006043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:24:48,000] Trial 14 finished with value: 0.026396769627457396 and parameters: {'n_reservoir': 239, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 342}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:24:52,135] Trial 15 finished with value: 0.032294273859139924 and parameters: {'n_reservoir': 324, 'sparsity': 0.1, 'spectral_radius': 0.4, 'noise': 0.004, 'look_back': 294}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008098744517820979\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:24:53,854] Trial 16 finished with value: 0.026840053721055784 and parameters: {'n_reservoir': 181, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.002, 'look_back': 188}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023379193719359374\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.007946554641670444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:01,585] Trial 17 finished with value: 0.03279703335374229 and parameters: {'n_reservoir': 581, 'sparsity': 0.4, 'spectral_radius': 0.1, 'noise': 0.006, 'look_back': 390}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:06,335] Trial 18 finished with value: 0.03719066653316871 and parameters: {'n_reservoir': 345, 'sparsity': 0.3, 'spectral_radius': 0.9, 'noise': 0.006, 'look_back': 443}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008540143112678795\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:07,810] Trial 19 finished with value: 0.02603952822451816 and parameters: {'n_reservoir': 129, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 225}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023700226220885478\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:09,108] Trial 20 finished with value: 1.3956790634598648 and parameters: {'n_reservoir': 27, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.009, 'look_back': 342}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.018056205005373728\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:11,048] Trial 21 finished with value: 0.02624207388697203 and parameters: {'n_reservoir': 132, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 314}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023480379524414268\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:12,232] Trial 22 finished with value: 0.025384222341400024 and parameters: {'n_reservoir': 11, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 345}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023935738207204914\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:14,050] Trial 23 finished with value: 0.025644187949242336 and parameters: {'n_reservoir': 14, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 362}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023830034329870204\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:16,779] Trial 24 finished with value: 0.03649334598773865 and parameters: {'n_reservoir': 155, 'sparsity': 0.01, 'spectral_radius': 0.3, 'noise': 0.002, 'look_back': 433}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.009100158912217129\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:19,668] Trial 25 finished with value: 0.029353315373280085 and parameters: {'n_reservoir': 284, 'sparsity': 0.4, 'spectral_radius': 0.6, 'noise': 0.004, 'look_back': 272}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008198499330245707\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:22,441] Trial 26 finished with value: 0.03275390963504687 and parameters: {'n_reservoir': 176, 'sparsity': 0.3, 'spectral_radius': 0.7, 'noise': 0.006, 'look_back': 419}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.009585929744123397\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:28,490] Trial 27 finished with value: 0.036219748113893134 and parameters: {'n_reservoir': 397, 'sparsity': 0.2, 'spectral_radius': 0.9, 'noise': 0.005, 'look_back': 470}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008485907566648729\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:29,626] Trial 28 finished with value: 0.03509002491221496 and parameters: {'n_reservoir': 15, 'sparsity': 0.5, 'spectral_radius': 0.8, 'noise': 0.006, 'look_back': 350}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.014626266405638732\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:33,533] Trial 29 finished with value: 0.028180188503362066 and parameters: {'n_reservoir': 429, 'sparsity': 0.3, 'spectral_radius': 0.1, 'noise': 0.006, 'look_back': 200}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007838288857847242\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.007650476710362898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:43,939] Trial 30 finished with value: 0.031236244592046578 and parameters: {'n_reservoir': 880, 'sparsity': 0.1, 'spectral_radius': 0.2, 'noise': 0.006, 'look_back': 153}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:45,662] Trial 31 finished with value: 0.025534164878537692 and parameters: {'n_reservoir': 68, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 385}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023750241190633185\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:47,168] Trial 32 finished with value: 0.025790937525369387 and parameters: {'n_reservoir': 83, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.005, 'look_back': 313}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023833951593248835\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:49,172] Trial 33 finished with value: 0.02573982691246265 and parameters: {'n_reservoir': 68, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 404}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023638747960204296\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.007959620236212297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:56,140] Trial 34 finished with value: 0.031121731448643185 and parameters: {'n_reservoir': 533, 'sparsity': 0.1, 'spectral_radius': 0.4, 'noise': 0.007, 'look_back': 369}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:58,425] Trial 35 finished with value: 0.035540438650073856 and parameters: {'n_reservoir': 200, 'sparsity': 0.4, 'spectral_radius': 1.25, 'noise': 0.003, 'look_back': 290}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008870065614917877\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:25:59,670] Trial 36 finished with value: 0.028624252803309044 and parameters: {'n_reservoir': 74, 'sparsity': 0.5, 'spectral_radius': 1.1, 'noise': 0.006, 'look_back': 248}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.010393173003210016\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:03,953] Trial 37 finished with value: 0.029582193263835405 and parameters: {'n_reservoir': 292, 'sparsity': 0.01, 'spectral_radius': 0.2, 'noise': 0.007, 'look_back': 327}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008354565038460326\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:05,795] Trial 38 finished with value: 0.039853323290936854 and parameters: {'n_reservoir': 63, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.008, 'look_back': 413}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011594259824779667\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:07,982] Trial 39 finished with value: 0.02600157660121171 and parameters: {'n_reservoir': 129, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.003, 'look_back': 376}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023486250121003964\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.00778530902684087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:16,271] Trial 40 finished with value: 0.028636831615435475 and parameters: {'n_reservoir': 678, 'sparsity': 0.2, 'spectral_radius': 0.3, 'noise': 0.009, 'look_back': 283}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:17,479] Trial 41 finished with value: 0.025436073460349276 and parameters: {'n_reservoir': 19, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 370}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.024061692117755898\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:19,480] Trial 42 finished with value: 0.025574670076840302 and parameters: {'n_reservoir': 50, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 459}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02381604251826394\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:22,139] Trial 43 finished with value: 0.02564743736372421 and parameters: {'n_reservoir': 116, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 492}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023343281595782493\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:24,215] Trial 44 finished with value: 0.02567045917868473 and parameters: {'n_reservoir': 62, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 471}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.0237233232858672\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:28,230] Trial 45 finished with value: 0.03335754088616385 and parameters: {'n_reservoir': 222, 'sparsity': 0.5, 'spectral_radius': 0.7, 'noise': 0.001, 'look_back': 434}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008887114426381742\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:30,544] Trial 46 finished with value: 0.0307390407985256 and parameters: {'n_reservoir': 99, 'sparsity': 0.01, 'spectral_radius': 1.25, 'noise': 0.008, 'look_back': 461}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01141264472028094\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:33,174] Trial 47 finished with value: 0.026122046227845593 and parameters: {'n_reservoir': 161, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.004, 'look_back': 408}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023081240051932184\n",
            "harvesting states...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:33,613] Trial 48 finished with value: 0.027490772647675153 and parameters: {'n_reservoir': 48, 'sparsity': 0.1, 'spectral_radius': 0.8, 'noise': 0.006, 'look_back': 32}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fitting...\n",
            "training error:\n",
            "0.008614315415083713\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:36,544] Trial 49 finished with value: 0.031434202552971385 and parameters: {'n_reservoir': 249, 'sparsity': 0.2, 'spectral_radius': 1, 'noise': 0.002, 'look_back': 333}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008419006080647527\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:39,088] Trial 50 finished with value: 0.025852621415187977 and parameters: {'n_reservoir': 99, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 391}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02350624334178771\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:40,686] Trial 51 finished with value: 0.02524564002910612 and parameters: {'n_reservoir': 24, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 358}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023932514958014274\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:41,851] Trial 52 finished with value: 0.02637094450863802 and parameters: {'n_reservoir': 15, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 362}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.022544019485045367\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:43,191] Trial 53 finished with value: 0.025421129318586685 and parameters: {'n_reservoir': 50, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 303}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023979793270843906\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:45,188] Trial 54 finished with value: 0.030122032692915888 and parameters: {'n_reservoir': 149, 'sparsity': 0.1, 'spectral_radius': 0.5, 'noise': 0.006, 'look_back': 305}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008757186964350063\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:46,621] Trial 55 finished with value: 0.025938805169416664 and parameters: {'n_reservoir': 103, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.005, 'look_back': 251}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02350476982321666\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:49,208] Trial 56 finished with value: 0.03182519416193377 and parameters: {'n_reservoir': 199, 'sparsity': 0.3, 'spectral_radius': 0.6, 'noise': 0.006, 'look_back': 349}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008754337256915169\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:50,957] Trial 57 finished with value: 0.029590928196714893 and parameters: {'n_reservoir': 41, 'sparsity': 0.4, 'spectral_radius': 1.1, 'noise': 0.009, 'look_back': 325}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.012380570464398995\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:52,150] Trial 58 finished with value: 0.02541441182272241 and parameters: {'n_reservoir': 14, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 225}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.024368466738624407\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:52,812] Trial 59 finished with value: 0.08096217404017399 and parameters: {'n_reservoir': 10, 'sparsity': 0.1, 'spectral_radius': 0.4, 'noise': 0.001, 'look_back': 204}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01692401637188986\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:54,116] Trial 60 finished with value: 0.02669627059619053 and parameters: {'n_reservoir': 133, 'sparsity': 0.1, 'spectral_radius': 0.1, 'noise': 0.006, 'look_back': 153}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.00849887216682615\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:55,316] Trial 61 finished with value: 0.025672278578567114 and parameters: {'n_reservoir': 48, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 267}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.024086513557204432\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:56,529] Trial 62 finished with value: 0.02598413454042283 and parameters: {'n_reservoir': 85, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 222}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02377535121581549\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:57,500] Trial 63 finished with value: 0.0253158194154165 and parameters: {'n_reservoir': 11, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 305}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02424315231300849\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:58,479] Trial 64 finished with value: 0.0253158194154165 and parameters: {'n_reservoir': 11, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 305}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02424315231300849\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:26:59,752] Trial 65 finished with value: 0.02670755072690967 and parameters: {'n_reservoir': 43, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.004, 'look_back': 305}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.022880597887267807\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:01,168] Trial 66 finished with value: 0.028496551439723326 and parameters: {'n_reservoir': 107, 'sparsity': 0.1, 'spectral_radius': 0.9, 'noise': 0.002, 'look_back': 236}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008875813246427767\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:02,048] Trial 67 finished with value: 0.025396879796881357 and parameters: {'n_reservoir': 12, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 267}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02409199748458185\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:03,361] Trial 68 finished with value: 0.0319606463754962 and parameters: {'n_reservoir': 12, 'sparsity': 0.01, 'spectral_radius': 0.3, 'noise': 0.007, 'look_back': 270}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.016781634710772195\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:06,307] Trial 69 finished with value: 0.03864438276293813 and parameters: {'n_reservoir': 170, 'sparsity': 0.01, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 339}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.009110293696764834\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:10,378] Trial 70 finished with value: 0.028636599248057282 and parameters: {'n_reservoir': 477, 'sparsity': 0.01, 'spectral_radius': 0.2, 'noise': 0.006, 'look_back': 170}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007767496015701514\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:11,863] Trial 71 finished with value: 0.025819562079569522 and parameters: {'n_reservoir': 79, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 299}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02379017249072799\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.02329706635951483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:12,412] Trial 72 finished with value: 0.026703329135506732 and parameters: {'n_reservoir': 43, 'sparsity': 0.3, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 78}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.019765853884099745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:26,292] Trial 73 finished with value: 0.030206049930048565 and parameters: {'n_reservoir': 988, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 287}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:29,106] Trial 74 finished with value: 0.02956598109948903 and parameters: {'n_reservoir': 133, 'sparsity': 0.4, 'spectral_radius': 0.7, 'noise': 0.006, 'look_back': 318}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.009120802273523628\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:30,344] Trial 75 finished with value: 0.025324243783314396 and parameters: {'n_reservoir': 39, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.008, 'look_back': 282}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.024114056095506757\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:31,723] Trial 76 finished with value: 0.038799764391606595 and parameters: {'n_reservoir': 85, 'sparsity': 0.2, 'spectral_radius': 0.8, 'noise': 0.008, 'look_back': 262}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.009897304709716082\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:32,540] Trial 77 finished with value: 0.025323350140121235 and parameters: {'n_reservoir': 28, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.008, 'look_back': 230}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.024125416040434357\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:33,678] Trial 78 finished with value: 1.195610838207925 and parameters: {'n_reservoir': 37, 'sparsity': 0.2, 'spectral_radius': 0.6, 'noise': 0.008, 'look_back': 275}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.01337330288786\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.020854066116710424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:42,289] Trial 79 finished with value: 0.028926596790883493 and parameters: {'n_reservoir': 730, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.008, 'look_back': 247}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:44,015] Trial 80 finished with value: 0.025644571566659013 and parameters: {'n_reservoir': 71, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.008, 'look_back': 357}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023684708325220377\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:44,847] Trial 81 finished with value: 0.025407863769118078 and parameters: {'n_reservoir': 34, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.008, 'look_back': 221}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.024318659838278658\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:46,290] Trial 82 finished with value: 0.0340941737648196 and parameters: {'n_reservoir': 111, 'sparsity': 0.2, 'spectral_radius': 1.25, 'noise': 0.008, 'look_back': 239}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.009581096583615379\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:47,176] Trial 83 finished with value: 0.025515139288648153 and parameters: {'n_reservoir': 42, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.008, 'look_back': 212}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.024229873410907626\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:48,559] Trial 84 finished with value: 0.02560974273731117 and parameters: {'n_reservoir': 69, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.008, 'look_back': 281}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023873183336119534\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.007761793429576647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:54,790] Trial 85 finished with value: 0.02711114913289574 and parameters: {'n_reservoir': 580, 'sparsity': 0.2, 'spectral_radius': 0.9, 'noise': 0.008, 'look_back': 191}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:55,688] Trial 86 finished with value: 0.02834944209068172 and parameters: {'n_reservoir': 31, 'sparsity': 0.2, 'spectral_radius': 0.1, 'noise': 0.008, 'look_back': 258}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011895900475254134\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:57,390] Trial 87 finished with value: 0.026131542491549108 and parameters: {'n_reservoir': 97, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.009, 'look_back': 325}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02327912584569909\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:27:59,337] Trial 88 finished with value: 0.031037681285503247 and parameters: {'n_reservoir': 141, 'sparsity': 0.2, 'spectral_radius': 0.5, 'noise': 0.005, 'look_back': 312}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.0090042153433602\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:01,036] Trial 89 finished with value: 0.035251054711935396 and parameters: {'n_reservoir': 186, 'sparsity': 0.2, 'spectral_radius': 0.4, 'noise': 0.008, 'look_back': 173}. Best is trial 11 with value: 0.02523306460854507.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008159403519851798\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:02,106] Trial 90 finished with value: 0.0251564879356312 and parameters: {'n_reservoir': 11, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 340}. Best is trial 90 with value: 0.0251564879356312.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02414602751770763\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:03,164] Trial 91 finished with value: 0.025206345066144924 and parameters: {'n_reservoir': 10, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 343}. Best is trial 90 with value: 0.0251564879356312.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02417958754861505\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:04,820] Trial 92 finished with value: 0.025238104163402223 and parameters: {'n_reservoir': 10, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 341}. Best is trial 90 with value: 0.0251564879356312.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.024105832336050174\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:06,657] Trial 93 finished with value: 0.033173356696085295 and parameters: {'n_reservoir': 71, 'sparsity': 0.01, 'spectral_radius': 1.1, 'noise': 0.001, 'look_back': 346}. Best is trial 90 with value: 0.0251564879356312.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.011585399631021629\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:08,136] Trial 94 finished with value: 0.025380719402653202 and parameters: {'n_reservoir': 58, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 335}. Best is trial 90 with value: 0.0251564879356312.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023970184541389314\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:09,788] Trial 95 finished with value: 0.025810821788519053 and parameters: {'n_reservoir': 58, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 377}. Best is trial 90 with value: 0.0251564879356312.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023674736944946442\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:11,454] Trial 96 finished with value: 0.025647364237889086 and parameters: {'n_reservoir': 90, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 333}. Best is trial 90 with value: 0.0251564879356312.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023724216914773923\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:12,880] Trial 97 finished with value: 0.025307625345737904 and parameters: {'n_reservoir': 27, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 354}. Best is trial 90 with value: 0.0251564879356312.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.0240487265844705\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:14,912] Trial 98 finished with value: 0.030190745673855608 and parameters: {'n_reservoir': 120, 'sparsity': 0.01, 'spectral_radius': 0.3, 'noise': 0.001, 'look_back': 358}. Best is trial 90 with value: 0.0251564879356312.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008996800023784862\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:17,093] Trial 99 finished with value: 0.025426904421104853 and parameters: {'n_reservoir': 31, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 400}. Best is trial 90 with value: 0.0251564879356312.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023971836243246756\n",
            "done\n",
            "ESN HyperParameters Tuning Elapsed Time : 271.18417 seconds\n",
            "Best Parameters:  {'n_reservoir': 11, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 340}\n",
            "Best Loss:  0.0251564879356312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 6\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "valid_size = int(len(train) * 0.1)\n",
        "temp_train = train[:-valid_size]\n",
        "valid = train[-valid_size:]\n",
        "print(len(train), len(temp_train), len(valid), len(test))\n",
        "\n",
        "# Define the objective function for Optuna optimization\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters to search\n",
        "\n",
        "    n_reservoir = trial.suggest_int('n_reservoir', 10, 1000)   # -\n",
        "    sparsity = trial.suggest_categorical('sparsity', [0.01, 0.1, 0.2, 0.3, 0.4, 0.5])   # -\n",
        "    spectral_radius = trial.suggest_categorical('spectral_radius', [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.25, 10.0])   # - spectral radius of W\n",
        "    noise = trial.suggest_categorical('noise', [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009])       # - Noise Set\n",
        "    look_back = trial.suggest_int('look_back', 10, 500)\n",
        "\n",
        "    # reshape into X=t and Y=t+1\n",
        "    trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "    validX, validY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "    # reshape input to be [samples, time steps, features]\n",
        "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1]))\n",
        "    validX = np.reshape(validX, (validX.shape[0], validX.shape[1]))\n",
        "\n",
        "    model = ESN(n_inputs = look_back,\n",
        "            n_outputs = horizon,\n",
        "            n_reservoir = n_reservoir,\n",
        "            sparsity=sparsity,\n",
        "            random_state=1234,\n",
        "            spectral_radius=spectral_radius,\n",
        "            noise = noise,\n",
        "            silent = False,\n",
        "            teacher_scaling = 10,\n",
        "    )\n",
        "\n",
        "    pred_train = model.fit(trainX, trainY)\n",
        "\n",
        "    # make predictions\n",
        "    testPredict = model.predict(validX)\n",
        "    testPredict = np.array(testPredict)\n",
        "\n",
        "    testRMSE = np.sqrt(mean_squared_error(validY, testPredict[:len(validY), :]))\n",
        "\n",
        "    return testRMSE\n",
        "\n",
        "\n",
        "# Create the Optuna study\n",
        "study = optuna.create_study(direction='minimize')\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "start_time = time.time()\n",
        "\n",
        "# Run the optimization\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Record the ending time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print('done')\n",
        "print(\"ESN HyperParameters Tuning Elapsed Time : %.5f\" % (elapsed_time), \"seconds\")\n",
        "\n",
        "# Print the best parameters and corresponding loss\n",
        "best_params = study.best_params\n",
        "best_loss = study.best_value\n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Best Loss: \", best_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpNOpj1kj0CZ",
        "outputId": "3d021568-9f69-4269-96fb-2a4bfd48a5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:17,152] A new study created in memory with name: no-name-0d7b7fb5-6452-4bc7-832c-c98c28af3513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 16192 1799 2000\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:21,271] Trial 0 finished with value: 0.04166467927237724 and parameters: {'n_reservoir': 290, 'sparsity': 0.4, 'spectral_radius': 1, 'noise': 0.002, 'look_back': 435}. Best is trial 0 with value: 0.04166467927237724.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008251314010583358\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006480920304544917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:35,083] Trial 1 finished with value: 0.03551645308697249 and parameters: {'n_reservoir': 939, 'sparsity': 0.1, 'spectral_radius': 0.4, 'noise': 0.004, 'look_back': 374}. Best is trial 1 with value: 0.03551645308697249.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006551621192409506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:38,563] Trial 2 finished with value: 0.030801385450078596 and parameters: {'n_reservoir': 485, 'sparsity': 0.3, 'spectral_radius': 0.5, 'noise': 0.001, 'look_back': 71}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0070726673453275594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:41,661] Trial 3 finished with value: 0.03830963955901596 and parameters: {'n_reservoir': 265, 'sparsity': 0.1, 'spectral_radius': 0.7, 'noise': 0.009, 'look_back': 217}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.007857635988901582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:47,261] Trial 4 finished with value: 0.2652147277394594 and parameters: {'n_reservoir': 392, 'sparsity': 0.1, 'spectral_radius': 1.25, 'noise': 0.008, 'look_back': 496}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:49,418] Trial 5 finished with value: 0.03439778155667284 and parameters: {'n_reservoir': 184, 'sparsity': 0.2, 'spectral_radius': 0.4, 'noise': 0.003, 'look_back': 284}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007950871723274407\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.02485240987898297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:52,540] Trial 6 finished with value: 0.033298843683067764 and parameters: {'n_reservoir': 299, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.002, 'look_back': 236}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:28:56,717] Trial 7 finished with value: 0.03894090786642476 and parameters: {'n_reservoir': 246, 'sparsity': 0.1, 'spectral_radius': 1.25, 'noise': 0.003, 'look_back': 219}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.0074070289251844685\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.007089004969202661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:29:03,828] Trial 8 finished with value: 0.03733003637419487 and parameters: {'n_reservoir': 589, 'sparsity': 0.2, 'spectral_radius': 1.25, 'noise': 0.005, 'look_back': 374}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006785348179342878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:29:11,437] Trial 9 finished with value: 0.03954379110219851 and parameters: {'n_reservoir': 605, 'sparsity': 0.5, 'spectral_radius': 0.9, 'noise': 0.008, 'look_back': 314}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:29:11,802] Trial 10 finished with value: 1.2984845936784193 and parameters: {'n_reservoir': 21, 'sparsity': 0.3, 'spectral_radius': 0.5, 'noise': 0.001, 'look_back': 33}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fitting...\n",
            "training error:\n",
            "0.009870705485244454\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.021595007745842644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:29:19,127] Trial 11 finished with value: 0.03822353125625177 and parameters: {'n_reservoir': 759, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 61}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:29:22,717] Trial 12 finished with value: 0.035436397299468535 and parameters: {'n_reservoir': 447, 'sparsity': 0.3, 'spectral_radius': 10.0, 'noise': 0.002, 'look_back': 138}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.023448615022789603\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:29:23,379] Trial 13 finished with value: 0.08920183910397608 and parameters: {'n_reservoir': 51, 'sparsity': 0.01, 'spectral_radius': 0.5, 'noise': 0.006, 'look_back': 132}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.009611101080193465\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006488979192243649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:29:28,096] Trial 14 finished with value: 0.03290024794836065 and parameters: {'n_reservoir': 557, 'sparsity': 0.01, 'spectral_radius': 0.6, 'noise': 0.007, 'look_back': 127}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0063892055861407655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:29:35,596] Trial 15 finished with value: 0.036683972393793264 and parameters: {'n_reservoir': 722, 'sparsity': 0.3, 'spectral_radius': 0.6, 'noise': 0.007, 'look_back': 119}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006507258015671957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:29:39,932] Trial 16 finished with value: 0.032700565264081896 and parameters: {'n_reservoir': 542, 'sparsity': 0.5, 'spectral_radius': 0.8, 'noise': 0.007, 'look_back': 85}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006908925488592362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:29:47,430] Trial 17 finished with value: 0.03608872042464279 and parameters: {'n_reservoir': 778, 'sparsity': 0.5, 'spectral_radius': 0.8, 'noise': 0.001, 'look_back': 18}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006566119902092187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:29:50,849] Trial 18 finished with value: 0.031028076785539694 and parameters: {'n_reservoir': 469, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.007, 'look_back': 80}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0066060567461804975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:29:55,088] Trial 19 finished with value: 0.032129648702311525 and parameters: {'n_reservoir': 411, 'sparsity': 0.4, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 155}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006344872108449903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:30:06,882] Trial 20 finished with value: 0.035434130048715796 and parameters: {'n_reservoir': 972, 'sparsity': 0.3, 'spectral_radius': 1.1, 'noise': 0.006, 'look_back': 177}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:30:09,850] Trial 21 finished with value: 0.03226640845313733 and parameters: {'n_reservoir': 395, 'sparsity': 0.4, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 75}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006649964811845876\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:30:13,653] Trial 22 finished with value: 0.03288367779226212 and parameters: {'n_reservoir': 453, 'sparsity': 0.4, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 164}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006588048576104153\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0064933616025966735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:30:20,053] Trial 23 finished with value: 0.03297916632801981 and parameters: {'n_reservoir': 661, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.005, 'look_back': 75}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:30:23,166] Trial 24 finished with value: 0.03329859930363333 and parameters: {'n_reservoir': 364, 'sparsity': 0.4, 'spectral_radius': 0.1, 'noise': 0.009, 'look_back': 184}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.00662287184612226\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:30:26,861] Trial 25 finished with value: 0.03161257480012087 and parameters: {'n_reservoir': 480, 'sparsity': 0.5, 'spectral_radius': 0.2, 'noise': 0.001, 'look_back': 106}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006468084255524858\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006978090702374229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:30:30,423] Trial 26 finished with value: 0.03313998983242827 and parameters: {'n_reservoir': 489, 'sparsity': 0.5, 'spectral_radius': 0.2, 'noise': 0.001, 'look_back': 17}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:30:32,280] Trial 27 finished with value: 0.03219330363518045 and parameters: {'n_reservoir': 144, 'sparsity': 0.5, 'spectral_radius': 0.2, 'noise': 0.001, 'look_back': 105}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007700586034091585\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0064950037486039905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:30:39,908] Trial 28 finished with value: 0.0329268395923456 and parameters: {'n_reservoir': 848, 'sparsity': 0.5, 'spectral_radius': 0.2, 'noise': 0.007, 'look_back': 51}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006445005310755104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:30:46,380] Trial 29 finished with value: 0.03410755464902766 and parameters: {'n_reservoir': 655, 'sparsity': 0.3, 'spectral_radius': 1, 'noise': 0.001, 'look_back': 98}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:30:49,715] Trial 30 finished with value: 0.038032732358362506 and parameters: {'n_reservoir': 334, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.001, 'look_back': 263}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.00691863478754331\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006516156464417231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:30:54,238] Trial 31 finished with value: 0.03446888759070609 and parameters: {'n_reservoir': 494, 'sparsity': 0.4, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 157}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006702572314313011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:30:58,900] Trial 32 finished with value: 0.03761979169561343 and parameters: {'n_reservoir': 439, 'sparsity': 0.4, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 196}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0065914796653792915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:02,854] Trial 33 finished with value: 0.03697496073652258 and parameters: {'n_reservoir': 521, 'sparsity': 0.2, 'spectral_radius': 0.7, 'noise': 0.007, 'look_back': 53}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.00664111521617955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:06,055] Trial 34 finished with value: 0.03191723909216032 and parameters: {'n_reservoir': 412, 'sparsity': 0.3, 'spectral_radius': 0.9, 'noise': 0.009, 'look_back': 97}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:08,618] Trial 35 finished with value: 0.035254525337695984 and parameters: {'n_reservoir': 227, 'sparsity': 0.3, 'spectral_radius': 0.1, 'noise': 0.009, 'look_back': 96}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006757449830301565\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:10,867] Trial 36 finished with value: 0.033648951240079376 and parameters: {'n_reservoir': 340, 'sparsity': 0.3, 'spectral_radius': 0.9, 'noise': 0.009, 'look_back': 39}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006779802831513374\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:14,870] Trial 37 finished with value: 0.045929394508533686 and parameters: {'n_reservoir': 296, 'sparsity': 0.1, 'spectral_radius': 0.9, 'noise': 0.009, 'look_back': 432}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008083047703087442\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006690765763862841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:22,732] Trial 38 finished with value: 0.033923257208313906 and parameters: {'n_reservoir': 629, 'sparsity': 0.3, 'spectral_radius': 0.4, 'noise': 0.008, 'look_back': 306}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006653415640370042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:28,244] Trial 39 finished with value: 0.03261944959744941 and parameters: {'n_reservoir': 570, 'sparsity': 0.2, 'spectral_radius': 1.1, 'noise': 0.003, 'look_back': 216}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006561367027384007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:32,475] Trial 40 finished with value: 0.03154378224149714 and parameters: {'n_reservoir': 478, 'sparsity': 0.5, 'spectral_radius': 0.9, 'noise': 0.005, 'look_back': 69}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:36,249] Trial 41 finished with value: 0.03288289435719487 and parameters: {'n_reservoir': 485, 'sparsity': 0.5, 'spectral_radius': 0.9, 'noise': 0.005, 'look_back': 109}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006624701936625456\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:38,826] Trial 42 finished with value: 0.03449773672425147 and parameters: {'n_reservoir': 396, 'sparsity': 0.5, 'spectral_radius': 0.9, 'noise': 0.005, 'look_back': 34}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.00679740425455093\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006516725877495816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:42,890] Trial 43 finished with value: 0.03264503697565081 and parameters: {'n_reservoir': 519, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.002, 'look_back': 68}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:46,764] Trial 44 finished with value: 0.033664594914822925 and parameters: {'n_reservoir': 453, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.005, 'look_back': 10}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007361076060127413\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006473997009439574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:51,636] Trial 45 finished with value: 0.03796539255916004 and parameters: {'n_reservoir': 598, 'sparsity': 0.3, 'spectral_radius': 0.9, 'noise': 0.001, 'look_back': 82}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:54,405] Trial 46 finished with value: 0.032626443996259205 and parameters: {'n_reservoir': 354, 'sparsity': 0.5, 'spectral_radius': 0.2, 'noise': 0.009, 'look_back': 139}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006613231966936842\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:31:58,592] Trial 47 finished with value: 0.04042673231830219 and parameters: {'n_reservoir': 273, 'sparsity': 0.3, 'spectral_radius': 0.7, 'noise': 0.003, 'look_back': 361}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007539965106203292\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006450530721221888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:04,783] Trial 48 finished with value: 0.036052982014255115 and parameters: {'n_reservoir': 690, 'sparsity': 0.01, 'spectral_radius': 1.25, 'noise': 0.008, 'look_back': 119}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:06,227] Trial 49 finished with value: 0.030955157764063337 and parameters: {'n_reservoir': 217, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.006, 'look_back': 52}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006728234621433545\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:07,459] Trial 50 finished with value: 0.03168889517032212 and parameters: {'n_reservoir': 129, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.006, 'look_back': 51}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007136036111990838\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:08,514] Trial 51 finished with value: 0.03329270249143398 and parameters: {'n_reservoir': 92, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.006, 'look_back': 49}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007679968963435667\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:10,219] Trial 52 finished with value: 0.033168682347393806 and parameters: {'n_reservoir': 203, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.006, 'look_back': 30}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.0069332576528672646\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:11,340] Trial 53 finished with value: 0.034226578077699396 and parameters: {'n_reservoir': 143, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.006, 'look_back': 73}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007222331889106832\n",
            "harvesting states...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:11,723] Trial 54 finished with value: 0.61119582547762 and parameters: {'n_reservoir': 14, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.006, 'look_back': 56}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fitting...\n",
            "training error:\n",
            "0.01290394676158387\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:12,775] Trial 55 finished with value: 0.03300707247967009 and parameters: {'n_reservoir': 101, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.006, 'look_back': 143}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02691011946112212\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:15,773] Trial 56 finished with value: 0.05155110930421436 and parameters: {'n_reservoir': 150, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.007, 'look_back': 492}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008713084042866429\n",
            "harvesting states...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:16,249] Trial 57 finished with value: 0.032980988185569333 and parameters: {'n_reservoir': 63, 'sparsity': 0.5, 'spectral_radius': 0.6, 'noise': 0.006, 'look_back': 29}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fitting...\n",
            "training error:\n",
            "0.0084129028260338\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006501654955317954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:21,307] Trial 58 finished with value: 0.032507688190277 and parameters: {'n_reservoir': 545, 'sparsity': 0.2, 'spectral_radius': 0.8, 'noise': 0.001, 'look_back': 87}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:23,206] Trial 59 finished with value: 0.033520035110342725 and parameters: {'n_reservoir': 246, 'sparsity': 0.5, 'spectral_radius': 0.4, 'noise': 0.002, 'look_back': 118}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006875493280753066\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.00647482336212471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:30,409] Trial 60 finished with value: 0.03252933537107225 and parameters: {'n_reservoir': 797, 'sparsity': 0.01, 'spectral_radius': 0.5, 'noise': 0.005, 'look_back': 63}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:34,388] Trial 61 finished with value: 0.03174049113574748 and parameters: {'n_reservoir': 425, 'sparsity': 0.3, 'spectral_radius': 0.9, 'noise': 0.001, 'look_back': 95}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006571589558015757\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006599521805378313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:37,722] Trial 62 finished with value: 0.03191312641128724 and parameters: {'n_reservoir': 453, 'sparsity': 0.1, 'spectral_radius': 1.1, 'noise': 0.001, 'look_back': 83}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:39,755] Trial 63 finished with value: 0.03087482573110506 and parameters: {'n_reservoir': 305, 'sparsity': 0.5, 'spectral_radius': 0.2, 'noise': 0.001, 'look_back': 47}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006688740359997199\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:41,766] Trial 64 finished with value: 0.032072926306040694 and parameters: {'n_reservoir': 312, 'sparsity': 0.5, 'spectral_radius': 0.2, 'noise': 0.001, 'look_back': 41}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006762944658518978\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:42,953] Trial 65 finished with value: 0.032283519147043596 and parameters: {'n_reservoir': 186, 'sparsity': 0.5, 'spectral_radius': 0.2, 'noise': 0.007, 'look_back': 19}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007089429221125527\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0066765356519133835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:45,239] Trial 66 finished with value: 0.03379618145518798 and parameters: {'n_reservoir': 233, 'sparsity': 0.5, 'spectral_radius': 0.2, 'noise': 0.001, 'look_back': 61}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006649115177380969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:49,804] Trial 67 finished with value: 0.034294662128479 and parameters: {'n_reservoir': 576, 'sparsity': 0.5, 'spectral_radius': 0.2, 'noise': 0.006, 'look_back': 45}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:53,615] Trial 68 finished with value: 0.03441354229607701 and parameters: {'n_reservoir': 476, 'sparsity': 0.5, 'spectral_radius': 1.25, 'noise': 0.001, 'look_back': 125}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.0065891990607043595\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:54,906] Trial 69 finished with value: 0.03407814408019627 and parameters: {'n_reservoir': 165, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.007, 'look_back': 106}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006923774033864962\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006604555675791621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:32:58,685] Trial 70 finished with value: 0.03200264220634816 and parameters: {'n_reservoir': 368, 'sparsity': 0.5, 'spectral_radius': 0.1, 'noise': 0.005, 'look_back': 67}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:33:02,499] Trial 71 finished with value: 0.03256586179092272 and parameters: {'n_reservoir': 434, 'sparsity': 0.3, 'spectral_radius': 0.9, 'noise': 0.001, 'look_back': 93}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006584180973790785\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006507543771068184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:33:06,566] Trial 72 finished with value: 0.03166221258589127 and parameters: {'n_reservoir': 523, 'sparsity': 0.3, 'spectral_radius': 0.5, 'noise': 0.001, 'look_back': 79}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006766748822103105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:33:11,052] Trial 73 finished with value: 0.03208318791046342 and parameters: {'n_reservoir': 521, 'sparsity': 0.3, 'spectral_radius': 0.5, 'noise': 0.001, 'look_back': 28}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:33:12,018] Trial 74 finished with value: 0.032988124425416376 and parameters: {'n_reservoir': 117, 'sparsity': 0.2, 'spectral_radius': 0.5, 'noise': 0.001, 'look_back': 78}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007358023078137124\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006531829784349854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:33:17,046] Trial 75 finished with value: 0.03163138694942003 and parameters: {'n_reservoir': 631, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.003, 'look_back': 53}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.007306554921257375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:33:22,433] Trial 76 finished with value: 0.03379544822073775 and parameters: {'n_reservoir': 620, 'sparsity': 0.01, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 12}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006449954162915292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:33:28,974] Trial 77 finished with value: 0.03253895671853582 and parameters: {'n_reservoir': 715, 'sparsity': 0.1, 'spectral_radius': 0.6, 'noise': 0.003, 'look_back': 107}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006442245354741819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:33:35,698] Trial 78 finished with value: 0.03410996417508181 and parameters: {'n_reservoir': 644, 'sparsity': 0.3, 'spectral_radius': 0.7, 'noise': 0.003, 'look_back': 172}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006581819748679534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:33:39,836] Trial 79 finished with value: 0.03145945657847379 and parameters: {'n_reservoir': 548, 'sparsity': 0.5, 'spectral_radius': 0.8, 'noise': 0.008, 'look_back': 64}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006574920570488894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:33:44,705] Trial 80 finished with value: 0.03207465403501336 and parameters: {'n_reservoir': 602, 'sparsity': 0.5, 'spectral_radius': 0.8, 'noise': 0.008, 'look_back': 61}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006507100651694672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:33:49,817] Trial 81 finished with value: 0.032547845405551445 and parameters: {'n_reservoir': 551, 'sparsity': 0.5, 'spectral_radius': 0.8, 'noise': 0.008, 'look_back': 77}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.023844953084845025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:33:53,406] Trial 82 finished with value: 0.03640025433465471 and parameters: {'n_reservoir': 504, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.008, 'look_back': 40}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006913304273364561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:33:56,459] Trial 83 finished with value: 0.03199425392487105 and parameters: {'n_reservoir': 463, 'sparsity': 0.4, 'spectral_radius': 0.8, 'noise': 0.001, 'look_back': 26}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006466412216891936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:01,960] Trial 84 finished with value: 0.034308613656986654 and parameters: {'n_reservoir': 532, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.002, 'look_back': 149}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006486555737075303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:06,746] Trial 85 finished with value: 0.03398440770330038 and parameters: {'n_reservoir': 572, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.003, 'look_back': 113}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006564511878374732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:10,270] Trial 86 finished with value: 0.03329595522844219 and parameters: {'n_reservoir': 381, 'sparsity': 0.3, 'spectral_radius': 0.2, 'noise': 0.008, 'look_back': 134}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:12,836] Trial 87 finished with value: 0.03285472055243151 and parameters: {'n_reservoir': 322, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.005, 'look_back': 51}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006664812184235271\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006544156780098779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:20,072] Trial 88 finished with value: 0.03272242046952309 and parameters: {'n_reservoir': 677, 'sparsity': 0.5, 'spectral_radius': 0.5, 'noise': 0.007, 'look_back': 254}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006556355576491042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:24,601] Trial 89 finished with value: 0.030894679808945138 and parameters: {'n_reservoir': 485, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 71}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006572409457290857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:28,092] Trial 90 finished with value: 0.03414016406348846 and parameters: {'n_reservoir': 486, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 66}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:30,909] Trial 91 finished with value: 0.03113641179116476 and parameters: {'n_reservoir': 398, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.001, 'look_back': 89}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006524447625109688\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006560062712987448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:33,974] Trial 92 finished with value: 0.03339473645102317 and parameters: {'n_reservoir': 394, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 86}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:37,658] Trial 93 finished with value: 0.03435237690251825 and parameters: {'n_reservoir': 415, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 100}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006548410550525642\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:39,366] Trial 94 finished with value: 0.031477538877315484 and parameters: {'n_reservoir': 266, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 37}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.00678993527736208\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:41,130] Trial 95 finished with value: 0.03201096376871019 and parameters: {'n_reservoir': 274, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 39}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006775846858392423\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:42,906] Trial 96 finished with value: 0.03139286534194375 and parameters: {'n_reservoir': 250, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 69}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006783750071822628\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:44,177] Trial 97 finished with value: 0.0322710842368517 and parameters: {'n_reservoir': 200, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 21}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007130952739219939\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006626008770791709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:46,822] Trial 98 finished with value: 0.03615291426368435 and parameters: {'n_reservoir': 301, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 68}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:49,695] Trial 99 finished with value: 0.03183808416693582 and parameters: {'n_reservoir': 342, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.004, 'look_back': 88}. Best is trial 2 with value: 0.030801385450078596.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006654324077371295\n",
            "done\n",
            "ESN HyperParameters Tuning Elapsed Time : 392.54067 seconds\n",
            "Best Parameters:  {'n_reservoir': 485, 'sparsity': 0.3, 'spectral_radius': 0.5, 'noise': 0.001, 'look_back': 71}\n",
            "Best Loss:  0.030801385450078596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 8\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "valid_size = int(len(train) * 0.1)\n",
        "temp_train = train[:-valid_size]\n",
        "valid = train[-valid_size:]\n",
        "print(len(train), len(temp_train), len(valid), len(test))\n",
        "\n",
        "# Define the objective function for Optuna optimization\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters to search\n",
        "\n",
        "    n_reservoir = trial.suggest_int('n_reservoir', 10, 1000)   # -\n",
        "    sparsity = trial.suggest_categorical('sparsity', [0.01, 0.1, 0.2, 0.3, 0.4, 0.5])   # -\n",
        "    spectral_radius = trial.suggest_categorical('spectral_radius', [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.25, 10.0])   # - spectral radius of W\n",
        "    noise = trial.suggest_categorical('noise', [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009])       # - Noise Set\n",
        "    look_back = trial.suggest_int('look_back', 10, 500)\n",
        "\n",
        "    # reshape into X=t and Y=t+1\n",
        "    trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "    validX, validY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "    # reshape input to be [samples, time steps, features]\n",
        "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1]))\n",
        "    validX = np.reshape(validX, (validX.shape[0], validX.shape[1]))\n",
        "\n",
        "    model = ESN(n_inputs = look_back,\n",
        "            n_outputs = horizon,\n",
        "            n_reservoir = n_reservoir,\n",
        "            sparsity=sparsity,\n",
        "            random_state=1234,\n",
        "            spectral_radius=spectral_radius,\n",
        "            noise = noise,\n",
        "            silent = False,\n",
        "            teacher_scaling = 10,\n",
        "    )\n",
        "\n",
        "    pred_train = model.fit(trainX, trainY)\n",
        "\n",
        "    # make predictions\n",
        "    testPredict = model.predict(validX)\n",
        "    testPredict = np.array(testPredict)\n",
        "\n",
        "    testRMSE = np.sqrt(mean_squared_error(validY, testPredict[:len(validY), :]))\n",
        "\n",
        "    return testRMSE\n",
        "\n",
        "\n",
        "# Create the Optuna study\n",
        "study = optuna.create_study(direction='minimize')\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "start_time = time.time()\n",
        "\n",
        "# Run the optimization\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Record the ending time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print('done')\n",
        "print(\"ESN HyperParameters Tuning Elapsed Time : %.5f\" % (elapsed_time), \"seconds\")\n",
        "\n",
        "# Print the best parameters and corresponding loss\n",
        "best_params = study.best_params\n",
        "best_loss = study.best_value\n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Best Loss: \", best_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktYDPrWokeUI",
        "outputId": "af8dac0f-d33d-40a8-d0be-ab7e02d32682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:49,735] A new study created in memory with name: no-name-179a7dd7-1741-42aa-8146-9e9788c9040a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 16192 1799 2000\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.007241253937770114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:34:56,266] Trial 0 finished with value: 0.04310083943641763 and parameters: {'n_reservoir': 464, 'sparsity': 0.2, 'spectral_radius': 1.1, 'noise': 0.005, 'look_back': 493}. Best is trial 0 with value: 0.04310083943641763.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006026653454832251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:35:05,474] Trial 1 finished with value: 0.03935861684859025 and parameters: {'n_reservoir': 721, 'sparsity': 0.3, 'spectral_radius': 1.1, 'noise': 0.004, 'look_back': 289}. Best is trial 1 with value: 0.03935861684859025.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006165599542756192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:35:14,952] Trial 2 finished with value: 0.038569874463180845 and parameters: {'n_reservoir': 712, 'sparsity': 0.1, 'spectral_radius': 0.9, 'noise': 0.005, 'look_back': 334}. Best is trial 2 with value: 0.038569874463180845.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.00565775322975698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:35:27,236] Trial 3 finished with value: 0.03958909572451682 and parameters: {'n_reservoir': 974, 'sparsity': 0.1, 'spectral_radius': 0.7, 'noise': 0.007, 'look_back': 209}. Best is trial 2 with value: 0.038569874463180845.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006386068084164307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:35:37,231] Trial 4 finished with value: 0.04289148413843493 and parameters: {'n_reservoir': 706, 'sparsity': 0.5, 'spectral_radius': 0.8, 'noise': 0.009, 'look_back': 448}. Best is trial 2 with value: 0.038569874463180845.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:35:38,854] Trial 5 finished with value: 0.03957011491223543 and parameters: {'n_reservoir': 220, 'sparsity': 0.5, 'spectral_radius': 0.9, 'noise': 0.005, 'look_back': 111}. Best is trial 2 with value: 0.038569874463180845.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006957916809424598\n",
            "harvesting states...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:35:39,227] Trial 6 finished with value: 0.10672037992708115 and parameters: {'n_reservoir': 42, 'sparsity': 0.3, 'spectral_radius': 0.7, 'noise': 0.006, 'look_back': 15}. Best is trial 2 with value: 0.038569874463180845.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fitting...\n",
            "training error:\n",
            "0.014550719099714874\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006108420135069508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:35:46,771] Trial 7 finished with value: 0.04266204412414113 and parameters: {'n_reservoir': 661, 'sparsity': 0.1, 'spectral_radius': 0.8, 'noise': 0.002, 'look_back': 304}. Best is trial 2 with value: 0.038569874463180845.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:35:49,922] Trial 8 finished with value: 0.0366592685910007 and parameters: {'n_reservoir': 301, 'sparsity': 0.2, 'spectral_radius': 0.7, 'noise': 0.008, 'look_back': 87}. Best is trial 8 with value: 0.0366592685910007.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006144511149045821\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:35:52,813] Trial 9 finished with value: 0.038046021305906015 and parameters: {'n_reservoir': 299, 'sparsity': 0.4, 'spectral_radius': 1.25, 'noise': 0.007, 'look_back': 249}. Best is trial 8 with value: 0.0366592685910007.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006955510152635205\n",
            "harvesting states...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:35:53,222] Trial 10 finished with value: 0.03744776558287213 and parameters: {'n_reservoir': 56, 'sparsity': 0.2, 'spectral_radius': 0.5, 'noise': 0.008, 'look_back': 11}. Best is trial 8 with value: 0.0366592685910007.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fitting...\n",
            "training error:\n",
            "0.008501937646868156\n",
            "harvesting states...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:35:53,567] Trial 11 finished with value: 1.1933669391422972 and parameters: {'n_reservoir': 28, 'sparsity': 0.2, 'spectral_radius': 0.5, 'noise': 0.008, 'look_back': 22}. Best is trial 8 with value: 0.0366592685910007.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fitting...\n",
            "training error:\n",
            "0.016285099196895792\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:35:55,423] Trial 12 finished with value: 0.040242479423606606 and parameters: {'n_reservoir': 248, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.008, 'look_back': 115}. Best is trial 8 with value: 0.0366592685910007.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.02671862454062592\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:35:58,175] Trial 13 finished with value: 0.036816575887162875 and parameters: {'n_reservoir': 371, 'sparsity': 0.01, 'spectral_radius': 0.5, 'noise': 0.008, 'look_back': 109}. Best is trial 8 with value: 0.0366592685910007.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006025311093661607\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:36:02,310] Trial 14 finished with value: 0.03664349964348987 and parameters: {'n_reservoir': 420, 'sparsity': 0.01, 'spectral_radius': 0.3, 'noise': 0.003, 'look_back': 135}. Best is trial 14 with value: 0.03664349964348987.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.0060371311593809054\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006002410517812402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:36:06,914] Trial 15 finished with value: 0.04189140260791323 and parameters: {'n_reservoir': 519, 'sparsity': 0.01, 'spectral_radius': 0.3, 'noise': 0.003, 'look_back': 175}. Best is trial 14 with value: 0.03664349964348987.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005846279753346865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:36:10,922] Trial 16 finished with value: 0.04217254044917682 and parameters: {'n_reservoir': 469, 'sparsity': 0.01, 'spectral_radius': 0.1, 'noise': 0.001, 'look_back': 166}. Best is trial 14 with value: 0.03664349964348987.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0057479048946711755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:36:16,285] Trial 17 finished with value: 0.03675151868299306 and parameters: {'n_reservoir': 557, 'sparsity': 0.4, 'spectral_radius': 0.3, 'noise': 0.003, 'look_back': 88}. Best is trial 14 with value: 0.03664349964348987.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:36:18,783] Trial 18 finished with value: 0.04686237369330825 and parameters: {'n_reservoir': 165, 'sparsity': 0.01, 'spectral_radius': 0.4, 'noise': 0.003, 'look_back': 382}. Best is trial 14 with value: 0.03664349964348987.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.009510610543603781\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005621904625845097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:36:27,975] Trial 19 finished with value: 0.03645996113141707 and parameters: {'n_reservoir': 865, 'sparsity': 0.2, 'spectral_radius': 0.2, 'noise': 0.002, 'look_back': 70}. Best is trial 19 with value: 0.03645996113141707.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005542627383677336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:36:40,510] Trial 20 finished with value: 0.03918713857467925 and parameters: {'n_reservoir': 979, 'sparsity': 0.01, 'spectral_radius': 0.2, 'noise': 0.002, 'look_back': 163}. Best is trial 19 with value: 0.03645996113141707.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005697793043128239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:36:49,178] Trial 21 finished with value: 0.0362639309457856 and parameters: {'n_reservoir': 860, 'sparsity': 0.2, 'spectral_radius': 1, 'noise': 0.002, 'look_back': 69}. Best is trial 21 with value: 0.0362639309457856.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005678042810008501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:36:57,151] Trial 22 finished with value: 0.038338137056568966 and parameters: {'n_reservoir': 839, 'sparsity': 0.2, 'spectral_radius': 1, 'noise': 0.002, 'look_back': 61}. Best is trial 21 with value: 0.0362639309457856.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0056919270788425265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:37:05,774] Trial 23 finished with value: 0.03710673537087784 and parameters: {'n_reservoir': 850, 'sparsity': 0.2, 'spectral_radius': 0.6, 'noise': 0.002, 'look_back': 61}. Best is trial 21 with value: 0.0362639309457856.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0055923570327637215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:37:15,020] Trial 24 finished with value: 0.03855605159553951 and parameters: {'n_reservoir': 826, 'sparsity': 0.2, 'spectral_radius': 0.2, 'noise': 0.002, 'look_back': 137}. Best is trial 21 with value: 0.0362639309457856.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0059278867405989895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:37:21,210] Trial 25 finished with value: 0.03989213374109054 and parameters: {'n_reservoir': 618, 'sparsity': 0.3, 'spectral_radius': 1, 'noise': 0.004, 'look_back': 217}. Best is trial 21 with value: 0.0362639309457856.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005632518212993759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:37:31,239] Trial 26 finished with value: 0.03642249674403862 and parameters: {'n_reservoir': 921, 'sparsity': 0.5, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 54}. Best is trial 21 with value: 0.0362639309457856.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005636727345853751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:37:41,190] Trial 27 finished with value: 0.035914705108512884 and parameters: {'n_reservoir': 905, 'sparsity': 0.5, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 51}. Best is trial 27 with value: 0.035914705108512884.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005663659288559724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:37:50,741] Trial 28 finished with value: 0.036161336330101275 and parameters: {'n_reservoir': 921, 'sparsity': 0.5, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 60}. Best is trial 27 with value: 0.035914705108512884.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005796448913459432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:37:59,306] Trial 29 finished with value: 0.037954650435243285 and parameters: {'n_reservoir': 786, 'sparsity': 0.5, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 201}. Best is trial 27 with value: 0.035914705108512884.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.00576475573187494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:38:08,861] Trial 30 finished with value: 0.03701881202243932 and parameters: {'n_reservoir': 918, 'sparsity': 0.5, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 47}. Best is trial 27 with value: 0.035914705108512884.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005732208281738811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:38:18,303] Trial 31 finished with value: 0.0368078334121192 and parameters: {'n_reservoir': 925, 'sparsity': 0.5, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 42}. Best is trial 27 with value: 0.035914705108512884.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005710868918912731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:38:25,633] Trial 32 finished with value: 0.039815625465855466 and parameters: {'n_reservoir': 782, 'sparsity': 0.5, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 82}. Best is trial 27 with value: 0.035914705108512884.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.00559961239361765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:38:36,455] Trial 33 finished with value: 0.036578040029849226 and parameters: {'n_reservoir': 922, 'sparsity': 0.5, 'spectral_radius': 1.1, 'noise': 0.006, 'look_back': 131}. Best is trial 27 with value: 0.035914705108512884.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0057160526179499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:38:47,082] Trial 34 finished with value: 0.03889133380816666 and parameters: {'n_reservoir': 1000, 'sparsity': 0.5, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 42}. Best is trial 27 with value: 0.035914705108512884.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0064440296063579304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:38:59,177] Trial 35 finished with value: 0.04503119246402809 and parameters: {'n_reservoir': 781, 'sparsity': 0.5, 'spectral_radius': 1, 'noise': 0.009, 'look_back': 477}. Best is trial 27 with value: 0.035914705108512884.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005953174015249214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:39:12,250] Trial 36 finished with value: 0.047801116258243445 and parameters: {'n_reservoir': 903, 'sparsity': 0.5, 'spectral_radius': 1, 'noise': 0.001, 'look_back': 386}. Best is trial 27 with value: 0.035914705108512884.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005786271047381415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:39:19,356] Trial 37 finished with value: 0.03569957355749263 and parameters: {'n_reservoir': 706, 'sparsity': 0.1, 'spectral_radius': 1.25, 'noise': 0.005, 'look_back': 100}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005730664107719632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:39:26,006] Trial 38 finished with value: 0.037662418903262364 and parameters: {'n_reservoir': 726, 'sparsity': 0.1, 'spectral_radius': 1.25, 'noise': 0.005, 'look_back': 100}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0061580403078687306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:39:33,429] Trial 39 finished with value: 0.039579645559653945 and parameters: {'n_reservoir': 627, 'sparsity': 0.1, 'spectral_radius': 1.25, 'noise': 0.005, 'look_back': 258}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.02142732091280202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:39:40,138] Trial 40 finished with value: 0.04766470064855719 and parameters: {'n_reservoir': 745, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.005, 'look_back': 33}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.00562205586141665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:39:49,561] Trial 41 finished with value: 0.03832482784430603 and parameters: {'n_reservoir': 944, 'sparsity': 0.3, 'spectral_radius': 0.4, 'noise': 0.006, 'look_back': 65}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006312329659575021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:39:58,522] Trial 42 finished with value: 0.03852524282271639 and parameters: {'n_reservoir': 885, 'sparsity': 0.5, 'spectral_radius': 0.1, 'noise': 0.007, 'look_back': 11}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.00567755080779043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:40:05,077] Trial 43 finished with value: 0.03877701593525139 and parameters: {'n_reservoir': 686, 'sparsity': 0.1, 'spectral_radius': 0.9, 'noise': 0.004, 'look_back': 87}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005587267508848479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:40:16,327] Trial 44 finished with value: 0.03875045884113345 and parameters: {'n_reservoir': 963, 'sparsity': 0.4, 'spectral_radius': 0.8, 'noise': 0.006, 'look_back': 122}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005759167902555294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:40:25,083] Trial 45 finished with value: 0.03718984203907833 and parameters: {'n_reservoir': 808, 'sparsity': 0.5, 'spectral_radius': 1.25, 'noise': 0.009, 'look_back': 153}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005869821780062872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:40:32,213] Trial 46 finished with value: 0.038728851439644885 and parameters: {'n_reservoir': 737, 'sparsity': 0.1, 'spectral_radius': 0.6, 'noise': 0.005, 'look_back': 31}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0056221662673464805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:40:41,324] Trial 47 finished with value: 0.03919304768495957 and parameters: {'n_reservoir': 876, 'sparsity': 0.5, 'spectral_radius': 1.1, 'noise': 0.006, 'look_back': 103}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005838226380002338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:40:48,183] Trial 48 finished with value: 0.04285105353791422 and parameters: {'n_reservoir': 658, 'sparsity': 0.3, 'spectral_radius': 0.7, 'noise': 0.007, 'look_back': 190}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005667149272576211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:40:59,132] Trial 49 finished with value: 0.03590612781967089 and parameters: {'n_reservoir': 997, 'sparsity': 0.4, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 49}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005619531782015719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:41:10,268] Trial 50 finished with value: 0.039358839283105046 and parameters: {'n_reservoir': 996, 'sparsity': 0.4, 'spectral_radius': 1.25, 'noise': 0.001, 'look_back': 81}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0056846449901774105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:41:19,979] Trial 51 finished with value: 0.0392012162373271 and parameters: {'n_reservoir': 936, 'sparsity': 0.4, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 51}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005831808856830149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:41:28,055] Trial 52 finished with value: 0.038866036759231934 and parameters: {'n_reservoir': 885, 'sparsity': 0.4, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 25}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006270836305152576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:41:38,983] Trial 53 finished with value: 0.04378972884243291 and parameters: {'n_reservoir': 969, 'sparsity': 0.4, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 10}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005669408231799493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:41:47,624] Trial 54 finished with value: 0.037340377426107615 and parameters: {'n_reservoir': 820, 'sparsity': 0.5, 'spectral_radius': 0.9, 'noise': 0.005, 'look_back': 73}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005619624857622608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:41:56,620] Trial 55 finished with value: 0.03698437845715805 and parameters: {'n_reservoir': 856, 'sparsity': 0.2, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 104}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005649220304017525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:42:06,089] Trial 56 finished with value: 0.03804932034606751 and parameters: {'n_reservoir': 953, 'sparsity': 0.1, 'spectral_radius': 0.5, 'noise': 0.002, 'look_back': 52}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005780949371871288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:42:11,891] Trial 57 finished with value: 0.04139647673102654 and parameters: {'n_reservoir': 575, 'sparsity': 0.5, 'spectral_radius': 0.8, 'noise': 0.004, 'look_back': 148}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.021237216490839185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:42:19,294] Trial 58 finished with value: 0.0480990589811069 and parameters: {'n_reservoir': 755, 'sparsity': 0.2, 'spectral_radius': 10.0, 'noise': 0.009, 'look_back': 116}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005637122770225132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:42:28,967] Trial 59 finished with value: 0.036417882395063286 and parameters: {'n_reservoir': 895, 'sparsity': 0.4, 'spectral_radius': 1, 'noise': 0.008, 'look_back': 93}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005631448791213622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:42:37,712] Trial 60 finished with value: 0.03695012162137158 and parameters: {'n_reservoir': 830, 'sparsity': 0.4, 'spectral_radius': 0.1, 'noise': 0.008, 'look_back': 98}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005603482564894479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:42:47,457] Trial 61 finished with value: 0.03737645581845009 and parameters: {'n_reservoir': 905, 'sparsity': 0.4, 'spectral_radius': 1, 'noise': 0.008, 'look_back': 75}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005683311750249341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:42:56,023] Trial 62 finished with value: 0.036663550572350136 and parameters: {'n_reservoir': 865, 'sparsity': 0.4, 'spectral_radius': 1, 'noise': 0.006, 'look_back': 61}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005817163125081623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:43:05,569] Trial 63 finished with value: 0.04205772359068815 and parameters: {'n_reservoir': 949, 'sparsity': 0.4, 'spectral_radius': 1, 'noise': 0.008, 'look_back': 33}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005795997358992114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:43:17,697] Trial 64 finished with value: 0.04192196711160074 and parameters: {'n_reservoir': 899, 'sparsity': 0.5, 'spectral_radius': 0.3, 'noise': 0.002, 'look_back': 301}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005644471042464305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:43:26,044] Trial 65 finished with value: 0.03796603457177419 and parameters: {'n_reservoir': 796, 'sparsity': 0.2, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 88}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005667176183890291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:43:36,599] Trial 66 finished with value: 0.0362478140549082 and parameters: {'n_reservoir': 993, 'sparsity': 0.01, 'spectral_radius': 0.4, 'noise': 0.008, 'look_back': 48}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005561947182394197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:43:48,925] Trial 67 finished with value: 0.03950784213681104 and parameters: {'n_reservoir': 1000, 'sparsity': 0.01, 'spectral_radius': 0.4, 'noise': 0.008, 'look_back': 122}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005970034781302484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:43:58,643] Trial 68 finished with value: 0.037373212864724545 and parameters: {'n_reservoir': 959, 'sparsity': 0.01, 'spectral_radius': 0.4, 'noise': 0.008, 'look_back': 21}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.00567847381580692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:44:06,188] Trial 69 finished with value: 0.036089816555964584 and parameters: {'n_reservoir': 843, 'sparsity': 0.01, 'spectral_radius': 0.4, 'noise': 0.008, 'look_back': 44}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005744290320398042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:44:14,700] Trial 70 finished with value: 0.03704849420553108 and parameters: {'n_reservoir': 853, 'sparsity': 0.01, 'spectral_radius': 0.4, 'noise': 0.008, 'look_back': 37}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0056250246586787974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:44:25,354] Trial 71 finished with value: 0.03842775095527346 and parameters: {'n_reservoir': 976, 'sparsity': 0.01, 'spectral_radius': 0.4, 'noise': 0.008, 'look_back': 66}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:44:27,023] Trial 72 finished with value: 0.05573424778588254 and parameters: {'n_reservoir': 84, 'sparsity': 0.01, 'spectral_radius': 0.4, 'noise': 0.008, 'look_back': 342}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.009470929860891262\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005687317559544891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:44:36,675] Trial 73 finished with value: 0.037373210449534254 and parameters: {'n_reservoir': 925, 'sparsity': 0.01, 'spectral_radius': 0.7, 'noise': 0.008, 'look_back': 48}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005583069807437536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:44:43,776] Trial 74 finished with value: 0.03890363417086563 and parameters: {'n_reservoir': 771, 'sparsity': 0.01, 'spectral_radius': 0.6, 'noise': 0.002, 'look_back': 94}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005915079831643151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:44:52,762] Trial 75 finished with value: 0.04039221949548755 and parameters: {'n_reservoir': 891, 'sparsity': 0.3, 'spectral_radius': 1.25, 'noise': 0.005, 'look_back': 23}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:44:55,693] Trial 76 finished with value: 0.03688578349082629 and parameters: {'n_reservoir': 415, 'sparsity': 0.1, 'spectral_radius': 0.4, 'noise': 0.001, 'look_back': 78}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005808294706166635\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.00567727842385031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:45:04,165] Trial 77 finished with value: 0.036420544301812706 and parameters: {'n_reservoir': 831, 'sparsity': 0.2, 'spectral_radius': 0.5, 'noise': 0.007, 'look_back': 60}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005822359035864342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:45:11,458] Trial 78 finished with value: 0.04053799079466366 and parameters: {'n_reservoir': 699, 'sparsity': 0.01, 'spectral_radius': 1.1, 'noise': 0.008, 'look_back': 143}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0055838885342373345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:45:21,660] Trial 79 finished with value: 0.03756608298544346 and parameters: {'n_reservoir': 938, 'sparsity': 0.4, 'spectral_radius': 0.2, 'noise': 0.004, 'look_back': 109}. Best is trial 37 with value: 0.03569957355749263.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005699949050913774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:45:32,333] Trial 80 finished with value: 0.035200750018335836 and parameters: {'n_reservoir': 981, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 39}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005709463318432795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:45:42,861] Trial 81 finished with value: 0.040388509163098864 and parameters: {'n_reservoir': 975, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 42}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005634520007793111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:45:54,340] Trial 82 finished with value: 0.03663771399313836 and parameters: {'n_reservoir': 915, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.005, 'look_back': 72}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005610489003336456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:46:05,132] Trial 83 finished with value: 0.03732214030728184 and parameters: {'n_reservoir': 996, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 51}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005858798824807414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:46:13,299] Trial 84 finished with value: 0.03911452130658912 and parameters: {'n_reservoir': 871, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 33}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.00596145443530271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:46:22,713] Trial 85 finished with value: 0.03763839997024614 and parameters: {'n_reservoir': 947, 'sparsity': 0.01, 'spectral_radius': 0.9, 'noise': 0.002, 'look_back': 16}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0059314489200206185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:46:33,419] Trial 86 finished with value: 0.03818089816771323 and parameters: {'n_reservoir': 845, 'sparsity': 0.1, 'spectral_radius': 1.25, 'noise': 0.003, 'look_back': 253}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005602990936763029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:46:43,670] Trial 87 finished with value: 0.0372128333629142 and parameters: {'n_reservoir': 896, 'sparsity': 0.2, 'spectral_radius': 0.3, 'noise': 0.006, 'look_back': 128}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0056658148127397626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:46:52,019] Trial 88 finished with value: 0.03864469124281802 and parameters: {'n_reservoir': 807, 'sparsity': 0.4, 'spectral_radius': 1, 'noise': 0.008, 'look_back': 91}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.01999153738448245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:47:02,259] Trial 89 finished with value: 0.050475395400643816 and parameters: {'n_reservoir': 979, 'sparsity': 0.3, 'spectral_radius': 10.0, 'noise': 0.009, 'look_back': 61}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005679781344160397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:47:11,853] Trial 90 finished with value: 0.03677977373005191 and parameters: {'n_reservoir': 929, 'sparsity': 0.5, 'spectral_radius': 0.8, 'noise': 0.006, 'look_back': 43}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0056683848571495235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:47:20,198] Trial 91 finished with value: 0.03725040946243938 and parameters: {'n_reservoir': 832, 'sparsity': 0.2, 'spectral_radius': 0.4, 'noise': 0.007, 'look_back': 57}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005787215723493478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:47:29,265] Trial 92 finished with value: 0.0402000827261183 and parameters: {'n_reservoir': 762, 'sparsity': 0.2, 'spectral_radius': 0.5, 'noise': 0.007, 'look_back': 233}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005669088309189322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:47:37,632] Trial 93 finished with value: 0.04177496403012216 and parameters: {'n_reservoir': 869, 'sparsity': 0.2, 'spectral_radius': 0.5, 'noise': 0.007, 'look_back': 73}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:47:42,048] Trial 94 finished with value: 0.035994218395773124 and parameters: {'n_reservoir': 506, 'sparsity': 0.2, 'spectral_radius': 0.5, 'noise': 0.007, 'look_back': 25}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005973550308064035\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:47:44,139] Trial 95 finished with value: 0.03584241983191725 and parameters: {'n_reservoir': 327, 'sparsity': 0.2, 'spectral_radius': 1, 'noise': 0.005, 'look_back': 25}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.00603252250042589\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:47:47,614] Trial 96 finished with value: 0.03574707925388125 and parameters: {'n_reservoir': 502, 'sparsity': 0.2, 'spectral_radius': 0.5, 'noise': 0.005, 'look_back': 26}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005926547821173832\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:47:49,608] Trial 97 finished with value: 0.03653675250625664 and parameters: {'n_reservoir': 323, 'sparsity': 0.2, 'spectral_radius': 0.5, 'noise': 0.005, 'look_back': 23}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.00619563463917762\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005936637576383639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:47:53,948] Trial 98 finished with value: 0.03575908481061348 and parameters: {'n_reservoir': 493, 'sparsity': 0.2, 'spectral_radius': 0.5, 'noise': 0.005, 'look_back': 29}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006382864695717489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:47:57,224] Trial 99 finished with value: 0.037455476547877724 and parameters: {'n_reservoir': 495, 'sparsity': 0.2, 'spectral_radius': 0.5, 'noise': 0.005, 'look_back': 11}. Best is trial 80 with value: 0.035200750018335836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "ESN HyperParameters Tuning Elapsed Time : 787.49570 seconds\n",
            "Best Parameters:  {'n_reservoir': 981, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 39}\n",
            "Best Loss:  0.035200750018335836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 10\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "valid_size = int(len(train) * 0.1)\n",
        "temp_train = train[:-valid_size]\n",
        "valid = train[-valid_size:]\n",
        "print(len(train), len(temp_train), len(valid), len(test))\n",
        "\n",
        "# Define the objective function for Optuna optimization\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters to search\n",
        "\n",
        "    n_reservoir = trial.suggest_int('n_reservoir', 10, 1000)   # -\n",
        "    sparsity = trial.suggest_categorical('sparsity', [0.01, 0.1, 0.2, 0.3, 0.4, 0.5])   # -\n",
        "    spectral_radius = trial.suggest_categorical('spectral_radius', [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.25, 10.0])   # - spectral radius of W\n",
        "    noise = trial.suggest_categorical('noise', [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009])       # - Noise Set\n",
        "    look_back = trial.suggest_int('look_back', 10, 500)\n",
        "\n",
        "    # reshape into X=t and Y=t+1\n",
        "    trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "    validX, validY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "    # reshape input to be [samples, time steps, features]\n",
        "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1]))\n",
        "    validX = np.reshape(validX, (validX.shape[0], validX.shape[1]))\n",
        "\n",
        "    model = ESN(n_inputs = look_back,\n",
        "            n_outputs = horizon,\n",
        "            n_reservoir = n_reservoir,\n",
        "            sparsity=sparsity,\n",
        "            random_state=1234,\n",
        "            spectral_radius=spectral_radius,\n",
        "            noise = noise,\n",
        "            silent = False,\n",
        "            teacher_scaling = 10,\n",
        "    )\n",
        "\n",
        "    pred_train = model.fit(trainX, trainY)\n",
        "\n",
        "    # make predictions\n",
        "    testPredict = model.predict(validX)\n",
        "    testPredict = np.array(testPredict)\n",
        "\n",
        "    testRMSE = np.sqrt(mean_squared_error(validY, testPredict[:len(validY), :]))\n",
        "\n",
        "    return testRMSE\n",
        "\n",
        "\n",
        "# Create the Optuna study\n",
        "study = optuna.create_study(direction='minimize')\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "start_time = time.time()\n",
        "\n",
        "# Run the optimization\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Record the ending time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print('done')\n",
        "print(\"ESN HyperParameters Tuning Elapsed Time : %.5f\" % (elapsed_time), \"seconds\")\n",
        "\n",
        "# Print the best parameters and corresponding loss\n",
        "best_params = study.best_params\n",
        "best_loss = study.best_value\n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Best Loss: \", best_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAaDufktkf7y",
        "outputId": "1883fef1-a369-4653-d004-431a9936d9c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:47:57,261] A new study created in memory with name: no-name-1bf0bcb8-87f6-4085-aecd-399fd0c54daa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 16192 1799 2000\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:47:58,983] Trial 0 finished with value: 0.0457867247462088 and parameters: {'n_reservoir': 225, 'sparsity': 0.4, 'spectral_radius': 1.25, 'noise': 0.001, 'look_back': 115}. Best is trial 0 with value: 0.0457867247462088.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.00694723702183422\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:48:01,976] Trial 1 finished with value: 0.6130436169988811 and parameters: {'n_reservoir': 167, 'sparsity': 0.3, 'spectral_radius': 0.3, 'noise': 0.008, 'look_back': 467}. Best is trial 0 with value: 0.0457867247462088.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.009226005407571475\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005655989462991681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:48:12,658] Trial 2 finished with value: 0.042929185138047936 and parameters: {'n_reservoir': 1000, 'sparsity': 0.3, 'spectral_radius': 0.2, 'noise': 0.007, 'look_back': 10}. Best is trial 2 with value: 0.042929185138047936.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006688715590952776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:48:14,931] Trial 3 finished with value: 0.04172803209447328 and parameters: {'n_reservoir': 251, 'sparsity': 0.4, 'spectral_radius': 0.7, 'noise': 0.008, 'look_back': 155}. Best is trial 3 with value: 0.04172803209447328.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005306244363360522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:48:25,239] Trial 4 finished with value: 0.0494823007974308 and parameters: {'n_reservoir': 876, 'sparsity': 0.1, 'spectral_radius': 0.8, 'noise': 0.008, 'look_back': 179}. Best is trial 3 with value: 0.04172803209447328.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:48:25,883] Trial 5 finished with value: 0.057619263354672795 and parameters: {'n_reservoir': 91, 'sparsity': 0.4, 'spectral_radius': 0.9, 'noise': 0.007, 'look_back': 64}. Best is trial 3 with value: 0.04172803209447328.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.009623427275122989\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006500687659302715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:48:33,872] Trial 6 finished with value: 0.056803538988888555 and parameters: {'n_reservoir': 525, 'sparsity': 0.4, 'spectral_radius': 0.4, 'noise': 0.004, 'look_back': 462}. Best is trial 3 with value: 0.04172803209447328.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005397013926016988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:48:40,705] Trial 7 finished with value: 0.04252884999538853 and parameters: {'n_reservoir': 679, 'sparsity': 0.2, 'spectral_radius': 0.2, 'noise': 0.005, 'look_back': 148}. Best is trial 3 with value: 0.04172803209447328.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:48:41,723] Trial 8 finished with value: 0.04033568167770203 and parameters: {'n_reservoir': 121, 'sparsity': 0.4, 'spectral_radius': 0.5, 'noise': 0.008, 'look_back': 42}. Best is trial 8 with value: 0.04033568167770203.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007049826085257044\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:48:45,526] Trial 9 finished with value: 0.2577293260943143 and parameters: {'n_reservoir': 319, 'sparsity': 0.4, 'spectral_radius': 0.2, 'noise': 0.004, 'look_back': 362}. Best is trial 8 with value: 0.04033568167770203.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006913345569336609\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:48:46,432] Trial 10 finished with value: 2.124378735874487 and parameters: {'n_reservoir': 20, 'sparsity': 0.01, 'spectral_radius': 0.5, 'noise': 0.003, 'look_back': 268}. Best is trial 8 with value: 0.04033568167770203.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.026468770631945936\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:48:49,959] Trial 11 finished with value: 0.04184909401316038 and parameters: {'n_reservoir': 367, 'sparsity': 0.5, 'spectral_radius': 0.7, 'noise': 0.009, 'look_back': 231}. Best is trial 8 with value: 0.04033568167770203.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006288029695931086\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:48:53,446] Trial 12 finished with value: 0.040694341816643644 and parameters: {'n_reservoir': 434, 'sparsity': 0.4, 'spectral_radius': 1, 'noise': 0.002, 'look_back': 15}. Best is trial 8 with value: 0.04033568167770203.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005643190152686488\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005437076091192739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:48:57,196] Trial 13 finished with value: 0.03979242766566336 and parameters: {'n_reservoir': 512, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.002, 'look_back': 35}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005477034015952869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:49:01,880] Trial 14 finished with value: 0.04096522430292669 and parameters: {'n_reservoir': 581, 'sparsity': 0.1, 'spectral_radius': 0.5, 'noise': 0.002, 'look_back': 81}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0055666554107789284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:49:11,341] Trial 15 finished with value: 0.048978503936032855 and parameters: {'n_reservoir': 717, 'sparsity': 0.1, 'spectral_radius': 0.1, 'noise': 0.006, 'look_back': 296}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.02240315128429972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:49:17,413] Trial 16 finished with value: 0.054431793877918314 and parameters: {'n_reservoir': 652, 'sparsity': 0.5, 'spectral_radius': 10.0, 'noise': 0.002, 'look_back': 65}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006272384966003315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:49:21,666] Trial 17 finished with value: 0.04660629228166506 and parameters: {'n_reservoir': 452, 'sparsity': 0.2, 'spectral_radius': 1.1, 'noise': 0.005, 'look_back': 213}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005885680402494757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:49:33,121] Trial 18 finished with value: 0.04875823061832389 and parameters: {'n_reservoir': 834, 'sparsity': 0.01, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 350}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:49:35,851] Trial 19 finished with value: 0.041487720583178775 and parameters: {'n_reservoir': 375, 'sparsity': 0.1, 'spectral_radius': 0.6, 'noise': 0.001, 'look_back': 100}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.0058213628676221235\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005477143703064449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:49:40,017] Trial 20 finished with value: 0.040480254491794924 and parameters: {'n_reservoir': 562, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.009, 'look_back': 37}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005441351888545451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:49:45,116] Trial 21 finished with value: 0.0402594372592882 and parameters: {'n_reservoir': 563, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.009, 'look_back': 41}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005400094346286514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:49:51,999] Trial 22 finished with value: 0.04313622110263737 and parameters: {'n_reservoir': 746, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.009, 'look_back': 122}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005334697190537723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:49:58,552] Trial 23 finished with value: 0.042120798211315 and parameters: {'n_reservoir': 603, 'sparsity': 0.1, 'spectral_radius': 0.5, 'noise': 0.006, 'look_back': 60}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006097155560275197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:02,763] Trial 24 finished with value: 0.04359060495086432 and parameters: {'n_reservoir': 485, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.009, 'look_back': 187}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.027306616776667403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:04,960] Trial 25 finished with value: 0.0461271992340117 and parameters: {'n_reservoir': 310, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.008, 'look_back': 42}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:08,884] Trial 26 finished with value: 0.04126122044759005 and parameters: {'n_reservoir': 413, 'sparsity': 0.2, 'spectral_radius': 0.6, 'noise': 0.002, 'look_back': 110}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005913381966627673\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005450229180965951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:15,292] Trial 27 finished with value: 0.04101540047146776 and parameters: {'n_reservoir': 767, 'sparsity': 0.3, 'spectral_radius': 0.8, 'noise': 0.009, 'look_back': 13}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:16,476] Trial 28 finished with value: 0.043156361121855426 and parameters: {'n_reservoir': 125, 'sparsity': 0.5, 'spectral_radius': 0.4, 'noise': 0.008, 'look_back': 138}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008672541318359654\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:19,139] Trial 29 finished with value: 0.042244644891665156 and parameters: {'n_reservoir': 230, 'sparsity': 0.4, 'spectral_radius': 1.25, 'noise': 0.001, 'look_back': 104}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006994084553112596\n",
            "harvesting states...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:19,574] Trial 30 finished with value: 0.11869427814830692 and parameters: {'n_reservoir': 16, 'sparsity': 0.1, 'spectral_radius': 0.1, 'noise': 0.002, 'look_back': 80}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fitting...\n",
            "training error:\n",
            "0.02105029786895456\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005416937789134622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:23,782] Trial 31 finished with value: 0.04038689244670715 and parameters: {'n_reservoir': 565, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.009, 'look_back': 41}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005491809516161069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:27,565] Trial 32 finished with value: 0.040953931090456974 and parameters: {'n_reservoir': 523, 'sparsity': 0.1, 'spectral_radius': 0.3, 'noise': 0.009, 'look_back': 41}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005380687204401143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:32,839] Trial 33 finished with value: 0.04139012125057739 and parameters: {'n_reservoir': 589, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.009, 'look_back': 37}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005358729841851018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:38,561] Trial 34 finished with value: 0.042435544204197234 and parameters: {'n_reservoir': 654, 'sparsity': 0.3, 'spectral_radius': 1, 'noise': 0.007, 'look_back': 87}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005861767318406057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:43,480] Trial 35 finished with value: 0.04525272120590272 and parameters: {'n_reservoir': 484, 'sparsity': 0.1, 'spectral_radius': 1.1, 'noise': 0.008, 'look_back': 170}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:44,585] Trial 36 finished with value: 0.04404304091420677 and parameters: {'n_reservoir': 175, 'sparsity': 0.4, 'spectral_radius': 0.9, 'noise': 0.008, 'look_back': 14}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006465675265395983\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0052266298948210055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:54,923] Trial 37 finished with value: 0.04465936719163528 and parameters: {'n_reservoir': 928, 'sparsity': 0.3, 'spectral_radius': 1.25, 'noise': 0.009, 'look_back': 124}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:50:57,259] Trial 38 finished with value: 0.04011862161761351 and parameters: {'n_reservoir': 310, 'sparsity': 0.1, 'spectral_radius': 0.5, 'noise': 0.004, 'look_back': 60}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005718219566649715\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:01,279] Trial 39 finished with value: 1.630540198623622 and parameters: {'n_reservoir': 267, 'sparsity': 0.4, 'spectral_radius': 0.5, 'noise': 0.004, 'look_back': 485}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.00929727487399635\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:02,432] Trial 40 finished with value: 0.04333338353770315 and parameters: {'n_reservoir': 164, 'sparsity': 0.2, 'spectral_radius': 0.5, 'noise': 0.004, 'look_back': 62}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.008059380711461651\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.009765512539380362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:03,037] Trial 41 finished with value: 0.04612176743976421 and parameters: {'n_reservoir': 87, 'sparsity': 0.1, 'spectral_radius': 0.5, 'noise': 0.004, 'look_back': 47}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:08,615] Trial 42 finished with value: 0.0478833474455065 and parameters: {'n_reservoir': 373, 'sparsity': 0.1, 'spectral_radius': 0.3, 'noise': 0.005, 'look_back': 428}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.007500718094204089\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005515178120627626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:12,890] Trial 43 finished with value: 0.04082476664328705 and parameters: {'n_reservoir': 540, 'sparsity': 0.1, 'spectral_radius': 0.7, 'noise': 0.007, 'look_back': 89}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:15,984] Trial 44 finished with value: 0.039843252549817534 and parameters: {'n_reservoir': 465, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 27}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005568357676123278\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:17,762] Trial 45 finished with value: 0.04161454717810819 and parameters: {'n_reservoir': 285, 'sparsity': 0.4, 'spectral_radius': 0.2, 'noise': 0.003, 'look_back': 13}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005751582471017734\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:21,182] Trial 46 finished with value: 0.04273854258923561 and parameters: {'n_reservoir': 349, 'sparsity': 0.5, 'spectral_radius': 0.8, 'noise': 0.003, 'look_back': 70}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005695396351288696\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:24,467] Trial 47 finished with value: 0.041480955881203666 and parameters: {'n_reservoir': 411, 'sparsity': 0.01, 'spectral_radius': 0.5, 'noise': 0.003, 'look_back': 142}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.00573051975319264\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:27,545] Trial 48 finished with value: 0.04049009755226563 and parameters: {'n_reservoir': 463, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.004, 'look_back': 26}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005569320539357503\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:28,983] Trial 49 finished with value: 0.042252478300322846 and parameters: {'n_reservoir': 210, 'sparsity': 0.4, 'spectral_radius': 0.9, 'noise': 0.002, 'look_back': 59}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.00633506987331248\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:30,082] Trial 50 finished with value: 0.20312491974175584 and parameters: {'n_reservoir': 56, 'sparsity': 0.1, 'spectral_radius': 0.4, 'noise': 0.006, 'look_back': 203}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.012811698811217926\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005486501874865588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:35,539] Trial 51 finished with value: 0.040101278889378165 and parameters: {'n_reservoir': 620, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 31}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005364019291261516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:41,095] Trial 52 finished with value: 0.04153047265863128 and parameters: {'n_reservoir': 647, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 87}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005371369740988496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:46,570] Trial 53 finished with value: 0.04165726555987266 and parameters: {'n_reservoir': 608, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 27}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005288773418287712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:52,661] Trial 54 finished with value: 0.04044011712162595 and parameters: {'n_reservoir': 704, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 57}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0062606421549054716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:51:58,847] Trial 55 finished with value: 0.04949590829806904 and parameters: {'n_reservoir': 501, 'sparsity': 0.1, 'spectral_radius': 0.5, 'noise': 0.008, 'look_back': 298}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.022387084065626655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:03,564] Trial 56 finished with value: 0.053842003490395696 and parameters: {'n_reservoir': 625, 'sparsity': 0.1, 'spectral_radius': 10.0, 'noise': 0.001, 'look_back': 28}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005159670668606144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:11,792] Trial 57 finished with value: 0.042324277225408484 and parameters: {'n_reservoir': 806, 'sparsity': 0.2, 'spectral_radius': 0.1, 'noise': 0.005, 'look_back': 76}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:15,237] Trial 58 finished with value: 0.04091491686077015 and parameters: {'n_reservoir': 438, 'sparsity': 0.01, 'spectral_radius': 1, 'noise': 0.002, 'look_back': 123}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005845310787675833\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0052808907622086294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:22,076] Trial 59 finished with value: 0.04189812098456052 and parameters: {'n_reservoir': 683, 'sparsity': 0.5, 'spectral_radius': 0.6, 'noise': 0.003, 'look_back': 99}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:24,832] Trial 60 finished with value: 0.05065802248067508 and parameters: {'n_reservoir': 330, 'sparsity': 0.4, 'spectral_radius': 0.7, 'noise': 0.004, 'look_back': 163}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.006452375753668527\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0053691205804621035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:29,028] Trial 61 finished with value: 0.04080765060117329 and parameters: {'n_reservoir': 554, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.009, 'look_back': 47}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005790190408460324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:34,090] Trial 62 finished with value: 0.04297126272009595 and parameters: {'n_reservoir': 551, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.009, 'look_back': 10}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:36,809] Trial 63 finished with value: 0.04008200848268175 and parameters: {'n_reservoir': 408, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.008, 'look_back': 51}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005635331522688807\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:39,327] Trial 64 finished with value: 0.040625737901489156 and parameters: {'n_reservoir': 397, 'sparsity': 0.1, 'spectral_radius': 1.1, 'noise': 0.008, 'look_back': 27}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005649655982104455\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005467157212339951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:42,660] Trial 65 finished with value: 0.044622819422657584 and parameters: {'n_reservoir': 471, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.008, 'look_back': 53}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005403323822918008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:47,594] Trial 66 finished with value: 0.04173893077285454 and parameters: {'n_reservoir': 520, 'sparsity': 0.1, 'spectral_radius': 0.2, 'noise': 0.008, 'look_back': 77}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0061623362320054964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:51,976] Trial 67 finished with value: 0.04463790869758138 and parameters: {'n_reservoir': 441, 'sparsity': 0.3, 'spectral_radius': 0.5, 'noise': 0.006, 'look_back': 262}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:55,025] Trial 68 finished with value: 0.04118894982659098 and parameters: {'n_reservoir': 398, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.002, 'look_back': 108}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005929567098361958\n",
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:52:58,024] Trial 69 finished with value: 0.04055959778259807 and parameters: {'n_reservoir': 293, 'sparsity': 0.1, 'spectral_radius': 1.25, 'noise': 0.007, 'look_back': 28}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005846765451985455\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005454132514553163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:53:01,722] Trial 70 finished with value: 0.04543936983923933 and parameters: {'n_reservoir': 501, 'sparsity': 0.4, 'spectral_radius': 0.8, 'noise': 0.008, 'look_back': 66}. Best is trial 13 with value: 0.03979242766566336.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005362055569574856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:53:06,223] Trial 71 finished with value: 0.03917062766755079 and parameters: {'n_reservoir': 590, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.009, 'look_back': 43}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005423638230111625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:53:11,552] Trial 72 finished with value: 0.039507456903469115 and parameters: {'n_reservoir': 600, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.001, 'look_back': 41}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005356144856221732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:53:16,205] Trial 73 finished with value: 0.04106844021993599 and parameters: {'n_reservoir': 595, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.001, 'look_back': 48}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.00535245921482321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:53:21,976] Trial 74 finished with value: 0.04102261033774683 and parameters: {'n_reservoir': 636, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.001, 'look_back': 32}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005304333832796805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:53:27,741] Trial 75 finished with value: 0.040029259586296134 and parameters: {'n_reservoir': 672, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.001, 'look_back': 98}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005253699164823894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:53:34,822] Trial 76 finished with value: 0.040189840587272825 and parameters: {'n_reservoir': 733, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.001, 'look_back': 74}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005366282209101111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:53:40,825] Trial 77 finished with value: 0.04099944730083764 and parameters: {'n_reservoir': 685, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.001, 'look_back': 97}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005468145415923295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:53:46,157] Trial 78 finished with value: 0.04013821547053497 and parameters: {'n_reservoir': 614, 'sparsity': 0.1, 'spectral_radius': 0.3, 'noise': 0.001, 'look_back': 20}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005280638486142445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:53:51,765] Trial 79 finished with value: 0.03972766807526814 and parameters: {'n_reservoir': 674, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.001, 'look_back': 56}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005751313346189873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:54:02,430] Trial 80 finished with value: 0.05176327965632937 and parameters: {'n_reservoir': 779, 'sparsity': 0.2, 'spectral_radius': 1, 'noise': 0.001, 'look_back': 351}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005334514324656209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:54:08,848] Trial 81 finished with value: 0.040459709504750405 and parameters: {'n_reservoir': 669, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.001, 'look_back': 53}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005400088977836708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:54:13,983] Trial 82 finished with value: 0.040359025089867516 and parameters: {'n_reservoir': 573, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.001, 'look_back': 65}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005281228379654817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:54:19,970] Trial 83 finished with value: 0.04074609105913106 and parameters: {'n_reservoir': 717, 'sparsity': 0.1, 'spectral_radius': 0.4, 'noise': 0.004, 'look_back': 41}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0055441027822185724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:54:25,178] Trial 84 finished with value: 0.041029841758520195 and parameters: {'n_reservoir': 533, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 89}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005843265833642427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:54:31,214] Trial 85 finished with value: 0.04425097390780565 and parameters: {'n_reservoir': 590, 'sparsity': 0.1, 'spectral_radius': 0.9, 'noise': 0.001, 'look_back': 242}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.022648718074024722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:54:37,163] Trial 86 finished with value: 0.05495348111819777 and parameters: {'n_reservoir': 658, 'sparsity': 0.01, 'spectral_radius': 10.0, 'noise': 0.002, 'look_back': 20}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005326025687097427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:54:44,481] Trial 87 finished with value: 0.04226203498418294 and parameters: {'n_reservoir': 754, 'sparsity': 0.5, 'spectral_radius': 1, 'noise': 0.003, 'look_back': 130}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006352667797544039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:54:54,678] Trial 88 finished with value: 0.04738920269362766 and parameters: {'n_reservoir': 701, 'sparsity': 0.3, 'spectral_radius': 0.6, 'noise': 0.005, 'look_back': 428}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005352641321919106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:55:00,386] Trial 89 finished with value: 0.041110906901357554 and parameters: {'n_reservoir': 623, 'sparsity': 0.1, 'spectral_radius': 0.1, 'noise': 0.001, 'look_back': 34}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:55:03,886] Trial 90 finished with value: 0.04480128572450068 and parameters: {'n_reservoir': 482, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.004, 'look_back': 67}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.0054821782775861605\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005541592826708201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:55:08,477] Trial 91 finished with value: 0.04093961910036959 and parameters: {'n_reservoir': 616, 'sparsity': 0.1, 'spectral_radius': 1.1, 'noise': 0.001, 'look_back': 13}. Best is trial 71 with value: 0.03917062766755079.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0052852026198604764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:55:13,823] Trial 92 finished with value: 0.03893053466437693 and parameters: {'n_reservoir': 579, 'sparsity': 0.1, 'spectral_radius': 0.3, 'noise': 0.001, 'look_back': 52}. Best is trial 92 with value: 0.03893053466437693.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005250272688181556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:55:18,256] Trial 93 finished with value: 0.040064887922854196 and parameters: {'n_reservoir': 578, 'sparsity': 0.1, 'spectral_radius': 0.3, 'noise': 0.001, 'look_back': 55}. Best is trial 92 with value: 0.03893053466437693.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005354372138434908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:55:23,337] Trial 94 finished with value: 0.04220984357614339 and parameters: {'n_reservoir': 577, 'sparsity': 0.1, 'spectral_radius': 0.3, 'noise': 0.001, 'look_back': 40}. Best is trial 92 with value: 0.03893053466437693.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005387787853933797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:55:27,201] Trial 95 finished with value: 0.03945247107757393 and parameters: {'n_reservoir': 525, 'sparsity': 0.1, 'spectral_radius': 1, 'noise': 0.001, 'look_back': 53}. Best is trial 92 with value: 0.03893053466437693.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005425085322244861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:55:31,314] Trial 96 finished with value: 0.040660431694076196 and parameters: {'n_reservoir': 518, 'sparsity': 0.1, 'spectral_radius': 0.3, 'noise': 0.001, 'look_back': 83}. Best is trial 92 with value: 0.03893053466437693.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005542204430498206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:55:36,541] Trial 97 finished with value: 0.041848047071567855 and parameters: {'n_reservoir': 548, 'sparsity': 0.1, 'spectral_radius': 0.3, 'noise': 0.001, 'look_back': 112}. Best is trial 92 with value: 0.03893053466437693.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005333974368127683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:55:40,363] Trial 98 finished with value: 0.040548108578194304 and parameters: {'n_reservoir': 511, 'sparsity': 0.1, 'spectral_radius': 0.3, 'noise': 0.001, 'look_back': 56}. Best is trial 92 with value: 0.03893053466437693.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harvesting states...\n",
            "fitting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-09-05 18:55:43,783] Trial 99 finished with value: 0.04060653331963552 and parameters: {'n_reservoir': 460, 'sparsity': 0.1, 'spectral_radius': 0.7, 'noise': 0.001, 'look_back': 96}. Best is trial 92 with value: 0.03893053466437693.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training error:\n",
            "0.005566443565576262\n",
            "done\n",
            "ESN HyperParameters Tuning Elapsed Time : 466.52411 seconds\n",
            "Best Parameters:  {'n_reservoir': 579, 'sparsity': 0.1, 'spectral_radius': 0.3, 'noise': 0.001, 'look_back': 52}\n",
            "Best Loss:  0.03893053466437693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 2"
      ],
      "metadata": {
        "id": "XjtWLbfa7DRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "n_reservoir = 13\n",
        "sparsity = 0.01\n",
        "spectral_radius = 10.0\n",
        "noise = 0.006\n",
        "look_back = 328"
      ],
      "metadata": {
        "id": "4qZcDBSJ7Cbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 2\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# Build and fit the ESN model\n",
        "\n",
        "model = ESN(n_inputs = look_back,\n",
        "            n_outputs = horizon,\n",
        "            n_reservoir = n_reservoir,\n",
        "            sparsity=sparsity,\n",
        "            random_state=1234,\n",
        "            spectral_radius=spectral_radius,\n",
        "            noise = noise,\n",
        "            silent = False,\n",
        "            teacher_scaling = 10,\n",
        "            )\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "pred_train = model.fit(trainX, trainY)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "# calculate root mean squared error\n",
        "ESN_RMSE = np.sqrt(mean_squared_error(testY, predictions))\n",
        "ESN_MAE = mean_absolute_error(testY, predictions)\n",
        "\n",
        "print(\"ESN Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"ESN Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('ESN Test RMSE : %.5f' % (ESN_RMSE))\n",
        "print('ESN Test MAE : %.5f' % (ESN_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8W6Lqmgkhad",
        "outputId": "5c42ba05-cece-44d8-c58c-19e53390cf9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.018728522009361006\n",
            "ESN Model Training Elapsed Time : 1.74218 seconds\n",
            "ESN Model Generate predictions Elapsed Time : 0.07664 seconds\n",
            "ESN Test RMSE : 0.01939\n",
            "ESN Test MAE : 0.01596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 4"
      ],
      "metadata": {
        "id": "n-Sed4pf8DQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_reservoir = 11\n",
        "sparsity = 0.01\n",
        "spectral_radius = 10.0\n",
        "noise = 0.001\n",
        "look_back = 340"
      ],
      "metadata": {
        "id": "qfB_K4EG7-vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 4\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# Build and fit the ESN model\n",
        "\n",
        "model = ESN(n_inputs = look_back,\n",
        "            n_outputs = horizon,\n",
        "            n_reservoir = n_reservoir,\n",
        "            sparsity=sparsity,\n",
        "            random_state=1234,\n",
        "            spectral_radius=spectral_radius,\n",
        "            noise = noise,\n",
        "            silent = False,\n",
        "            teacher_scaling = 10,\n",
        "            )\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "pred_train = model.fit(trainX, trainY)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "# calculate root mean squared error\n",
        "ESN_RMSE = np.sqrt(mean_squared_error(testY, predictions))\n",
        "ESN_MAE = mean_absolute_error(testY, predictions)\n",
        "\n",
        "print(\"ESN Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"ESN Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('ESN Test RMSE : %.5f' % (ESN_RMSE))\n",
        "print('ESN Test MAE : %.5f' % (ESN_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCxXcQfc8T06",
        "outputId": "89d72e65-7f27-4252-870f-0c006ab79967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.02414602751770763\n",
            "ESN Model Training Elapsed Time : 1.71317 seconds\n",
            "ESN Model Generate predictions Elapsed Time : 0.09333 seconds\n",
            "ESN Test RMSE : 0.02516\n",
            "ESN Test MAE : 0.01997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 6"
      ],
      "metadata": {
        "id": "MPBj7ZaK8c89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_reservoir = 485\n",
        "sparsity = 0.3\n",
        "spectral_radius = 0.5\n",
        "noise = 0.001\n",
        "look_back = 71\n"
      ],
      "metadata": {
        "id": "USYj4IpH8XZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 6\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# Build and fit the ESN model\n",
        "\n",
        "model = ESN(n_inputs = look_back,\n",
        "            n_outputs = horizon,\n",
        "            n_reservoir = n_reservoir,\n",
        "            sparsity=sparsity,\n",
        "            random_state=1234,\n",
        "            spectral_radius=spectral_radius,\n",
        "            noise = noise,\n",
        "            silent = False,\n",
        "            teacher_scaling = 10,\n",
        "            )\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "pred_train = model.fit(trainX, trainY)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "# calculate root mean squared error\n",
        "ESN_RMSE = np.sqrt(mean_squared_error(testY, predictions))\n",
        "ESN_MAE = mean_absolute_error(testY, predictions)\n",
        "\n",
        "print(\"ESN Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"ESN Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('ESN Test RMSE : %.5f' % (ESN_RMSE))\n",
        "print('ESN Test MAE : %.5f' % (ESN_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDQlHGR28m01",
        "outputId": "a612367f-7153-4179-a61c-ef37dc254991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.006551621192409506\n",
            "ESN Model Training Elapsed Time : 7.33919 seconds\n",
            "ESN Model Generate predictions Elapsed Time : 0.38094 seconds\n",
            "ESN Test RMSE : 0.03080\n",
            "ESN Test MAE : 0.02410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 8"
      ],
      "metadata": {
        "id": "96VJLPs78uQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_reservoir = 981\n",
        "sparsity = 0.1\n",
        "spectral_radius = 1\n",
        "noise = 0.003\n",
        "look_back = 39\n"
      ],
      "metadata": {
        "id": "g1kh3Kp18p90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 8\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# Build and fit the ESN model\n",
        "\n",
        "model = ESN(n_inputs = look_back,\n",
        "            n_outputs = horizon,\n",
        "            n_reservoir = n_reservoir,\n",
        "            sparsity=sparsity,\n",
        "            random_state=1234,\n",
        "            spectral_radius=spectral_radius,\n",
        "            noise = noise,\n",
        "            silent = False,\n",
        "            teacher_scaling = 10,\n",
        "            )\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "pred_train = model.fit(trainX, trainY)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "# calculate root mean squared error\n",
        "ESN_RMSE = np.sqrt(mean_squared_error(testY, predictions))\n",
        "ESN_MAE = mean_absolute_error(testY, predictions)\n",
        "\n",
        "print(\"ESN Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"ESN Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('ESN Test RMSE : %.5f' % (ESN_RMSE))\n",
        "print('ESN Test MAE : %.5f' % (ESN_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIzqIPIs84OD",
        "outputId": "49ab91d8-20e8-4a90-9a77-b7a011ff83fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.005699949050913774\n",
            "ESN Model Training Elapsed Time : 16.30125 seconds\n",
            "ESN Model Generate predictions Elapsed Time : 1.09223 seconds\n",
            "ESN Test RMSE : 0.03520\n",
            "ESN Test MAE : 0.02723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Horizon = 10"
      ],
      "metadata": {
        "id": "WSrRVc929AeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_reservoir = 579\n",
        "sparsity = 0.1\n",
        "spectral_radius = 0.3\n",
        "noise = 0.001\n",
        "look_back = 52"
      ],
      "metadata": {
        "id": "CKgcT56F863o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 10\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, horizon)\n",
        "testX, testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "# Build and fit the ESN model\n",
        "\n",
        "model = ESN(n_inputs = look_back,\n",
        "            n_outputs = horizon,\n",
        "            n_reservoir = n_reservoir,\n",
        "            sparsity=sparsity,\n",
        "            random_state=1234,\n",
        "            spectral_radius=spectral_radius,\n",
        "            noise = noise,\n",
        "            silent = False,\n",
        "            teacher_scaling = 10,\n",
        "            )\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "pred_train = model.fit(trainX, trainY)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "# Record the starting time to generate predictions\n",
        "predictions_start_time = time.time()\n",
        "\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "# Record the ending time of generating predictions\n",
        "predictions_end_time = time.time()\n",
        "predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "# calculate root mean squared error\n",
        "ESN_RMSE = np.sqrt(mean_squared_error(testY, predictions))\n",
        "ESN_MAE = mean_absolute_error(testY, predictions)\n",
        "\n",
        "print(\"ESN Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "print(\"ESN Model Generate predictions Elapsed Time : %.5f\" % (predictions_elapsed_time), \"seconds\")\n",
        "print('ESN Test RMSE : %.5f' % (ESN_RMSE))\n",
        "print('ESN Test MAE : %.5f' % (ESN_MAE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYZHfrpA9KSV",
        "outputId": "4298d383-3ce6-4f18-a388-84f722410238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "harvesting states...\n",
            "fitting...\n",
            "training error:\n",
            "0.0052852026198604764\n",
            "ESN Model Training Elapsed Time : 7.05726 seconds\n",
            "ESN Model Generate predictions Elapsed Time : 0.42938 seconds\n",
            "ESN Test RMSE : 0.03893\n",
            "ESN Test MAE : 0.03022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5w-s4gq9NjF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}