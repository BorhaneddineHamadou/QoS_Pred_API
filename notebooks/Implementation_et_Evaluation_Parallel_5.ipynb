{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6f0_kMgJ8xT"
      },
      "outputs": [],
      "source": [
        "# Importing functions and classes we'll use\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dropout, Flatten\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from warnings import catch_warnings\n",
        "from warnings import filterwarnings\n",
        "from tqdm import tqdm\n",
        "import keras\n",
        "import sys\n",
        "import scipy.stats\n",
        "import json\n",
        "import numpy.fft\n",
        "import time\n",
        "from decimal import Decimal\n",
        "import math\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "tf.random.set_seed(1234)"
      ],
      "metadata": {
        "id": "mcc1V9_hKCLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/sample_data/new_dataset.xlsx')\n",
        "node1_delay = df[['node1_delay']]\n",
        "\n",
        "dataset = node1_delay.values"
      ],
      "metadata": {
        "id": "Zo5hl8dPKEEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p6bkZ6qyKI2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MA Filter Implementation"
      ],
      "metadata": {
        "id": "axWor-mx-rMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ASAP\n",
        "class Metrics(object):\n",
        "    def __init__(self, values):\n",
        "        self.set_values( values )\n",
        "\n",
        "    def set_values(self, values):\n",
        "        self.values = values\n",
        "        self.r = self.k = None\n",
        "\n",
        "    @property\n",
        "    def kurtosis(self):\n",
        "        if self.k is None:\n",
        "            self.k = scipy.stats.kurtosis(self.values)\n",
        "        return self.k\n",
        "\n",
        "    @property\n",
        "    def roughness(self):\n",
        "        if self.r is None:\n",
        "            self.r = np.std(np.diff(self.values))\n",
        "        return self.r\n",
        "\n",
        "class ACF(Metrics):\n",
        "    CORR_THRESH = 0.2\n",
        "    def __init__(self, values, max_lag=None):\n",
        "        super(ACF, self).__init__(values)\n",
        "        if max_lag is None:\n",
        "            max_lag = len(values) / 5\n",
        "        self.max_lag = int(max_lag)\n",
        "        self.max_acf = 0.0\n",
        "\n",
        "        # Calculate autocorrelation via FFT\n",
        "        # Demean\n",
        "        demeaned = values - np.mean(values)\n",
        "        # Pad data to power of 2\n",
        "        l = int(2.0 ** (int(math.log(len(demeaned),2.0)) + 1))\n",
        "        padded = np.append(demeaned, ([0.0] * (l - len(demeaned))))\n",
        "        # FFT and inverse FFT\n",
        "        F_f = numpy.fft.fft( padded )\n",
        "        R_t = numpy.fft.ifft( F_f * np.conjugate(F_f) )\n",
        "        self.correlations = R_t[:int(max_lag)].real / R_t[0].real\n",
        "\n",
        "        # Find autocorrelation peaks\n",
        "        self.peaks = []\n",
        "        if len(self.correlations) >1 :\n",
        "            positive = self.correlations[1] > self.correlations[0]\n",
        "            max = 1\n",
        "            for i in range(2, len(self.correlations)):\n",
        "                if not positive and self.correlations[i] > self.correlations[i-1]:\n",
        "                    max = i\n",
        "                    positive = not positive\n",
        "                elif positive and self.correlations[i] > self.correlations[max]:\n",
        "                    max = i\n",
        "                elif positive and self.correlations[i] < self.correlations[i-1]:\n",
        "                    if max > 1 and self.correlations[max] > self.CORR_THRESH:\n",
        "                        self.peaks.append(max)\n",
        "                        if self.correlations[max] > self.max_acf:\n",
        "                            self.max_acf = self.correlations[max]\n",
        "                    positive = not positive\n",
        "        # If there is no autocorrelation peak within the MAX_WINDOW boundary,\n",
        "        # try windows from the largest to the smallest\n",
        "        if len(self.peaks) <= 1:\n",
        "            self.peaks = range(2, len(self.correlations))\n",
        "\n",
        "def moving_average(data, _range):\n",
        "    ret = np.cumsum(data)\n",
        "    ret[_range:] = ret[int(_range):] - ret[:-int(_range)]\n",
        "    return ret[int(_range) - 1:] / _range\n",
        "\n",
        "def SMA(data, _range, slide):\n",
        "    ret = moving_average(data, int(_range))[::int(slide)]\n",
        "    return list(ret)\n",
        "\n",
        "def binary_search(head,tail,data,min_obj,orig_kurt,window_size):\n",
        "    while head <= tail:\n",
        "        w = int(round((head + tail) / 2.0))\n",
        "        smoothed = SMA(data,w,1)\n",
        "        metrics  = Metrics(smoothed)\n",
        "        if metrics.kurtosis >= orig_kurt:\n",
        "            if metrics.roughness < min_obj:\n",
        "                window_size = w\n",
        "                min_obj = metrics.roughness\n",
        "            head = w + 1\n",
        "        else:\n",
        "            tail = w - 1\n",
        "    return window_size\n",
        "\n",
        "def smooth_ASAP(data, max_window=5, resolution=None):\n",
        "    data = np.array(data)\n",
        "    # Preaggregate according to resolution\n",
        "    slide_size = 1\n",
        "    window_size = 1\n",
        "    if resolution and len(data) >= 2 * resolution:\n",
        "        slide_size = len(data) / resolution\n",
        "        data = SMA(data, slide_size, slide_size)\n",
        "    acf         = ACF(data, max_lag=len(data) / max_window)\n",
        "    peaks       = acf.peaks\n",
        "    orig_kurt   = acf.kurtosis\n",
        "    min_obj     = acf.roughness\n",
        "    lb          = 1\n",
        "    largest_feasible = -1\n",
        "    tail = len(data) / max_window\n",
        "    for i in range(len(peaks) - 1, -1, -1):\n",
        "        w = peaks[i]\n",
        "\n",
        "        if w < lb or w == 1:\n",
        "            break\n",
        "        elif math.sqrt(1 - acf.correlations[w]) * window_size > math.sqrt(1 - acf.correlations[window_size]) * w:\n",
        "            continue\n",
        "\n",
        "        smoothed = SMA(data, w, 1)\n",
        "        metrics = Metrics(smoothed)\n",
        "        if metrics.roughness < min_obj and metrics.kurtosis >= orig_kurt:\n",
        "            min_obj = metrics.roughness\n",
        "            window_size = w\n",
        "            lb = round( max(w*math.sqrt( (acf.max_acf -1) / (acf.correlations[w]-1) ), lb) )\n",
        "    if largest_feasible > 0:\n",
        "        if largest_feasible < len(peaks) - 2:\n",
        "            tail = peaks[largest_feasible + 1]\n",
        "        lb = max(lb, peaks[largest_feasible] + 1)\n",
        "\n",
        "    window_size = binary_search(lb, tail, data, min_obj, orig_kurt, window_size)\n",
        "    return window_size, slide_size"
      ],
      "metadata": {
        "id": "zzZ-MWSI-hQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm to transfer data in tabular format"
      ],
      "metadata": {
        "id": "o15lRHTkVj8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multistep_dataset(data, n_input, n_out=1):\n",
        "    X, y = list(), list()\n",
        "    in_start = 0\n",
        "    # step over the entire history one time step at a time\n",
        "    for _ in range(len(data)):\n",
        "        # define the end of the input sequence\n",
        "        in_end = in_start + n_input\n",
        "        out_end = in_end + n_out\n",
        "        # ensure we have enough data for this instance\n",
        "        if out_end <= len(data):\n",
        "            x_input = data[in_start:in_end]\n",
        "            X.append(x_input)\n",
        "            y.append(data[in_end:out_end])\n",
        "        # move along one time step\n",
        "        in_start += 1\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "aknEDSo0VoTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ESN Model Library"
      ],
      "metadata": {
        "id": "REoUU7it1k9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_dimensions(s, targetlength):\n",
        "    \"\"\"checks the dimensionality of some numeric argument s, broadcasts it\n",
        "       to the specified length if possible.\n",
        "\n",
        "    Args:\n",
        "        s: None, scalar or 1D array\n",
        "        targetlength: expected length of s\n",
        "\n",
        "    Returns:\n",
        "        None if s is None, else numpy vector of length targetlength\n",
        "    \"\"\"\n",
        "    if s is not None:\n",
        "        s = np.array(s)\n",
        "        if s.ndim == 0:\n",
        "            s = np.array([s] * targetlength)\n",
        "        elif s.ndim == 1:\n",
        "            if not len(s) == targetlength:\n",
        "                raise ValueError(\"arg must have length \" + str(targetlength))\n",
        "        else:\n",
        "            raise ValueError(\"Invalid argument\")\n",
        "    return s\n",
        "\n",
        "\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "class ESN():\n",
        "\n",
        "    def __init__(self, n_inputs, n_outputs, n_reservoir=200,\n",
        "                 spectral_radius=0.95, sparsity=0, noise=0.001, input_shift=None,\n",
        "                 input_scaling=None, teacher_forcing=True, feedback_scaling=None,\n",
        "                 teacher_scaling=None, teacher_shift=None,\n",
        "                 out_activation=identity, inverse_out_activation=identity,\n",
        "                 random_state=None, silent=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_inputs: nr of input dimensions\n",
        "            n_outputs: nr of output dimensions\n",
        "            n_reservoir: nr of reservoir neurons\n",
        "            spectral_radius: spectral radius of the recurrent weight matrix\n",
        "            sparsity: proportion of recurrent weights set to zero\n",
        "            noise: noise added to each neuron (regularization)\n",
        "            input_shift: scalar or vector of length n_inputs to add to each\n",
        "                        input dimension before feeding it to the network.\n",
        "            input_scaling: scalar or vector of length n_inputs to multiply\n",
        "                        with each input dimension before feeding it to the netw.\n",
        "            teacher_forcing: if True, feed the target back into output units\n",
        "            teacher_scaling: factor applied to the target signal\n",
        "            teacher_shift: additive term applied to the target signal\n",
        "            out_activation: output activation function (applied to the readout)\n",
        "            inverse_out_activation: inverse of the output activation function\n",
        "            random_state: positive integer seed, np.rand.RandomState object,\n",
        "                          or None to use numpy's builting RandomState.\n",
        "            silent: supress messages\n",
        "        \"\"\"\n",
        "        # check for proper dimensionality of all arguments and write them down.\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_reservoir = n_reservoir\n",
        "        self.n_outputs = n_outputs\n",
        "        self.spectral_radius = spectral_radius\n",
        "        self.sparsity = sparsity\n",
        "        self.noise = noise\n",
        "        self.input_shift = correct_dimensions(input_shift, n_inputs)\n",
        "        self.input_scaling = correct_dimensions(input_scaling, n_inputs)\n",
        "\n",
        "        self.teacher_scaling = teacher_scaling\n",
        "        self.teacher_shift = teacher_shift\n",
        "\n",
        "        self.out_activation = out_activation\n",
        "        self.inverse_out_activation = inverse_out_activation\n",
        "        self.random_state = random_state\n",
        "\n",
        "        # the given random_state might be either an actual RandomState object,\n",
        "        # a seed or None (in which case we use numpy's builtin RandomState)\n",
        "        if isinstance(random_state, np.random.RandomState):\n",
        "            self.random_state_ = random_state\n",
        "        elif random_state:\n",
        "            try:\n",
        "                self.random_state_ = np.random.RandomState(random_state)\n",
        "            except TypeError as e:\n",
        "                raise Exception(\"Invalid seed: \" + str(e))\n",
        "        else:\n",
        "            self.random_state_ = np.random.mtrand._rand\n",
        "\n",
        "        self.teacher_forcing = teacher_forcing\n",
        "        self.silent = silent\n",
        "        self.initweights()\n",
        "\n",
        "    def initweights(self):\n",
        "        # initialize recurrent weights:\n",
        "        # begin with a random matrix centered around zero:\n",
        "        W = self.random_state_.rand(self.n_reservoir, self.n_reservoir) - 0.5\n",
        "        # delete the fraction of connections given by (self.sparsity):\n",
        "        W[self.random_state_.rand(*W.shape) < self.sparsity] = 0\n",
        "        # compute the spectral radius of these weights:\n",
        "        radius = np.max(np.abs(np.linalg.eigvals(W)))\n",
        "        # rescale them to reach the requested spectral radius:\n",
        "        self.W = W * (self.spectral_radius / radius)\n",
        "\n",
        "        # random input weights:\n",
        "        self.W_in = self.random_state_.rand(\n",
        "            self.n_reservoir, self.n_inputs) * 2 - 1\n",
        "        # random feedback (teacher forcing) weights:\n",
        "        self.W_feedb = self.random_state_.rand(\n",
        "            self.n_reservoir, self.n_outputs) * 2 - 1\n",
        "\n",
        "    def _update(self, state, input_pattern, output_pattern):\n",
        "        \"\"\"performs one update step.\n",
        "\n",
        "        i.e., computes the next network state by applying the recurrent weights\n",
        "        to the last state & and feeding in the current input and output patterns\n",
        "        \"\"\"\n",
        "        if self.teacher_forcing:\n",
        "            preactivation = (np.dot(self.W, state)\n",
        "                             + np.dot(self.W_in, input_pattern)\n",
        "                             + np.dot(self.W_feedb, output_pattern))\n",
        "        else:\n",
        "            preactivation = (np.dot(self.W, state)\n",
        "                             + np.dot(self.W_in, input_pattern))\n",
        "        return (np.tanh(preactivation)\n",
        "                + self.noise * (self.random_state_.rand(self.n_reservoir) - 0.5))\n",
        "\n",
        "    def _scale_inputs(self, inputs):\n",
        "        \"\"\"for each input dimension j: multiplies by the j'th entry in the\n",
        "        input_scaling argument, then adds the j'th entry of the input_shift\n",
        "        argument.\"\"\"\n",
        "        if self.input_scaling is not None:\n",
        "            inputs = np.dot(inputs, np.diag(self.input_scaling))\n",
        "        if self.input_shift is not None:\n",
        "            inputs = inputs + self.input_shift\n",
        "        return inputs\n",
        "\n",
        "    def _scale_teacher(self, teacher):\n",
        "        \"\"\"multiplies the teacher/target signal by the teacher_scaling argument,\n",
        "        then adds the teacher_shift argument to it.\"\"\"\n",
        "        if self.teacher_scaling is not None:\n",
        "            teacher = teacher * self.teacher_scaling\n",
        "        if self.teacher_shift is not None:\n",
        "            teacher = teacher + self.teacher_shift\n",
        "        return teacher\n",
        "\n",
        "    def _unscale_teacher(self, teacher_scaled):\n",
        "        \"\"\"inverse operation of the _scale_teacher method.\"\"\"\n",
        "        if self.teacher_shift is not None:\n",
        "            teacher_scaled = teacher_scaled - self.teacher_shift\n",
        "        if self.teacher_scaling is not None:\n",
        "            teacher_scaled = teacher_scaled / self.teacher_scaling\n",
        "        return teacher_scaled\n",
        "\n",
        "    def fit(self, inputs, outputs, inspect=False):\n",
        "        \"\"\"\n",
        "        Collect the network's reaction to training data, train readout weights.\n",
        "\n",
        "        Args:\n",
        "            inputs: array of dimensions (N_training_samples x n_inputs)\n",
        "            outputs: array of dimension (N_training_samples x n_outputs)\n",
        "            inspect: show a visualisation of the collected reservoir states\n",
        "\n",
        "        Returns:\n",
        "            the network's output on the training data, using the trained weights\n",
        "        \"\"\"\n",
        "        # transform any vectors of shape (x,) into vectors of shape (x,1):\n",
        "        if inputs.ndim < 2:\n",
        "            inputs = np.reshape(inputs, (len(inputs), -1))\n",
        "        if outputs.ndim < 2:\n",
        "            outputs = np.reshape(outputs, (len(outputs), -1))\n",
        "        # transform input and teacher signal:\n",
        "        inputs_scaled = self._scale_inputs(inputs)\n",
        "        teachers_scaled = self._scale_teacher(outputs)\n",
        "\n",
        "        if not self.silent:\n",
        "            print(\"harvesting states...\")\n",
        "        # step the reservoir through the given input,output pairs:\n",
        "        states = np.zeros((inputs.shape[0], self.n_reservoir))\n",
        "        for n in range(1, inputs.shape[0]):\n",
        "            states[n, :] = self._update(states[n - 1], inputs_scaled[n, :],\n",
        "                                        teachers_scaled[n - 1, :])\n",
        "\n",
        "        # learn the weights, i.e. find the linear combination of collected\n",
        "        # network states that is closest to the target output\n",
        "        if not self.silent:\n",
        "            print(\"fitting...\")\n",
        "        # we'll disregard the first few states:\n",
        "        transient = min(int(inputs.shape[1] / 10), 100)\n",
        "        # include the raw inputs:\n",
        "        extended_states = np.hstack((states, inputs_scaled))\n",
        "        # Solve for W_out:\n",
        "        self.W_out = np.dot(np.linalg.pinv(extended_states[transient:, :]),\n",
        "                            self.inverse_out_activation(teachers_scaled[transient:, :])).T\n",
        "\n",
        "        # remember the last state for later:\n",
        "        self.laststate = states[-1, :]\n",
        "        self.lastinput = inputs[-1, :]\n",
        "        self.lastoutput = teachers_scaled[-1, :]\n",
        "\n",
        "        # optionally visualize the collected states\n",
        "        if inspect:\n",
        "            from matplotlib import pyplot as plt\n",
        "            # (^-- we depend on matplotlib only if this option is used)\n",
        "            plt.figure(\n",
        "                figsize=(states.shape[0] * 0.0025, states.shape[1] * 0.01))\n",
        "            plt.imshow(extended_states.T, aspect='auto',\n",
        "                       interpolation='nearest')\n",
        "            plt.colorbar()\n",
        "\n",
        "        if not self.silent:\n",
        "            print(\"training error:\")\n",
        "        # apply learned weights to the collected states:\n",
        "        pred_train = self._unscale_teacher(self.out_activation(\n",
        "            np.dot(extended_states, self.W_out.T)))\n",
        "        if not self.silent:\n",
        "            print(np.sqrt(np.mean((pred_train - outputs)**2)))\n",
        "        return pred_train\n",
        "\n",
        "    def predict(self, inputs, continuation=True):\n",
        "        \"\"\"\n",
        "        Apply the learned weights to the network's reactions to new input.\n",
        "\n",
        "        Args:\n",
        "            inputs: array of dimensions (N_test_samples x n_inputs)\n",
        "            continuation: if True, start the network from the last training state\n",
        "\n",
        "        Returns:\n",
        "            Array of output activations\n",
        "        \"\"\"\n",
        "        if inputs.ndim < 2:\n",
        "            inputs = np.reshape(inputs, (len(inputs), -1))\n",
        "        n_samples = inputs.shape[0]\n",
        "\n",
        "        if continuation:\n",
        "            laststate = self.laststate\n",
        "            lastinput = self.lastinput\n",
        "            lastoutput = self.lastoutput\n",
        "        else:\n",
        "            laststate = np.zeros(self.n_reservoir)\n",
        "            lastinput = np.zeros(self.n_inputs)\n",
        "            lastoutput = np.zeros(self.n_outputs)\n",
        "\n",
        "        inputs = np.vstack([lastinput, self._scale_inputs(inputs)])\n",
        "        states = np.vstack(\n",
        "            [laststate, np.zeros((n_samples, self.n_reservoir))])\n",
        "        outputs = np.vstack(\n",
        "            [lastoutput, np.zeros((n_samples, self.n_outputs))])\n",
        "\n",
        "        for n in range(n_samples):\n",
        "            states[\n",
        "                n + 1, :] = self._update(states[n, :], inputs[n + 1, :], outputs[n, :])\n",
        "            outputs[n + 1, :] = self.out_activation(np.dot(self.W_out,\n",
        "                                                           np.concatenate([states[n + 1, :], inputs[n + 1, :]])))\n",
        "\n",
        "        return self._unscale_teacher(self.out_activation(outputs[1:]))"
      ],
      "metadata": {
        "id": "nZUjxupO1pYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to evaluate multi-step forecasting models"
      ],
      "metadata": {
        "id": "0Ce-fp-VQAlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def eval_model(Y_true, Y_pred):\n",
        "    scores = []\n",
        "\n",
        "    # Calculate scores for each value on the horizon\n",
        "    # For each of the col in Y_true we have corresponding y_pred\n",
        "    for i in range(Y_true.shape[1]):\n",
        "        mse = mean_squared_error(Y_true[:, i], Y_pred[:, i])\n",
        "        rmse = np.sqrt(mse)\n",
        "        scores.append(rmse)\n",
        "\n",
        "    # Calculate score for the whole prediction\n",
        "    total_score = 0\n",
        "    for row in range(Y_true.shape[0]):\n",
        "        for col in range(Y_true.shape[1]):\n",
        "            total_score += (Y_true[row, col] - Y_pred[row, col]) ** 2\n",
        "\n",
        "    total_score = np.sqrt(total_score / (Y_true.shape[0] * Y_true.shape[1]))\n",
        "    return total_score, scores"
      ],
      "metadata": {
        "id": "XNneOfmSUpD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_multistep_forecast(predictions, actual):\n",
        "    # Calculate RMSE and MAE for each prediction\n",
        "    rmse = np.sqrt(mean_squared_error(actual, predictions))\n",
        "    mae = mean_absolute_error(actual, predictions)\n",
        "\n",
        "    return rmse, mae"
      ],
      "metadata": {
        "id": "TO7-lU-cUpFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recursive Strategy"
      ],
      "metadata": {
        "id": "kb9oD1SzOkJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recursive_strategy(model, X_row, n_steps):\n",
        "    forecasts = []\n",
        "    shape_0 = X_row.shape[0]\n",
        "    shape_1 = X_row.shape[1]\n",
        "\n",
        "    for i in range(n_steps):\n",
        "        X_row = np.reshape(X_row, (shape_0, 1, shape_1))\n",
        "        forecast = model.predict(X_row, verbose=0)\n",
        "        X_row.reshape(X_row.shape[2],)\n",
        "        forecasts.append(forecast[0, 0])\n",
        "        X_row = X_row.tolist()\n",
        "        X_row[0][0].append(forecast[0, 0])\n",
        "        X_row = X_row[0][0][1:]\n",
        "        X_row = np.array(X_row)\n",
        "    return forecasts\n",
        "\n",
        "def make_predictions(model, X, n_steps):\n",
        "    predictions = []\n",
        "    for i in tqdm(range(len(X)), desc=\"Progress\"):\n",
        "      row_forecasts = recursive_strategy(model, X[i, :], n_steps)\n",
        "      predictions.append(row_forecasts)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "IZ2IdGeCdnnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Dataset : Node1 Delay"
      ],
      "metadata": {
        "id": "QZJA94T3OoUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Model"
      ],
      "metadata": {
        "id": "MRnOjx5HeIas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "look_back = 300 #200\n",
        "num_hidden_layers = 2\n",
        "learning_rate = 0.0015250869829489782\n",
        "batch_size = 44\n",
        "epochs = 82\n",
        "rnn_units_layer_0 = 15\n",
        "rnn_units_layer_1 = 32\n",
        "\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, 1)\n",
        "testX, testY = create_multistep_dataset(test, look_back, 1)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=rnn_units_layer_0, return_sequences=True))\n",
        "model.add(SimpleRNN(units=rnn_units_layer_1))\n",
        "model.add(Dense(1))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "print(\"RNN Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "\n",
        "horizons = [2, 4, 6, 8]\n",
        "\n",
        "for horizon in horizons:\n",
        "    print(f\"=================== horizon = {horizon}======================\")\n",
        "    # Record the starting time to generate predictions\n",
        "    predictions_start_time = time.time()\n",
        "\n",
        "    testPredict = make_predictions(model, testX, horizon)\n",
        "\n",
        "    # Record the ending time of generating predictions\n",
        "    predictions_end_time = time.time()\n",
        "    predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "\n",
        "    testPredict = np.array(testPredict)\n",
        "    _, new_testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "    testRMSE = np.sqrt(mean_squared_error(new_testY, testPredict[:len(new_testY), :]))\n",
        "    testMAE = mean_absolute_error(new_testY, testPredict[:len(new_testY), :])\n",
        "\n",
        "    print('RNN Test RMSE : %.5f' % (testRMSE))\n",
        "    print('RNN Test MAE : %.5f' % (testMAE))\n",
        "    print(\"RNN Elapsed Time To generate Predictions : %.5f\" % (predictions_elapsed_time), \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9SjvRbyh2S7",
        "outputId": "42367e64-27a3-4e68-f0b4-6c4a5a948d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "Epoch 1/82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "403/403 [==============================] - 7s 5ms/step - loss: 0.0040\n",
            "Epoch 2/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 0.0017\n",
            "Epoch 3/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 0.0010\n",
            "Epoch 4/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 8.8680e-04\n",
            "Epoch 5/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 7.8370e-04\n",
            "Epoch 6/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 7.5121e-04\n",
            "Epoch 7/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 6.1610e-04\n",
            "Epoch 8/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 6.1900e-04\n",
            "Epoch 9/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 6.2578e-04\n",
            "Epoch 10/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 6.0413e-04\n",
            "Epoch 11/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 5.5635e-04\n",
            "Epoch 12/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 5.2603e-04\n",
            "Epoch 13/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 5.0536e-04\n",
            "Epoch 14/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 5.1552e-04\n",
            "Epoch 15/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 5.2135e-04\n",
            "Epoch 16/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 5.3357e-04\n",
            "Epoch 17/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 5.5183e-04\n",
            "Epoch 18/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.8253e-04\n",
            "Epoch 19/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.9787e-04\n",
            "Epoch 20/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 5.1319e-04\n",
            "Epoch 21/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 4.8044e-04\n",
            "Epoch 22/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.8758e-04\n",
            "Epoch 23/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.8448e-04\n",
            "Epoch 24/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.8589e-04\n",
            "Epoch 25/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.7097e-04\n",
            "Epoch 26/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.6611e-04\n",
            "Epoch 27/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 4.6650e-04\n",
            "Epoch 28/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.6739e-04\n",
            "Epoch 29/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.2849e-04\n",
            "Epoch 30/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.7958e-04\n",
            "Epoch 31/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.7515e-04\n",
            "Epoch 32/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.5799e-04\n",
            "Epoch 33/82\n",
            "403/403 [==============================] - 3s 8ms/step - loss: 4.3031e-04\n",
            "Epoch 34/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.5367e-04\n",
            "Epoch 35/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.4471e-04\n",
            "Epoch 36/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.4351e-04\n",
            "Epoch 37/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.4875e-04\n",
            "Epoch 38/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 4.2915e-04\n",
            "Epoch 39/82\n",
            "403/403 [==============================] - 3s 6ms/step - loss: 4.3311e-04\n",
            "Epoch 40/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 4.2572e-04\n",
            "Epoch 41/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 4.1323e-04\n",
            "Epoch 42/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.4375e-04\n",
            "Epoch 43/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 4.2582e-04\n",
            "Epoch 44/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 3.9466e-04\n",
            "Epoch 45/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.2826e-04\n",
            "Epoch 46/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.9642e-04\n",
            "Epoch 47/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.1174e-04\n",
            "Epoch 48/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.2238e-04\n",
            "Epoch 49/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 4.1528e-04\n",
            "Epoch 50/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.9159e-04\n",
            "Epoch 51/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.1429e-04\n",
            "Epoch 52/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.2179e-04\n",
            "Epoch 53/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.1517e-04\n",
            "Epoch 54/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.0544e-04\n",
            "Epoch 55/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 3.8012e-04\n",
            "Epoch 56/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.0032e-04\n",
            "Epoch 57/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.9906e-04\n",
            "Epoch 58/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.0501e-04\n",
            "Epoch 59/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.2264e-04\n",
            "Epoch 60/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 3.8942e-04\n",
            "Epoch 61/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 3.9988e-04\n",
            "Epoch 62/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.7573e-04\n",
            "Epoch 63/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.9732e-04\n",
            "Epoch 64/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.0884e-04\n",
            "Epoch 65/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.9677e-04\n",
            "Epoch 66/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 4.0897e-04\n",
            "Epoch 67/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 3.7091e-04\n",
            "Epoch 68/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.1221e-04\n",
            "Epoch 69/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.0624e-04\n",
            "Epoch 70/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.7678e-04\n",
            "Epoch 71/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.8132e-04\n",
            "Epoch 72/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 3.8940e-04\n",
            "Epoch 73/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.7145e-04\n",
            "Epoch 74/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.7065e-04\n",
            "Epoch 75/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.7748e-04\n",
            "Epoch 76/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.6461e-04\n",
            "Epoch 77/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.7671e-04\n",
            "Epoch 78/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 3.9253e-04\n",
            "Epoch 79/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.8377e-04\n",
            "Epoch 80/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.0319e-04\n",
            "Epoch 81/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.6464e-04\n",
            "Epoch 82/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.8896e-04\n",
            "RNN Model Training Elapsed Time : 203.59897 seconds\n",
            "=================== horizon = 2======================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress: 100%|██████████| 1700/1700 [02:54<00:00,  9.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN Test RMSE : 0.02286\n",
            "RNN Test MAE : 0.01835\n",
            "RNN Elapsed Time To generate Predictions : 174.77102 seconds\n",
            "=================== horizon = 4======================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress: 100%|██████████| 1700/1700 [05:42<00:00,  4.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN Test RMSE : 0.02878\n",
            "RNN Test MAE : 0.02266\n",
            "RNN Elapsed Time To generate Predictions : 342.23202 seconds\n",
            "=================== horizon = 6======================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress: 100%|██████████| 1700/1700 [08:35<00:00,  3.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN Test RMSE : 0.03353\n",
            "RNN Test MAE : 0.02611\n",
            "RNN Elapsed Time To generate Predictions : 515.89146 seconds\n",
            "=================== horizon = 8======================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress: 100%|██████████| 1700/1700 [11:29<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN Test RMSE : 0.03770\n",
            "RNN Test MAE : 0.02916\n",
            "RNN Elapsed Time To generate Predictions : 689.07837 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Params\n",
        "look_back = 300 #200\n",
        "num_hidden_layers = 2\n",
        "learning_rate = 0.0015250869829489782\n",
        "batch_size = 44\n",
        "epochs = 82\n",
        "rnn_units_layer_0 = 15\n",
        "rnn_units_layer_1 = 32\n",
        "\n",
        "\n",
        "node1_delay = df[['node1_delay']]\n",
        "node1_delay_dataset = node1_delay.values\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "trainX, trainY = create_multistep_dataset(train, look_back, 1)\n",
        "testX, testY = create_multistep_dataset(test, look_back, 1)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "\n",
        "# Crée et entraîne le modèle pour l'horizon de prévision i\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=rnn_units_layer_0, return_sequences=True))\n",
        "model.add(SimpleRNN(units=rnn_units_layer_1))\n",
        "model.add(Dense(1))\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "# Record the starting time to training the model\n",
        "training_start_time = time.time()\n",
        "\n",
        "model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Record the ending time of training the model\n",
        "training_end_time = time.time()\n",
        "training_elapsed_time = training_end_time - training_start_time\n",
        "\n",
        "print(\"RNN Model Training Elapsed Time : %.5f\" % (training_elapsed_time), \"seconds\")\n",
        "\n",
        "horizons = [10]\n",
        "\n",
        "for horizon in horizons:\n",
        "    print(f\"=================== horizon = {horizon}======================\")\n",
        "    # Record the starting time to generate predictions\n",
        "    predictions_start_time = time.time()\n",
        "\n",
        "    testPredict = make_predictions(model, testX, horizon)\n",
        "\n",
        "    # Record the ending time of generating predictions\n",
        "    predictions_end_time = time.time()\n",
        "    predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "\n",
        "    testPredict = np.array(testPredict)\n",
        "    _, new_testY = create_multistep_dataset(test, look_back, horizon)\n",
        "\n",
        "    testRMSE = np.sqrt(mean_squared_error(new_testY, testPredict[:len(new_testY), :]))\n",
        "    testMAE = mean_absolute_error(new_testY, testPredict[:len(new_testY), :])\n",
        "\n",
        "    print('RNN Test RMSE : %.5f' % (testRMSE))\n",
        "    print('RNN Test MAE : %.5f' % (testMAE))\n",
        "    print(\"RNN Elapsed Time To generate Predictions : %.5f\" % (predictions_elapsed_time), \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAyWkLyNTeND",
        "outputId": "7724b7c0-1010-4924-ee9f-340af6f6c8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/82\n",
            "403/403 [==============================] - 5s 5ms/step - loss: 0.0037\n",
            "Epoch 2/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 0.0013\n",
            "Epoch 3/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 9.4036e-04\n",
            "Epoch 4/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 8.8412e-04\n",
            "Epoch 5/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 7.3693e-04\n",
            "Epoch 6/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 7.4868e-04\n",
            "Epoch 7/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 6.5978e-04\n",
            "Epoch 8/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 6.7927e-04\n",
            "Epoch 9/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 6.4661e-04\n",
            "Epoch 10/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 6.7353e-04\n",
            "Epoch 11/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 6.1222e-04\n",
            "Epoch 12/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 5.5861e-04\n",
            "Epoch 13/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 5.3850e-04\n",
            "Epoch 14/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 5.4840e-04\n",
            "Epoch 15/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 5.6161e-04\n",
            "Epoch 16/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 5.6133e-04\n",
            "Epoch 17/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 5.8987e-04\n",
            "Epoch 18/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.9497e-04\n",
            "Epoch 19/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 5.3374e-04\n",
            "Epoch 20/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 5.3329e-04\n",
            "Epoch 21/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 5.0916e-04\n",
            "Epoch 22/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.7153e-04\n",
            "Epoch 23/82\n",
            "403/403 [==============================] - 3s 8ms/step - loss: 4.7999e-04\n",
            "Epoch 24/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 5.0188e-04\n",
            "Epoch 25/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 5.0767e-04\n",
            "Epoch 26/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.8241e-04\n",
            "Epoch 27/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.8007e-04\n",
            "Epoch 28/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 4.6914e-04\n",
            "Epoch 29/82\n",
            "403/403 [==============================] - 3s 6ms/step - loss: 4.5134e-04\n",
            "Epoch 30/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.7843e-04\n",
            "Epoch 31/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.9001e-04\n",
            "Epoch 32/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.6382e-04\n",
            "Epoch 33/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.4361e-04\n",
            "Epoch 34/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 4.5529e-04\n",
            "Epoch 35/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.5357e-04\n",
            "Epoch 36/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.6680e-04\n",
            "Epoch 37/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.6577e-04\n",
            "Epoch 38/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.2758e-04\n",
            "Epoch 39/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 4.4876e-04\n",
            "Epoch 40/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 4.3061e-04\n",
            "Epoch 41/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.2217e-04\n",
            "Epoch 42/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.5276e-04\n",
            "Epoch 43/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.3277e-04\n",
            "Epoch 44/82\n",
            "403/403 [==============================] - 3s 6ms/step - loss: 4.1651e-04\n",
            "Epoch 45/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 4.3083e-04\n",
            "Epoch 46/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.1529e-04\n",
            "Epoch 47/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.2564e-04\n",
            "Epoch 48/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.2653e-04\n",
            "Epoch 49/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.3898e-04\n",
            "Epoch 50/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 3.9606e-04\n",
            "Epoch 51/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 4.1663e-04\n",
            "Epoch 52/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.5093e-04\n",
            "Epoch 53/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.1472e-04\n",
            "Epoch 54/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.1057e-04\n",
            "Epoch 55/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.8805e-04\n",
            "Epoch 56/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 3.9622e-04\n",
            "Epoch 57/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.0495e-04\n",
            "Epoch 58/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.1456e-04\n",
            "Epoch 59/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.2733e-04\n",
            "Epoch 60/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.9689e-04\n",
            "Epoch 61/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 3.9911e-04\n",
            "Epoch 62/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 3.9280e-04\n",
            "Epoch 63/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.9672e-04\n",
            "Epoch 64/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.2332e-04\n",
            "Epoch 65/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.9843e-04\n",
            "Epoch 66/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.0638e-04\n",
            "Epoch 67/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 3.7989e-04\n",
            "Epoch 68/82\n",
            "403/403 [==============================] - 3s 6ms/step - loss: 4.1884e-04\n",
            "Epoch 69/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 4.0615e-04\n",
            "Epoch 70/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.8153e-04\n",
            "Epoch 71/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.8630e-04\n",
            "Epoch 72/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.8848e-04\n",
            "Epoch 73/82\n",
            "403/403 [==============================] - 3s 7ms/step - loss: 3.8014e-04\n",
            "Epoch 74/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.7714e-04\n",
            "Epoch 75/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.7962e-04\n",
            "Epoch 76/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 3.6866e-04\n",
            "Epoch 77/82\n",
            "403/403 [==============================] - 3s 6ms/step - loss: 3.7584e-04\n",
            "Epoch 78/82\n",
            "403/403 [==============================] - 3s 6ms/step - loss: 4.0014e-04\n",
            "Epoch 79/82\n",
            "403/403 [==============================] - 2s 6ms/step - loss: 3.9600e-04\n",
            "Epoch 80/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.8759e-04\n",
            "Epoch 81/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.7272e-04\n",
            "Epoch 82/82\n",
            "403/403 [==============================] - 2s 5ms/step - loss: 3.9601e-04\n",
            "RNN Model Training Elapsed Time : 182.54361 seconds\n",
            "=================== horizon = 10======================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress: 100%|██████████| 1700/1700 [14:24<00:00,  1.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN Test RMSE : 0.04520\n",
            "RNN Test MAE : 0.03481\n",
            "RNN Elapsed Time To generate Predictions : 864.36111 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UgIeuqlkK9bt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}