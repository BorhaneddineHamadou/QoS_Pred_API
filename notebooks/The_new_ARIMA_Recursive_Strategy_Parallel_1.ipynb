{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfjPoYr3_QV7"
      },
      "outputs": [],
      "source": [
        "# Importing functions and classes we'll use\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dropout, Flatten\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from warnings import catch_warnings\n",
        "from warnings import filterwarnings\n",
        "from tqdm import tqdm\n",
        "import keras\n",
        "import sys\n",
        "import scipy.stats\n",
        "import json\n",
        "import numpy.fft\n",
        "import time\n",
        "from decimal import Decimal\n",
        "import math\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "tf.random.set_seed(1234)"
      ],
      "metadata": {
        "id": "P3wbJzbK_V1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/sample_data/new_dataset.xlsx')\n",
        "node1_delay = df[['node1_delay']]\n",
        "\n",
        "dataset = node1_delay.values"
      ],
      "metadata": {
        "id": "58v5hW2e_aTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YW5WuTQQ_cQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MA Filter Implementation"
      ],
      "metadata": {
        "id": "axWor-mx-rMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ASAP\n",
        "class Metrics(object):\n",
        "    def __init__(self, values):\n",
        "        self.set_values( values )\n",
        "\n",
        "    def set_values(self, values):\n",
        "        self.values = values\n",
        "        self.r = self.k = None\n",
        "\n",
        "    @property\n",
        "    def kurtosis(self):\n",
        "        if self.k is None:\n",
        "            self.k = scipy.stats.kurtosis(self.values)\n",
        "        return self.k\n",
        "\n",
        "    @property\n",
        "    def roughness(self):\n",
        "        if self.r is None:\n",
        "            self.r = np.std(np.diff(self.values))\n",
        "        return self.r\n",
        "\n",
        "class ACF(Metrics):\n",
        "    CORR_THRESH = 0.2\n",
        "    def __init__(self, values, max_lag=None):\n",
        "        super(ACF, self).__init__(values)\n",
        "        if max_lag is None:\n",
        "            max_lag = len(values) / 5\n",
        "        self.max_lag = int(max_lag)\n",
        "        self.max_acf = 0.0\n",
        "\n",
        "        # Calculate autocorrelation via FFT\n",
        "        # Demean\n",
        "        demeaned = values - np.mean(values)\n",
        "        # Pad data to power of 2\n",
        "        l = int(2.0 ** (int(math.log(len(demeaned),2.0)) + 1))\n",
        "        padded = np.append(demeaned, ([0.0] * (l - len(demeaned))))\n",
        "        # FFT and inverse FFT\n",
        "        F_f = numpy.fft.fft( padded )\n",
        "        R_t = numpy.fft.ifft( F_f * np.conjugate(F_f) )\n",
        "        self.correlations = R_t[:int(max_lag)].real / R_t[0].real\n",
        "\n",
        "        # Find autocorrelation peaks\n",
        "        self.peaks = []\n",
        "        if len(self.correlations) >1 :\n",
        "            positive = self.correlations[1] > self.correlations[0]\n",
        "            max = 1\n",
        "            for i in range(2, len(self.correlations)):\n",
        "                if not positive and self.correlations[i] > self.correlations[i-1]:\n",
        "                    max = i\n",
        "                    positive = not positive\n",
        "                elif positive and self.correlations[i] > self.correlations[max]:\n",
        "                    max = i\n",
        "                elif positive and self.correlations[i] < self.correlations[i-1]:\n",
        "                    if max > 1 and self.correlations[max] > self.CORR_THRESH:\n",
        "                        self.peaks.append(max)\n",
        "                        if self.correlations[max] > self.max_acf:\n",
        "                            self.max_acf = self.correlations[max]\n",
        "                    positive = not positive\n",
        "        # If there is no autocorrelation peak within the MAX_WINDOW boundary,\n",
        "        # try windows from the largest to the smallest\n",
        "        if len(self.peaks) <= 1:\n",
        "            self.peaks = range(2, len(self.correlations))\n",
        "\n",
        "def moving_average(data, _range):\n",
        "    ret = np.cumsum(data)\n",
        "    ret[_range:] = ret[int(_range):] - ret[:-int(_range)]\n",
        "    return ret[int(_range) - 1:] / _range\n",
        "\n",
        "def SMA(data, _range, slide):\n",
        "    ret = moving_average(data, int(_range))[::int(slide)]\n",
        "    return list(ret)\n",
        "\n",
        "def binary_search(head,tail,data,min_obj,orig_kurt,window_size):\n",
        "    while head <= tail:\n",
        "        w = int(round((head + tail) / 2.0))\n",
        "        smoothed = SMA(data,w,1)\n",
        "        metrics  = Metrics(smoothed)\n",
        "        if metrics.kurtosis >= orig_kurt:\n",
        "            if metrics.roughness < min_obj:\n",
        "                window_size = w\n",
        "                min_obj = metrics.roughness\n",
        "            head = w + 1\n",
        "        else:\n",
        "            tail = w - 1\n",
        "    return window_size\n",
        "\n",
        "def smooth_ASAP(data, max_window=5, resolution=None):\n",
        "    data = np.array(data)\n",
        "    # Preaggregate according to resolution\n",
        "    slide_size = 1\n",
        "    window_size = 1\n",
        "    if resolution and len(data) >= 2 * resolution:\n",
        "        slide_size = len(data) / resolution\n",
        "        data = SMA(data, slide_size, slide_size)\n",
        "    acf         = ACF(data, max_lag=len(data) / max_window)\n",
        "    peaks       = acf.peaks\n",
        "    orig_kurt   = acf.kurtosis\n",
        "    min_obj     = acf.roughness\n",
        "    lb          = 1\n",
        "    largest_feasible = -1\n",
        "    tail = len(data) / max_window\n",
        "    for i in range(len(peaks) - 1, -1, -1):\n",
        "        w = peaks[i]\n",
        "\n",
        "        if w < lb or w == 1:\n",
        "            break\n",
        "        elif math.sqrt(1 - acf.correlations[w]) * window_size > math.sqrt(1 - acf.correlations[window_size]) * w:\n",
        "            continue\n",
        "\n",
        "        smoothed = SMA(data, w, 1)\n",
        "        metrics = Metrics(smoothed)\n",
        "        if metrics.roughness < min_obj and metrics.kurtosis >= orig_kurt:\n",
        "            min_obj = metrics.roughness\n",
        "            window_size = w\n",
        "            lb = round( max(w*math.sqrt( (acf.max_acf -1) / (acf.correlations[w]-1) ), lb) )\n",
        "    if largest_feasible > 0:\n",
        "        if largest_feasible < len(peaks) - 2:\n",
        "            tail = peaks[largest_feasible + 1]\n",
        "        lb = max(lb, peaks[largest_feasible] + 1)\n",
        "\n",
        "    window_size = binary_search(lb, tail, data, min_obj, orig_kurt, window_size)\n",
        "    return window_size, slide_size"
      ],
      "metadata": {
        "id": "zzZ-MWSI-hQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ARIMA Model"
      ],
      "metadata": {
        "id": "XgY56PJwaC0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from math import sqrt\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# multistep sarima forecast\n",
        "def sarima_multistep_forecast(history, config, window_size, n_steps):\n",
        "    order, sorder, trend = config\n",
        "    new_hist = history[:]\n",
        "    yhat = []\n",
        "    total_training_elapsed_time = []\n",
        "    # define model\n",
        "    for i in range(n_steps):\n",
        "\n",
        "      model = SARIMAX(new_hist[-window_size:], order=order, seasonal_order=sorder, trend=trend,\n",
        "                    enforce_stationarity=False, enforce_invertibility=False)\n",
        "\n",
        "      # Record the starting time to train ARIMA Model\n",
        "      training_start_time = time.time()\n",
        "\n",
        "      # fit model\n",
        "      model_fit = model.fit(disp=False)\n",
        "\n",
        "      # Record the ending time of Training\n",
        "      training_end_time = time.time()\n",
        "      training_elapsed_time = training_end_time - training_start_time\n",
        "      total_training_elapsed_time.append(training_elapsed_time)\n",
        "\n",
        "      # make multistep forecast\n",
        "      # yhat = model_fit.forecast(steps=n_steps)\n",
        "      prediction = model_fit.predict(start=len(history[-window_size:]), end=len(history[-window_size:]))\n",
        "      yhat = np.append(yhat, prediction)\n",
        "      new_hist = np.append(new_hist, prediction)\n",
        "      new_hist = new_hist[1:]\n",
        "    return yhat, np.sum(total_training_elapsed_time), np.mean(total_training_elapsed_time)\n",
        "\n",
        "# root mean squared error or rmse\n",
        "def measure_rmse(actual, predicted):\n",
        "  return sqrt(mean_squared_error(actual, predicted))"
      ],
      "metadata": {
        "id": "MM2q5JEPZypw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node1_delay = df[['node1_delay']]\n",
        "\n",
        "node1_delay_dataset = node1_delay.values\n",
        "\n",
        "window_size, slide_size = smooth_ASAP(node1_delay_dataset, resolution=50)\n",
        "print(\"Window Size: \", window_size)\n",
        "denoised_node1_delay_dataset = moving_average(node1_delay_dataset, window_size)\n",
        "\n",
        "train_size = int(len(denoised_node1_delay_dataset) * 0.9)\n",
        "test_size = len(denoised_node1_delay_dataset) - train_size\n",
        "train, test = denoised_node1_delay_dataset[0:train_size], denoised_node1_delay_dataset[train_size:]\n",
        "print(len(train), len(test))\n",
        "\n",
        "cfg = ((1, 0, 1), (0, 0, 0, 0), 'c')\n",
        "window_length = 285\n",
        "\n",
        "horizons = [2, 4, 6, 8, 10]\n",
        "\n",
        "for horizon in horizons:\n",
        "    print(f'================== horizon = {horizon} ==========================')\n",
        "    predictions = []\n",
        "    ys = []\n",
        "    # seed history with training dataset\n",
        "    history = []\n",
        "    history.extend(train)\n",
        "\n",
        "    sum_total_training_elapsed_time, mean_total_training_elapsed_time = [], []\n",
        "\n",
        "\n",
        "    # Record the starting time to generate predictions\n",
        "    predictions_start_time = time.time()\n",
        "\n",
        "    # step over each time-step in the test set\n",
        "    # for i = 0\n",
        "    # fit model and make forecast for history\n",
        "    yhat, sum_training_elapsed_time, mean_training_elapsed_time = sarima_multistep_forecast(np.array(history), cfg, window_length, horizon)\n",
        "    sum_total_training_elapsed_time.append(sum_training_elapsed_time)\n",
        "    mean_total_training_elapsed_time.append(mean_training_elapsed_time)\n",
        "    # store forecast in list of predictions\n",
        "    predictions.append(yhat)\n",
        "    ys.append(test[:horizon])\n",
        "    # add actual observation to history for the next loop\n",
        "    history.extend(test[:horizon])\n",
        "    for i in tqdm(range(horizon, len(test))):\n",
        "        # fit model and make forecast for history\n",
        "        yhat, sum_training_elapsed_time, mean_training_elapsed_time = sarima_multistep_forecast(np.array(history), cfg, window_length, horizon)\n",
        "        # store forecast in list of predictions\n",
        "        predictions.append(yhat)\n",
        "        ys.append(test[i:i+horizon])\n",
        "        sum_total_training_elapsed_time.append(sum_training_elapsed_time)\n",
        "        mean_total_training_elapsed_time.append(mean_training_elapsed_time)\n",
        "        # add actual observation to history for the next loop\n",
        "        history.append(test[i])\n",
        "\n",
        "    # Record the ending time of generating predictions\n",
        "    predictions_end_time = time.time()\n",
        "    predictions_elapsed_time = predictions_end_time - predictions_start_time\n",
        "\n",
        "    # estimate prediction error\n",
        "    ys_converted = [array.tolist() for array in ys if len(array) == horizon]\n",
        "    predictions_converted = [array.tolist() for array in predictions]\n",
        "\n",
        "    testRMSE = np.sqrt(mean_squared_error(ys_converted, predictions_converted[:len(ys_converted)]))\n",
        "    testMAE = mean_absolute_error(ys_converted, predictions_converted[:len(ys_converted)])\n",
        "\n",
        "    print('ARIMA Test RMSE : %.5f' % (testRMSE))\n",
        "    print('ARIMA Test MAE : %.5f' % (testMAE))\n",
        "    print(\"ARIMA Total Time to train models : %.5f\" % (np.sum(sum_total_training_elapsed_time)), \"seconds\")\n",
        "    print(\"ARIMA mean Time to train models : %.5f\" % (np.mean(mean_total_training_elapsed_time)), \"seconds\")\n",
        "    print(\"ARIMA Elapsed Time To generate Predictions : %.5f\" % (predictions_elapsed_time), \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDnGtOCcaG1i",
        "outputId": "50191430-3a80-4129-98c8-d6db50bd0180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window Size:  10\n",
            "17991 2000\n",
            "================== horizon = 2 ==========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1998/1998 [11:27<00:00,  2.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA Test RMSE : 0.02629\n",
            "ARIMA Test MAE : 0.02119\n",
            "ARIMA Total Time to train models : 653.75044 seconds\n",
            "ARIMA mean Time to train models : 0.16352 seconds\n",
            "ARIMA Elapsed Time To generate Predictions : 688.39788 seconds\n",
            "================== horizon = 4 ==========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1996/1996 [21:35<00:00,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA Test RMSE : 0.03295\n",
            "ARIMA Test MAE : 0.02622\n",
            "ARIMA Total Time to train models : 1244.10167 seconds\n",
            "ARIMA mean Time to train models : 0.15575 seconds\n",
            "ARIMA Elapsed Time To generate Predictions : 1296.05685 seconds\n",
            "================== horizon = 6 ==========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1994/1994 [32:03<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA Test RMSE : 0.03770\n",
            "ARIMA Test MAE : 0.02971\n",
            "ARIMA Total Time to train models : 1852.95325 seconds\n",
            "ARIMA mean Time to train models : 0.15480 seconds\n",
            "ARIMA Elapsed Time To generate Predictions : 1924.17167 seconds\n",
            "================== horizon = 8 ==========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1992/1992 [42:45<00:00,  1.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA Test RMSE : 0.04142\n",
            "ARIMA Test MAE : 0.03255\n",
            "ARIMA Total Time to train models : 2474.79536 seconds\n",
            "ARIMA mean Time to train models : 0.15522 seconds\n",
            "ARIMA Elapsed Time To generate Predictions : 2566.81526 seconds\n",
            "================== horizon = 10 ==========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1990/1990 [53:25<00:00,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA Test RMSE : 0.04443\n",
            "ARIMA Test MAE : 0.03492\n",
            "ARIMA Total Time to train models : 3095.37520 seconds\n",
            "ARIMA mean Time to train models : 0.15547 seconds\n",
            "ARIMA Elapsed Time To generate Predictions : 3206.97499 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oeqf47RbEdJR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}